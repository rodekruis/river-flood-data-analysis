{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Google Runoff Reanalysis & Reforecast dataset (GRRR) analysis**\n",
    "\n",
    "This notebook contains an analysis of the Google Runoff Reanalysis & Reforecast dataset (GRRR) (riverine) flood forecasting skill for Mali. Data is first downloaded manually from an complementary online colab environment (https://colab.research.google.com/drive/1FnXXSEQqU1TJhMPiNeWUTr9LnbJwZzMm?usp=sharing) with access to the GRRR, and then ran through the pipeline with a configuration of choice. Options are different lead times (1-7), trigger thresholds (1.5 to 10-year return period or 95th to 99th-percentiles), or the data to verify the forecasts with (i.e. either impact data or observational data).\n",
    "\n",
    "The impact and observational data are available upon request, see the README.\n",
    "\n",
    "#### **GRRR data**\n",
    "\n",
    "The GRRR consists of hydrologic predictions by Google state-of-the-art (SOTA) hydrologic model, an improved version of the model described in the Nature article (https://www.nature.com/articles/s41586-024-07145-1), with full global coverage and daily resolution. It includes:\n",
    "* Streamflow reanalysis data for ±1M hydrobasins (hybases) (hydrological basin) for 1980 - 2023, on which, too, return period values are based and calculated (the ‘severity threshold levels’ from the API);\n",
    "* Streamflow reforecast data for ±1M hybases for 2016 - 2022, with 7-day lead times.\n",
    "\n",
    "Of all these hybases, some are so-called \"verified\" and others are \"unverified,\" meaning they have been verified according to certain standard with e.g. discharge or satellite data by the Google Research team, or not. The ratio [verified : unverified] for Mali is around [1 : 100]. All predictions, however, are made with the same model. In this analysis, we consider all hybas locations for Mali, with priority for verified gauges when appropriate and available.\n",
    "\n",
    "#### **Methods**\n",
    "\n",
    "A high-level written explanation of the workings of the code is available at [INSERT LINK]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Mali_gauges, df_Mali_gauge_models, df_Mali_forecasts = \\\n",
    "    analyse.get_country_data('Mali','2024-07-08', '2024-10-08', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_Mali_gauges['gaugeId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def print_gauge_IDs(df: pd.DataFrame, width = 1):\n",
    "#     \"\"\"\n",
    "#     Prints the gauge IDs in the dataframe in a pretty format\n",
    "\n",
    "#     :param df: dataframe containing gauge IDs\n",
    "#     :param width: width of the columns to print with\n",
    "#     \"\"\"\n",
    "#     print(\"Mali_hybases = [\")\n",
    "#     for idx in range(0, len(df), width):\n",
    "#         print(\"    \" + \", \".join([f\"'{g}'\" for g in df[idx : idx + width]]) + \",\")\n",
    "#     print(\"]\")\n",
    "\n",
    "\n",
    "# print_gauge_IDs(df_Mali_gauges['gaugeId'], 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To download the GRRR data for a specific location**, we need to access it through the online environment (of the aforementioned URL). In there, replace the cell with code equal to comment-block (1) with the code in comment-block (2) for the verified gauges and comment-block (3) for the unverified gauges. Then, paste comment-block (4) in the colab as well for the Mali data to download locally in the colab environment. When the data is placed into a folder there (and if all is processed correctly), download it manually as a .zip (by converting: ``!zip -r gauge_data.zip /content/gauge_data`` -- and downloading: ``from google.colab import files`` and ``files.download('gauge_data.zip'))``.\n",
    "\n",
    "With the current comment-blocks with the Mali gauge IDs, all Mali data will be downloaded. To download data for another country, generate the gauge IDs with the API's ListGauges() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" comment-block (1)\n",
    "gauge_reforecast_ds = reforecast_ds.sel(gauge_id=hybas_id).compute()\n",
    "gauge_reanalysis_ds = reanalysis_ds.sel(gauge_id=hybas_id).compute()\n",
    "gauge_return_periods_ds = return_periods_ds.sel(gauge_id=hybas_id).compute()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" comment-block (2)\n",
    "Mali_hybases = [\n",
    "  'hybas_1120641660',\n",
    "  'hybas_1120650110',\n",
    "  'hybas_1120661040',\n",
    "  'hybas_1120679780',\n",
    "  'hybas_1120689830',\n",
    "  'hybas_1120705070',\n",
    "  'hybas_1120737100',\n",
    "  'hybas_1120739110',\n",
    "  'hybas_1120758950',\n",
    "  'hybas_1120766460',\n",
    "  'hybas_1121890140',\n",
    "  'hybas_1121893090',\n",
    "  'hybas_1121895840',\n",
    "  'hybas_1121900350',\n",
    "  'hybas_1121905290',\n",
    "  'hybas_1121919510'\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" comment-block (3)\n",
    "Mali_hybases = [\n",
    "    'hybas_1120040380', 'hybas_1120040400', 'hybas_1120040850', 'hybas_1120040860', 'hybas_1120040960', 'hybas_1120041110', 'hybas_1120041510', 'hybas_1120041730',\n",
    "    'hybas_1120043640', 'hybas_1120303190', 'hybas_1120303810', 'hybas_1120397600', 'hybas_1120410560', 'hybas_1120411350', 'hybas_1120412240', 'hybas_1120429260',\n",
    "    'hybas_1120432960', 'hybas_1120466750', 'hybas_1120470630', 'hybas_1120471020', 'hybas_1120471030', 'hybas_1120471950', 'hybas_1120471970', 'hybas_1120472120',\n",
    "    'hybas_1120472270', 'hybas_1120472580', 'hybas_1120472590', 'hybas_1120473940', 'hybas_1120475600', 'hybas_1120477150', 'hybas_1120477170', 'hybas_1120478130',\n",
    "    'hybas_1120479380', 'hybas_1120479830', 'hybas_1120481330', 'hybas_1120482130', 'hybas_1120483990', 'hybas_1120484930', 'hybas_1120487900', 'hybas_1120488260',\n",
    "    'hybas_1120491030', 'hybas_1120491690', 'hybas_1120492490', 'hybas_1120493530', 'hybas_1120494590', 'hybas_1120496140', 'hybas_1120498360', 'hybas_1120504370',\n",
    "    'hybas_1120504470', 'hybas_1120504640', 'hybas_1120505610', 'hybas_1120509630', 'hybas_1120510390', 'hybas_1120510890', 'hybas_1120512060', 'hybas_1120512280',\n",
    "    'hybas_1120512460', 'hybas_1120513470', 'hybas_1120515750', 'hybas_1120517670', 'hybas_1120518390', 'hybas_1120518600', 'hybas_1120519840', 'hybas_1120520690',\n",
    "    'hybas_1120520940', 'hybas_1120521780', 'hybas_1120522540', 'hybas_1120523280', 'hybas_1120523380', 'hybas_1120523420', 'hybas_1120523620', 'hybas_1120523630',\n",
    "    'hybas_1120524850', 'hybas_1120525450', 'hybas_1120525650', 'hybas_1120527140', 'hybas_1120527350', 'hybas_1120527680', 'hybas_1120528640', 'hybas_1120529240',\n",
    "    'hybas_1120529440', 'hybas_1120529940', 'hybas_1120530060', 'hybas_1120530160', 'hybas_1120530300', 'hybas_1120531120', 'hybas_1120531460', 'hybas_1120531880',\n",
    "    'hybas_1120532050', 'hybas_1120532230', 'hybas_1120532750', 'hybas_1120532960', 'hybas_1120533300', 'hybas_1120533310', 'hybas_1120533700', 'hybas_1120533720',\n",
    "    'hybas_1120533810', 'hybas_1120534130', 'hybas_1120534140', 'hybas_1120536810', 'hybas_1120538130', 'hybas_1120538290', 'hybas_1120539270', 'hybas_1120540030',\n",
    "    'hybas_1120540480', 'hybas_1120540500', 'hybas_1120540680', 'hybas_1120540820', 'hybas_1120540830', 'hybas_1120544610', 'hybas_1120545390', 'hybas_1120545620',\n",
    "    'hybas_1120547920', 'hybas_1120547940', 'hybas_1120548470', 'hybas_1120549740', 'hybas_1120550010', 'hybas_1120550020', 'hybas_1120550030', 'hybas_1120550230',\n",
    "    'hybas_1120550380', 'hybas_1120550540', 'hybas_1120550770', 'hybas_1120550780', 'hybas_1120551370', 'hybas_1120551410', 'hybas_1120551560', 'hybas_1120551770',\n",
    "    'hybas_1120551780', 'hybas_1120552270', 'hybas_1120552280', 'hybas_1120552460', 'hybas_1120552650', 'hybas_1120553250', 'hybas_1120553570', 'hybas_1120553740',\n",
    "    'hybas_1120553750', 'hybas_1120554320', 'hybas_1120554770', 'hybas_1120554930', 'hybas_1120554940', 'hybas_1120555140', 'hybas_1120555550', 'hybas_1120555730',\n",
    "    'hybas_1120556160', 'hybas_1120556480', 'hybas_1120556700', 'hybas_1120556990', 'hybas_1120557110', 'hybas_1120557170', 'hybas_1120557770', 'hybas_1120557870',\n",
    "    'hybas_1120558440', 'hybas_1120558570', 'hybas_1120558910', 'hybas_1120559030', 'hybas_1120559430', 'hybas_1120559460', 'hybas_1120559600', 'hybas_1120559630',\n",
    "    'hybas_1120560050', 'hybas_1120560230', 'hybas_1120560390', 'hybas_1120560420', 'hybas_1120560430', 'hybas_1120560650', 'hybas_1120561470', 'hybas_1120561650',\n",
    "    'hybas_1120561810', 'hybas_1120561830', 'hybas_1120562900', 'hybas_1120562920', 'hybas_1120563590', 'hybas_1120563820', 'hybas_1120564590', 'hybas_1120564740',\n",
    "    'hybas_1120564860', 'hybas_1120564960', 'hybas_1120564980', 'hybas_1120565220', 'hybas_1120565330', 'hybas_1120565450', 'hybas_1120565570', 'hybas_1120565730',\n",
    "    'hybas_1120565890', 'hybas_1120565930', 'hybas_1120566540', 'hybas_1120566570', 'hybas_1120566910', 'hybas_1120567030', 'hybas_1120567200', 'hybas_1120567210',\n",
    "    'hybas_1120567350', 'hybas_1120567380', 'hybas_1120567860', 'hybas_1120567870', 'hybas_1120567880', 'hybas_1120568290', 'hybas_1120568530', 'hybas_1120568560',\n",
    "    'hybas_1120568750', 'hybas_1120568760', 'hybas_1120569130', 'hybas_1120569140', 'hybas_1120569160', 'hybas_1120569550', 'hybas_1120569730', 'hybas_1120570200',\n",
    "    'hybas_1120570310', 'hybas_1120570840', 'hybas_1120571020', 'hybas_1120571170', 'hybas_1120571190', 'hybas_1120571210', 'hybas_1120571400', 'hybas_1120571780',\n",
    "    'hybas_1120571790', 'hybas_1120571810', 'hybas_1120571920', 'hybas_1120572020', 'hybas_1120572030', 'hybas_1120572190', 'hybas_1120572200', 'hybas_1120572390',\n",
    "    'hybas_1120572870', 'hybas_1120573290', 'hybas_1120573670', 'hybas_1120574720', 'hybas_1120574930', 'hybas_1120575020', 'hybas_1120575340', 'hybas_1120575820',\n",
    "    'hybas_1120575840', 'hybas_1120576250', 'hybas_1120576260', 'hybas_1120576370', 'hybas_1120576380', 'hybas_1120576510', 'hybas_1120577130', 'hybas_1120577220',\n",
    "    'hybas_1120577230', 'hybas_1120577590', 'hybas_1120577630', 'hybas_1120577750', 'hybas_1120577880', 'hybas_1120578220', 'hybas_1120579520', 'hybas_1120580940',\n",
    "    'hybas_1120581610', 'hybas_1120582050', 'hybas_1120582390', 'hybas_1120582680', 'hybas_1120582840', 'hybas_1120582940', 'hybas_1120582950', 'hybas_1120583150',\n",
    "    'hybas_1120583620', 'hybas_1120583640', 'hybas_1120584260', 'hybas_1120584350', 'hybas_1120584360', 'hybas_1120584370', 'hybas_1120584500', 'hybas_1120585080',\n",
    "    'hybas_1120585100', 'hybas_1120585450', 'hybas_1120585480', 'hybas_1120585920', 'hybas_1120586040', 'hybas_1120586210', 'hybas_1120587030', 'hybas_1120587040',\n",
    "    'hybas_1120587440', 'hybas_1120587780', 'hybas_1120587790', 'hybas_1120587880', 'hybas_1120588270', 'hybas_1120588310', 'hybas_1120588700', 'hybas_1120588980',\n",
    "    'hybas_1120589130', 'hybas_1120589730', 'hybas_1120589740', 'hybas_1120589750', 'hybas_1120590180', 'hybas_1120590190', 'hybas_1120590750', 'hybas_1120591050',\n",
    "    'hybas_1120591190', 'hybas_1120591200', 'hybas_1120591220', 'hybas_1120591420', 'hybas_1120591560', 'hybas_1120591740', 'hybas_1120591770', 'hybas_1120592240',\n",
    "    'hybas_1120592250', 'hybas_1120592350', 'hybas_1120592790', 'hybas_1120593030', 'hybas_1120593040', 'hybas_1120593050', 'hybas_1120593470', 'hybas_1120594180',\n",
    "    'hybas_1120594190', 'hybas_1120594490', 'hybas_1120594520', 'hybas_1120595390', 'hybas_1120595410', 'hybas_1120595660', 'hybas_1120595690', 'hybas_1120595820',\n",
    "    'hybas_1120595960', 'hybas_1120596090', 'hybas_1120596100', 'hybas_1120596260', 'hybas_1120596300', 'hybas_1120596680', 'hybas_1120596900', 'hybas_1120596910',\n",
    "    'hybas_1120597170', 'hybas_1120597370', 'hybas_1120597480', 'hybas_1120597500', 'hybas_1120597890', 'hybas_1120598090', 'hybas_1120598250', 'hybas_1120598480',\n",
    "    'hybas_1120599220', 'hybas_1120599600', 'hybas_1120599620', 'hybas_1120600140', 'hybas_1120600480', 'hybas_1120600830', 'hybas_1120601080', 'hybas_1120601110',\n",
    "    'hybas_1120601830', 'hybas_1120602200', 'hybas_1120602230', 'hybas_1120602680', 'hybas_1120602870', 'hybas_1120603030', 'hybas_1120603340', 'hybas_1120603370',\n",
    "    'hybas_1120603580', 'hybas_1120603600', 'hybas_1120603890', 'hybas_1120603910', 'hybas_1120604170', 'hybas_1120604190', 'hybas_1120604370', 'hybas_1120604770',\n",
    "    'hybas_1120605370', 'hybas_1120605610', 'hybas_1120605720', 'hybas_1120605730', 'hybas_1120605790', 'hybas_1120605800', 'hybas_1120605930', 'hybas_1120606420',\n",
    "    'hybas_1120606590', 'hybas_1120606620', 'hybas_1120606800', 'hybas_1120606910', 'hybas_1120606920', 'hybas_1120607100', 'hybas_1120607120', 'hybas_1120607270',\n",
    "    'hybas_1120608260', 'hybas_1120608270', 'hybas_1120608280', 'hybas_1120608450', 'hybas_1120609020', 'hybas_1120609100', 'hybas_1120609250', 'hybas_1120609270',\n",
    "    'hybas_1120609400', 'hybas_1120609580', 'hybas_1120609590', 'hybas_1120609790', 'hybas_1120609940', 'hybas_1120609950', 'hybas_1120610400', 'hybas_1120610540',\n",
    "    'hybas_1120610710', 'hybas_1120610850', 'hybas_1120611050', 'hybas_1120611180', 'hybas_1120611190', 'hybas_1120611200', 'hybas_1120611650', 'hybas_1120611660',\n",
    "    'hybas_1120612020', 'hybas_1120612370', 'hybas_1120613290', 'hybas_1120613300', 'hybas_1120613430', 'hybas_1120613460', 'hybas_1120614120', 'hybas_1120614290',\n",
    "    'hybas_1120614820', 'hybas_1120614850', 'hybas_1120614860', 'hybas_1120615070', 'hybas_1120615200', 'hybas_1120615740', 'hybas_1120616130', 'hybas_1120616150',\n",
    "    'hybas_1120616250', 'hybas_1120616410', 'hybas_1120616420', 'hybas_1120616500', 'hybas_1120616670', 'hybas_1120616690', 'hybas_1120616700', 'hybas_1120617070',\n",
    "    'hybas_1120617160', 'hybas_1120617360', 'hybas_1120617570', 'hybas_1120617870', 'hybas_1120617880', 'hybas_1120617930', 'hybas_1120618270', 'hybas_1120618560',\n",
    "    'hybas_1120618570', 'hybas_1120618670', 'hybas_1120618790', 'hybas_1120618800', 'hybas_1120619400', 'hybas_1120619810', 'hybas_1120619820', 'hybas_1120619920',\n",
    "    'hybas_1120619930', 'hybas_1120620140', 'hybas_1120620420', 'hybas_1120620550', 'hybas_1120620950', 'hybas_1120620980', 'hybas_1120621160', 'hybas_1120621340',\n",
    "    'hybas_1120621490', 'hybas_1120621510', 'hybas_1120621520', 'hybas_1120621820', 'hybas_1120621980', 'hybas_1120622370', 'hybas_1120622540', 'hybas_1120623020',\n",
    "    'hybas_1120623040', 'hybas_1120623900', 'hybas_1120623920', 'hybas_1120623930', 'hybas_1120624050', 'hybas_1120624420', 'hybas_1120624560', 'hybas_1120624690',\n",
    "    'hybas_1120624700', 'hybas_1120624920', 'hybas_1120625000', 'hybas_1120625010', 'hybas_1120625310', 'hybas_1120625570', 'hybas_1120625970', 'hybas_1120626120',\n",
    "    'hybas_1120626130', 'hybas_1120626240', 'hybas_1120626260', 'hybas_1120626430', 'hybas_1120626460', 'hybas_1120626560', 'hybas_1120626670', 'hybas_1120626700',\n",
    "    'hybas_1120626730', 'hybas_1120626740', 'hybas_1120626870', 'hybas_1120627060', 'hybas_1120627170', 'hybas_1120627280', 'hybas_1120627290', 'hybas_1120627430',\n",
    "    'hybas_1120627450', 'hybas_1120627460', 'hybas_1120627470', 'hybas_1120627480', 'hybas_1120627660', 'hybas_1120627670', 'hybas_1120627850', 'hybas_1120627860',\n",
    "    'hybas_1120627960', 'hybas_1120627980', 'hybas_1120628110', 'hybas_1120628120', 'hybas_1120628260', 'hybas_1120628330', 'hybas_1120628350', 'hybas_1120628510',\n",
    "    'hybas_1120628520', 'hybas_1120628530', 'hybas_1120628540', 'hybas_1120628550', 'hybas_1120628680', 'hybas_1120628730', 'hybas_1120629420', 'hybas_1120629430',\n",
    "    'hybas_1120630150', 'hybas_1120630290', 'hybas_1120630440', 'hybas_1120630620', 'hybas_1120630640', 'hybas_1120630800', 'hybas_1120630930', 'hybas_1120631400',\n",
    "    'hybas_1120631550', 'hybas_1120631730', 'hybas_1120631740', 'hybas_1120631890', 'hybas_1120632060', 'hybas_1120632070', 'hybas_1120632490', 'hybas_1120632620',\n",
    "    'hybas_1120632630', 'hybas_1120633370', 'hybas_1120633580', 'hybas_1120633600', 'hybas_1120634230', 'hybas_1120634240', 'hybas_1120634440', 'hybas_1120634800',\n",
    "    'hybas_1120634810', 'hybas_1120635400', 'hybas_1120635720', 'hybas_1120635740', 'hybas_1120636080', 'hybas_1120636160', 'hybas_1120636170', 'hybas_1120636660',\n",
    "    'hybas_1120636820', 'hybas_1120636970', 'hybas_1120637330', 'hybas_1120637340', 'hybas_1120637510', 'hybas_1120637690', 'hybas_1120637880', 'hybas_1120638180',\n",
    "    'hybas_1120638320', 'hybas_1120638470', 'hybas_1120638490', 'hybas_1120638510', 'hybas_1120638530', 'hybas_1120638640', 'hybas_1120638930', 'hybas_1120639070',\n",
    "    'hybas_1120639310', 'hybas_1120639320', 'hybas_1120639330', 'hybas_1120639350', 'hybas_1120639370', 'hybas_1120639480', 'hybas_1120639620', 'hybas_1120639840',\n",
    "    'hybas_1120639850', 'hybas_1120639990', 'hybas_1120640000', 'hybas_1120640160', 'hybas_1120640190', 'hybas_1120640350', 'hybas_1120640370', 'hybas_1120640580',\n",
    "    'hybas_1120640680', 'hybas_1120640840', 'hybas_1120641440', 'hybas_1120641630', 'hybas_1120641640', 'hybas_1120641660', 'hybas_1120641680', 'hybas_1120642000',\n",
    "    'hybas_1120642010', 'hybas_1120642020', 'hybas_1120642140', 'hybas_1120642160', 'hybas_1120642490', 'hybas_1120642990', 'hybas_1120643020', 'hybas_1120643170',\n",
    "    'hybas_1120643390', 'hybas_1120643540', 'hybas_1120643800', 'hybas_1120643820', 'hybas_1120643950', 'hybas_1120644110', 'hybas_1120644270', 'hybas_1120644470',\n",
    "    'hybas_1120644480', 'hybas_1120644490', 'hybas_1120644500', 'hybas_1120644510', 'hybas_1120644620', 'hybas_1120644920', 'hybas_1120645180', 'hybas_1120645190',\n",
    "    'hybas_1120645630', 'hybas_1120645770', 'hybas_1120646000', 'hybas_1120646230', 'hybas_1120646630', 'hybas_1120646780', 'hybas_1120646950', 'hybas_1120646960',\n",
    "    'hybas_1120646970', 'hybas_1120647210', 'hybas_1120647330', 'hybas_1120647580', 'hybas_1120647600', 'hybas_1120647730', 'hybas_1120647860', 'hybas_1120647880',\n",
    "    'hybas_1120647910', 'hybas_1120648120', 'hybas_1120648650', 'hybas_1120648910', 'hybas_1120649070', 'hybas_1120649770', 'hybas_1120650110', 'hybas_1120650150',\n",
    "    'hybas_1120650510', 'hybas_1120650760', 'hybas_1120650770', 'hybas_1120650880', 'hybas_1120651040', 'hybas_1120651140', 'hybas_1120651160', 'hybas_1120652080',\n",
    "    'hybas_1120652110', 'hybas_1120652930', 'hybas_1120653070', 'hybas_1120653220', 'hybas_1120653430', 'hybas_1120653920', 'hybas_1120653930', 'hybas_1120653940',\n",
    "    'hybas_1120654250', 'hybas_1120654440', 'hybas_1120654450', 'hybas_1120655200', 'hybas_1120655490', 'hybas_1120655670', 'hybas_1120655680', 'hybas_1120655880',\n",
    "    'hybas_1120656040', 'hybas_1120656050', 'hybas_1120656400', 'hybas_1120656520', 'hybas_1120656740', 'hybas_1120656870', 'hybas_1120657320', 'hybas_1120657440',\n",
    "    'hybas_1120657550', 'hybas_1120657740', 'hybas_1120657830', 'hybas_1120657850', 'hybas_1120657960', 'hybas_1120657990', 'hybas_1120658110', 'hybas_1120658270',\n",
    "    'hybas_1120658280', 'hybas_1120658790', 'hybas_1120659170', 'hybas_1120659180', 'hybas_1120659570', 'hybas_1120659990', 'hybas_1120660180', 'hybas_1120660330',\n",
    "    'hybas_1120660570', 'hybas_1120660580', 'hybas_1120660860', 'hybas_1120661020', 'hybas_1120661040', 'hybas_1120661190', 'hybas_1120661200', 'hybas_1120661220',\n",
    "    'hybas_1120661290', 'hybas_1120661440', 'hybas_1120661550', 'hybas_1120661570', 'hybas_1120661810', 'hybas_1120661820', 'hybas_1120662020', 'hybas_1120662030',\n",
    "    'hybas_1120662040', 'hybas_1120662150', 'hybas_1120662160', 'hybas_1120662170', 'hybas_1120662370', 'hybas_1120662450', 'hybas_1120662460', 'hybas_1120662570',\n",
    "    'hybas_1120662940', 'hybas_1120663010', 'hybas_1120663210', 'hybas_1120663220', 'hybas_1120663330', 'hybas_1120663340', 'hybas_1120663550', 'hybas_1120664010',\n",
    "    'hybas_1120664030', 'hybas_1120664620', 'hybas_1120664760', 'hybas_1120664890', 'hybas_1120665270', 'hybas_1120665450', 'hybas_1120665680', 'hybas_1120665690',\n",
    "    'hybas_1120665700', 'hybas_1120665730', 'hybas_1120665930', 'hybas_1120666250', 'hybas_1120666260', 'hybas_1120666270', 'hybas_1120666450', 'hybas_1120666460',\n",
    "    'hybas_1120666470', 'hybas_1120666620', 'hybas_1120666640', 'hybas_1120666890', 'hybas_1120666920', 'hybas_1120667130', 'hybas_1120667360', 'hybas_1120667560',\n",
    "    'hybas_1120667570', 'hybas_1120667880', 'hybas_1120668080', 'hybas_1120668110', 'hybas_1120668220', 'hybas_1120668310', 'hybas_1120668430', 'hybas_1120668580',\n",
    "    'hybas_1120668750', 'hybas_1120668890', 'hybas_1120668900', 'hybas_1120669370', 'hybas_1120669410', 'hybas_1120669420', 'hybas_1120669690', 'hybas_1120669700',\n",
    "    'hybas_1120669950', 'hybas_1120670290', 'hybas_1120670530', 'hybas_1120670650', 'hybas_1120671040', 'hybas_1120671050', 'hybas_1120671150', 'hybas_1120671380',\n",
    "    'hybas_1120671410', 'hybas_1120671550', 'hybas_1120671560', 'hybas_1120671750', 'hybas_1120671900', 'hybas_1120671910', 'hybas_1120671920', 'hybas_1120672510',\n",
    "    'hybas_1120673030', 'hybas_1120673060', 'hybas_1120673240', 'hybas_1120673440', 'hybas_1120673450', 'hybas_1120673460', 'hybas_1120673470', 'hybas_1120673740',\n",
    "    'hybas_1120673890', 'hybas_1120673900', 'hybas_1120674630', 'hybas_1120674800', 'hybas_1120674900', 'hybas_1120674920', 'hybas_1120674930', 'hybas_1120675110',\n",
    "    'hybas_1120675120', 'hybas_1120675320', 'hybas_1120675440', 'hybas_1120675450', 'hybas_1120675460', 'hybas_1120675470', 'hybas_1120676440', 'hybas_1120676680',\n",
    "    'hybas_1120676810', 'hybas_1120676840', 'hybas_1120677030', 'hybas_1120677520', 'hybas_1120677670', 'hybas_1120677900', 'hybas_1120677910', 'hybas_1120678060',\n",
    "    'hybas_1120678070', 'hybas_1120679250', 'hybas_1120679750', 'hybas_1120679780', 'hybas_1120679900', 'hybas_1120680040', 'hybas_1120680250', 'hybas_1120680440',\n",
    "    'hybas_1120681230', 'hybas_1120681490', 'hybas_1120681510', 'hybas_1120681700', 'hybas_1120681730', 'hybas_1120681900', 'hybas_1120682030', 'hybas_1120682040',\n",
    "    'hybas_1120682230', 'hybas_1120682720', 'hybas_1120682870', 'hybas_1120682900', 'hybas_1120683000', 'hybas_1120683440', 'hybas_1120683450', 'hybas_1120683650',\n",
    "    'hybas_1120683670', 'hybas_1120683750', 'hybas_1120684180', 'hybas_1120684520', 'hybas_1120684640', 'hybas_1120684820', 'hybas_1120684830', 'hybas_1120685060',\n",
    "    'hybas_1120685070', 'hybas_1120685220', 'hybas_1120685600', 'hybas_1120685890', 'hybas_1120686300', 'hybas_1120686430', 'hybas_1120687330', 'hybas_1120687340',\n",
    "    'hybas_1120687350', 'hybas_1120687650', 'hybas_1120688060', 'hybas_1120688340', 'hybas_1120688460', 'hybas_1120688690', 'hybas_1120688870', 'hybas_1120688880',\n",
    "    'hybas_1120689060', 'hybas_1120689070', 'hybas_1120689080', 'hybas_1120689090', 'hybas_1120689180', 'hybas_1120689210', 'hybas_1120689430', 'hybas_1120689440',\n",
    "    'hybas_1120689470', 'hybas_1120689480', 'hybas_1120689830', 'hybas_1120689850', 'hybas_1120689980', 'hybas_1120690000', 'hybas_1120690130', 'hybas_1120690300',\n",
    "    'hybas_1120690510', 'hybas_1120690730', 'hybas_1120690900', 'hybas_1120691130', 'hybas_1120691160', 'hybas_1120691400', 'hybas_1120691540', 'hybas_1120691660',\n",
    "    'hybas_1120692310', 'hybas_1120692440', 'hybas_1120692550', 'hybas_1120692570', 'hybas_1120692820', 'hybas_1120693340', 'hybas_1120693600', 'hybas_1120693800',\n",
    "    'hybas_1120693950', 'hybas_1120694100', 'hybas_1120694110', 'hybas_1120694300', 'hybas_1120694600', 'hybas_1120694610', 'hybas_1120694990', 'hybas_1120695240',\n",
    "    'hybas_1120695630', 'hybas_1120696030', 'hybas_1120696310', 'hybas_1120696830', 'hybas_1120696840', 'hybas_1120697160', 'hybas_1120697620', 'hybas_1120697630',\n",
    "    'hybas_1120697730', 'hybas_1120697740', 'hybas_1120697760', 'hybas_1120698040', 'hybas_1120698390', 'hybas_1120698600', 'hybas_1120698610', 'hybas_1120699220',\n",
    "    'hybas_1120699410', 'hybas_1120699620', 'hybas_1120699850', 'hybas_1120700000', 'hybas_1120700030', 'hybas_1120700210', 'hybas_1120700220', 'hybas_1120700240',\n",
    "    'hybas_1120700250', 'hybas_1120700420', 'hybas_1120700680', 'hybas_1120700840', 'hybas_1120701160', 'hybas_1120701180', 'hybas_1120701570', 'hybas_1120702100',\n",
    "    'hybas_1120702240', 'hybas_1120702250', 'hybas_1120702620', 'hybas_1120702630', 'hybas_1120702650', 'hybas_1120703110', 'hybas_1120703660', 'hybas_1120703670',\n",
    "    'hybas_1120703920', 'hybas_1120704050', 'hybas_1120704600', 'hybas_1120704610', 'hybas_1120704760', 'hybas_1120704900', 'hybas_1120705070', 'hybas_1120705080',\n",
    "    'hybas_1120705240', 'hybas_1120705440', 'hybas_1120705580', 'hybas_1120705730', 'hybas_1120705940', 'hybas_1120705950', 'hybas_1120706170', 'hybas_1120706180',\n",
    "    'hybas_1120706350', 'hybas_1120706520', 'hybas_1120706530', 'hybas_1120706730', 'hybas_1120706900', 'hybas_1120707170', 'hybas_1120707570', 'hybas_1120707730',\n",
    "    'hybas_1120707850', 'hybas_1120707970', 'hybas_1120707980', 'hybas_1120707990', 'hybas_1120708000', 'hybas_1120708130', 'hybas_1120708810', 'hybas_1120709490',\n",
    "    'hybas_1120709510', 'hybas_1120709750', 'hybas_1120709950', 'hybas_1120710240', 'hybas_1120710930', 'hybas_1120711110', 'hybas_1120711120', 'hybas_1120711150',\n",
    "    'hybas_1120711290', 'hybas_1120711310', 'hybas_1120712000', 'hybas_1120712100', 'hybas_1120712360', 'hybas_1120712710', 'hybas_1120712740', 'hybas_1120712890',\n",
    "    'hybas_1120712910', 'hybas_1120713090', 'hybas_1120713430', 'hybas_1120713810', 'hybas_1120713990', 'hybas_1120714260', 'hybas_1120714560', 'hybas_1120714730',\n",
    "    'hybas_1120714890', 'hybas_1120714900', 'hybas_1120715340', 'hybas_1120715970', 'hybas_1120715980', 'hybas_1120716000', 'hybas_1120716140', 'hybas_1120716370',\n",
    "    'hybas_1120716510', 'hybas_1120716930', 'hybas_1120717080', 'hybas_1120717590', 'hybas_1120717700', 'hybas_1120717710', 'hybas_1120717720', 'hybas_1120717980',\n",
    "    'hybas_1120718150', 'hybas_1120718190', 'hybas_1120718200', 'hybas_1120718480', 'hybas_1120718500', 'hybas_1120718910', 'hybas_1120718920', 'hybas_1120719200',\n",
    "    'hybas_1120719300', 'hybas_1120719310', 'hybas_1120719480', 'hybas_1120719720', 'hybas_1120719840', 'hybas_1120720050', 'hybas_1120720250', 'hybas_1120721160',\n",
    "    'hybas_1120721290', 'hybas_1120721300', 'hybas_1120721460', 'hybas_1120721770', 'hybas_1120722020', 'hybas_1120722180', 'hybas_1120722200', 'hybas_1120722400',\n",
    "    'hybas_1120722680', 'hybas_1120722900', 'hybas_1120722910', 'hybas_1120722920', 'hybas_1120723030', 'hybas_1120723040', 'hybas_1120723240', 'hybas_1120723250',\n",
    "    'hybas_1120723380', 'hybas_1120724310', 'hybas_1120724320', 'hybas_1120724330', 'hybas_1120724680', 'hybas_1120724830', 'hybas_1120726130', 'hybas_1120726300',\n",
    "    'hybas_1120726500', 'hybas_1120726900', 'hybas_1120726910', 'hybas_1120727280', 'hybas_1120727290', 'hybas_1120727300', 'hybas_1120727660', 'hybas_1120727970',\n",
    "    'hybas_1120728250', 'hybas_1120728560', 'hybas_1120728760', 'hybas_1120728770', 'hybas_1120728930', 'hybas_1120728950', 'hybas_1120728960', 'hybas_1120729190',\n",
    "    'hybas_1120729430', 'hybas_1120729570', 'hybas_1120729580', 'hybas_1120729590', 'hybas_1120729700', 'hybas_1120729940', 'hybas_1120729950', 'hybas_1120729960',\n",
    "    'hybas_1120730140', 'hybas_1120730150', 'hybas_1120730460', 'hybas_1120730650', 'hybas_1120730840', 'hybas_1120731010', 'hybas_1120731020', 'hybas_1120731270',\n",
    "    'hybas_1120731410', 'hybas_1120731570', 'hybas_1120731720', 'hybas_1120731730', 'hybas_1120732080', 'hybas_1120732390', 'hybas_1120732420', 'hybas_1120732970',\n",
    "    'hybas_1120732980', 'hybas_1120733370', 'hybas_1120733820', 'hybas_1120733900', 'hybas_1120733920', 'hybas_1120734100', 'hybas_1120734770', 'hybas_1120734820',\n",
    "    'hybas_1120734990', 'hybas_1120735330', 'hybas_1120735660', 'hybas_1120735680', 'hybas_1120735690', 'hybas_1120735950', 'hybas_1120736230', 'hybas_1120736540',\n",
    "    'hybas_1120736800', 'hybas_1120737100', 'hybas_1120737520', 'hybas_1120738120', 'hybas_1120738140', 'hybas_1120738150', 'hybas_1120738320', 'hybas_1120738560',\n",
    "    'hybas_1120739050', 'hybas_1120739060', 'hybas_1120739090', 'hybas_1120739110', 'hybas_1120739380', 'hybas_1120739800', 'hybas_1120739910', 'hybas_1120740080',\n",
    "    'hybas_1120741580', 'hybas_1120741590', 'hybas_1120741710', 'hybas_1120741720', 'hybas_1120741790', 'hybas_1120742920', 'hybas_1120743410', 'hybas_1120743810',\n",
    "    'hybas_1120744170', 'hybas_1120744580', 'hybas_1120744600', 'hybas_1120744740', 'hybas_1120744750', 'hybas_1120744990', 'hybas_1120745160', 'hybas_1120745390',\n",
    "    'hybas_1120746520', 'hybas_1120747090', 'hybas_1120747780', 'hybas_1120748210', 'hybas_1120748220', 'hybas_1120748680', 'hybas_1120748690', 'hybas_1120749670',\n",
    "    'hybas_1120750740', 'hybas_1120751120', 'hybas_1120751340', 'hybas_1120751610', 'hybas_1120751630', 'hybas_1120751750', 'hybas_1120752200', 'hybas_1120752860',\n",
    "    'hybas_1120753110', 'hybas_1120753120', 'hybas_1120753520', 'hybas_1120753530', 'hybas_1120753720', 'hybas_1120753730', 'hybas_1120754480', 'hybas_1120755450',\n",
    "    'hybas_1120756360', 'hybas_1120758320', 'hybas_1120758440', 'hybas_1120758950', 'hybas_1120759390', 'hybas_1120759680', 'hybas_1120760490', 'hybas_1120760630',\n",
    "    'hybas_1120760640', 'hybas_1120760650', 'hybas_1120761040', 'hybas_1120763240', 'hybas_1120764070', 'hybas_1120764530', 'hybas_1120764550', 'hybas_1120764870',\n",
    "    'hybas_1120764880', 'hybas_1120765350', 'hybas_1120765840', 'hybas_1120766460', 'hybas_1120766700', 'hybas_1120766810', 'hybas_1120769340', 'hybas_1120769890',\n",
    "    'hybas_1120770210', 'hybas_1120771040', 'hybas_1120771890', 'hybas_1120772020', 'hybas_1120772260', 'hybas_1120772580', 'hybas_1120774270', 'hybas_1120775120',\n",
    "    'hybas_1120775680', 'hybas_1120775880', 'hybas_1120776340', 'hybas_1120776760', 'hybas_1120777580', 'hybas_1120777590', 'hybas_1120778600', 'hybas_1120779390',\n",
    "    'hybas_1120779510', 'hybas_1120779770', 'hybas_1120779870', 'hybas_1120779990', 'hybas_1120780240', 'hybas_1120780250', 'hybas_1120780530', 'hybas_1120780540',\n",
    "    'hybas_1120780550', 'hybas_1120780560', 'hybas_1120780900', 'hybas_1120781290', 'hybas_1120781490', 'hybas_1120782480', 'hybas_1120783050', 'hybas_1120783740',\n",
    "    'hybas_1120784270', 'hybas_1120784430', 'hybas_1120784440', 'hybas_1120784660', 'hybas_1120784870', 'hybas_1120785340', 'hybas_1120785350', 'hybas_1120785520',\n",
    "    'hybas_1120785540', 'hybas_1120785950', 'hybas_1120786380', 'hybas_1120786990', 'hybas_1120787010', 'hybas_1120787150', 'hybas_1120787170', 'hybas_1120788300',\n",
    "    'hybas_1120788560', 'hybas_1120788820', 'hybas_1120789240', 'hybas_1120789450', 'hybas_1120791410', 'hybas_1120791820', 'hybas_1120793000', 'hybas_1120793010',\n",
    "    'hybas_1120793460', 'hybas_1120794380', 'hybas_1120796030', 'hybas_1120796190', 'hybas_1120797430', 'hybas_1120797630', 'hybas_1120797940', 'hybas_1120798710',\n",
    "    'hybas_1120799330', 'hybas_1121749330', 'hybas_1121801600', 'hybas_1121816480', 'hybas_1121818730', 'hybas_1121819310', 'hybas_1121830390', 'hybas_1121830470',\n",
    "    'hybas_1121834830', 'hybas_1121835270', 'hybas_1121836460', 'hybas_1121838070', 'hybas_1121839340', 'hybas_1121840160', 'hybas_1121840170', 'hybas_1121841450',\n",
    "    'hybas_1121841500', 'hybas_1121842020', 'hybas_1121842590', 'hybas_1121843060', 'hybas_1121843470', 'hybas_1121844840', 'hybas_1121844910', 'hybas_1121845300',\n",
    "    'hybas_1121847490', 'hybas_1121847590', 'hybas_1121848030', 'hybas_1121848210', 'hybas_1121848430', 'hybas_1121849050', 'hybas_1121849550', 'hybas_1121850030',\n",
    "    'hybas_1121850580', 'hybas_1121850950', 'hybas_1121851680', 'hybas_1121852270', 'hybas_1121852820', 'hybas_1121853410', 'hybas_1121853780', 'hybas_1121854000',\n",
    "    'hybas_1121854220', 'hybas_1121854350', 'hybas_1121854820', 'hybas_1121855010', 'hybas_1121855120', 'hybas_1121855250', 'hybas_1121855570', 'hybas_1121855830',\n",
    "    'hybas_1121856220', 'hybas_1121856440', 'hybas_1121856490', 'hybas_1121856630', 'hybas_1121857000', 'hybas_1121857220', 'hybas_1121857430', 'hybas_1121857720',\n",
    "    'hybas_1121857860', 'hybas_1121858150', 'hybas_1121858650', 'hybas_1121859400', 'hybas_1121859690', 'hybas_1121859700', 'hybas_1121859890', 'hybas_1121860750',\n",
    "    'hybas_1121861150', 'hybas_1121862050', 'hybas_1121862110', 'hybas_1121862400', 'hybas_1121862720', 'hybas_1121863320', 'hybas_1121864920', 'hybas_1121865150',\n",
    "    'hybas_1121865370', 'hybas_1121865950', 'hybas_1121866060', 'hybas_1121866140', 'hybas_1121867220', 'hybas_1121867500', 'hybas_1121867640', 'hybas_1121869140',\n",
    "    'hybas_1121869890', 'hybas_1121869980', 'hybas_1121870020', 'hybas_1121870490', 'hybas_1121870500', 'hybas_1121870830', 'hybas_1121872260', 'hybas_1121872360',\n",
    "    'hybas_1121872840', 'hybas_1121872870', 'hybas_1121873010', 'hybas_1121873350', 'hybas_1121873650', 'hybas_1121874520', 'hybas_1121874830', 'hybas_1121874880',\n",
    "    'hybas_1121875740', 'hybas_1121875790', 'hybas_1121876500', 'hybas_1121876780', 'hybas_1121876860', 'hybas_1121877040', 'hybas_1121877200', 'hybas_1121877770',\n",
    "    'hybas_1121877900', 'hybas_1121878070', 'hybas_1121878260', 'hybas_1121878870', 'hybas_1121879220', 'hybas_1121879830', 'hybas_1121879860', 'hybas_1121879870',\n",
    "    'hybas_1121879920', 'hybas_1121879970', 'hybas_1121879980', 'hybas_1121880270', 'hybas_1121880360', 'hybas_1121880500', 'hybas_1121880720', 'hybas_1121880730',\n",
    "    'hybas_1121880840', 'hybas_1121880970', 'hybas_1121881070', 'hybas_1121881080', 'hybas_1121881440', 'hybas_1121881700', 'hybas_1121882160', 'hybas_1121882230',\n",
    "    'hybas_1121882410', 'hybas_1121882790', 'hybas_1121882940', 'hybas_1121882950', 'hybas_1121883200', 'hybas_1121883210', 'hybas_1121883310', 'hybas_1121883900',\n",
    "    'hybas_1121884010', 'hybas_1121884390', 'hybas_1121884590', 'hybas_1121884990', 'hybas_1121885010', 'hybas_1121885080', 'hybas_1121885290', 'hybas_1121885370',\n",
    "    'hybas_1121885420', 'hybas_1121885620', 'hybas_1121886020', 'hybas_1121886030', 'hybas_1121886210', 'hybas_1121886260', 'hybas_1121886330', 'hybas_1121886340',\n",
    "    'hybas_1121887040', 'hybas_1121887260', 'hybas_1121887380', 'hybas_1121887460', 'hybas_1121887780', 'hybas_1121887790', 'hybas_1121887900', 'hybas_1121887990',\n",
    "    'hybas_1121888000', 'hybas_1121888050', 'hybas_1121888250', 'hybas_1121888350', 'hybas_1121888360', 'hybas_1121888470', 'hybas_1121888710', 'hybas_1121888730',\n",
    "    'hybas_1121888820', 'hybas_1121889070', 'hybas_1121889260', 'hybas_1121889380', 'hybas_1121889530', 'hybas_1121889650', 'hybas_1121889730', 'hybas_1121889810',\n",
    "    'hybas_1121890090', 'hybas_1121890100', 'hybas_1121890110', 'hybas_1121890140', 'hybas_1121890200', 'hybas_1121890500', 'hybas_1121890510', 'hybas_1121890720',\n",
    "    'hybas_1121890800', 'hybas_1121891110', 'hybas_1121891280', 'hybas_1121891450', 'hybas_1121891570', 'hybas_1121891870', 'hybas_1121891920', 'hybas_1121892020',\n",
    "    'hybas_1121892440', 'hybas_1121892630', 'hybas_1121892640', 'hybas_1121892850', 'hybas_1121892910', 'hybas_1121893090', 'hybas_1121893130', 'hybas_1121893280',\n",
    "    'hybas_1121893390', 'hybas_1121893580', 'hybas_1121893730', 'hybas_1121893740', 'hybas_1121893880', 'hybas_1121893950', 'hybas_1121894010', 'hybas_1121894460',\n",
    "    'hybas_1121894470', 'hybas_1121894900', 'hybas_1121894970', 'hybas_1121895430', 'hybas_1121895500', 'hybas_1121895580', 'hybas_1121895690', 'hybas_1121895840',\n",
    "    'hybas_1121895950', 'hybas_1121896100', 'hybas_1121896230', 'hybas_1121896320', 'hybas_1121896370', 'hybas_1121896390', 'hybas_1121896450', 'hybas_1121896810',\n",
    "    'hybas_1121896880', 'hybas_1121897330', 'hybas_1121897340', 'hybas_1121897430', 'hybas_1121897510', 'hybas_1121897620', 'hybas_1121897710', 'hybas_1121898220',\n",
    "    'hybas_1121898390', 'hybas_1121898520', 'hybas_1121898580', 'hybas_1121898650', 'hybas_1121898720', 'hybas_1121898800', 'hybas_1121898920', 'hybas_1121899050',\n",
    "    'hybas_1121899110', 'hybas_1121899210', 'hybas_1121899250', 'hybas_1121899510', 'hybas_1121899570', 'hybas_1121899800', 'hybas_1121900020', 'hybas_1121900350',\n",
    "    'hybas_1121900400', 'hybas_1121900510', 'hybas_1121900570', 'hybas_1121900700', 'hybas_1121900800', 'hybas_1121901020', 'hybas_1121901130', 'hybas_1121901200',\n",
    "    'hybas_1121901370', 'hybas_1121901670', 'hybas_1121901680', 'hybas_1121901820', 'hybas_1121901960', 'hybas_1121902020', 'hybas_1121902250', 'hybas_1121902260',\n",
    "    'hybas_1121902270', 'hybas_1121902350', 'hybas_1121902580', 'hybas_1121902750', 'hybas_1121902930', 'hybas_1121903010', 'hybas_1121903350', 'hybas_1121903540',\n",
    "    'hybas_1121903600', 'hybas_1121903900', 'hybas_1121904080', 'hybas_1121904180', 'hybas_1121904990', 'hybas_1121905090', 'hybas_1121905180', 'hybas_1121905290',\n",
    "    'hybas_1121905350', 'hybas_1121905590', 'hybas_1121905730', 'hybas_1121905800', 'hybas_1121905810', 'hybas_1121905840', 'hybas_1121906240', 'hybas_1121906460',\n",
    "    'hybas_1121906660', 'hybas_1121907090', 'hybas_1121907100', 'hybas_1121907480', 'hybas_1121907650', 'hybas_1121907760', 'hybas_1121907960', 'hybas_1121907970',\n",
    "    'hybas_1121907980', 'hybas_1121908100', 'hybas_1121908920', 'hybas_1121909060', 'hybas_1121909160', 'hybas_1121910100', 'hybas_1121910110', 'hybas_1121910560',\n",
    "    'hybas_1121910650', 'hybas_1121910970', 'hybas_1121910980', 'hybas_1121911290', 'hybas_1121911430', 'hybas_1121911680', 'hybas_1121911810', 'hybas_1121911950',\n",
    "    'hybas_1121912140', 'hybas_1121912180', 'hybas_1121912970', 'hybas_1121913580', 'hybas_1121914060', 'hybas_1121914080', 'hybas_1121914430', 'hybas_1121914440',\n",
    "    'hybas_1121914740', 'hybas_1121914860', 'hybas_1121914930', 'hybas_1121915130', 'hybas_1121915140', 'hybas_1121915150', 'hybas_1121915390', 'hybas_1121915430',\n",
    "    'hybas_1121915620', 'hybas_1121915900', 'hybas_1121915910', 'hybas_1121915970', 'hybas_1121916030', 'hybas_1121916060', 'hybas_1121916080', 'hybas_1121916250',\n",
    "    'hybas_1121916400', 'hybas_1121916500', 'hybas_1121916600', 'hybas_1121916680', 'hybas_1121916910', 'hybas_1121917110', 'hybas_1121917290', 'hybas_1121917300',\n",
    "    'hybas_1121917810', 'hybas_1121917830', 'hybas_1121917850', 'hybas_1121917860', 'hybas_1121918190', 'hybas_1121918200', 'hybas_1121918320', 'hybas_1121919510',\n",
    "    'hybas_1121919710', 'hybas_1121919720', 'hybas_1121919800', 'hybas_1121920380', 'hybas_1121920590', 'hybas_1121920790', 'hybas_1121921020', 'hybas_1121921100',\n",
    "    'hybas_1121921400', 'hybas_1121922550', 'hybas_1121922880', 'hybas_1121923140', 'hybas_1121923150', 'hybas_1121923300', 'hybas_1121924160', 'hybas_1121924260',\n",
    "    'hybas_1121924350', 'hybas_1121924610', 'hybas_1121924810', 'hybas_1121924970', 'hybas_1121925500', 'hybas_1121925740', 'hybas_1121926130', 'hybas_1121926190',\n",
    "    'hybas_1121926260', 'hybas_1121926670', 'hybas_1121927280', 'hybas_1121927360', 'hybas_1121927400', 'hybas_1121927640', 'hybas_1121927840', 'hybas_1121928280',\n",
    "    'hybas_1121928360', 'hybas_1121928500', 'hybas_1121928630', 'hybas_1121928830', 'hybas_1121929040', 'hybas_1121929140', 'hybas_1121929680', 'hybas_1121929850',\n",
    "    'hybas_1121929860', 'hybas_1121930330', 'hybas_1121930790', 'hybas_1121930800', 'hybas_1121931200', 'hybas_1121931460', 'hybas_1121931640', 'hybas_1121931980',\n",
    "    'hybas_1121932130', 'hybas_1121932420', 'hybas_1121932550', 'hybas_1121933410', 'hybas_1121933800', 'hybas_1121934500', 'hybas_1121934670', 'hybas_1121935030',\n",
    "    'hybas_1121935350', 'hybas_1121936790', 'hybas_1121937030', 'hybas_1121938800', 'hybas_1121939410', 'hybas_1121939420', 'hybas_1121939430', 'hybas_1121939720',\n",
    "    'hybas_1121939780', 'hybas_1121940130', 'hybas_1121940450', 'hybas_1121940880', 'hybas_1121942020', 'hybas_1121942570', 'hybas_1121942910', 'hybas_1121943580',\n",
    "    'hybas_1121943810', 'hybas_1121944140', 'hybas_1121944430', 'hybas_1121945470', 'hybas_1121947460', 'hybas_1121947780', 'hybas_1121948560', 'hybas_1121948660',\n",
    "]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" comment-block (4)\n",
    "import os\n",
    "\n",
    "local_directory = '/content/gauge_data'\n",
    "if not os.path.exists(local_directory):\n",
    "    os.makedirs(local_directory)\n",
    "                                # for every hybas: get data and store locally\n",
    "count = 0                       # in the directory /gauge_data/\n",
    "expected_count = len(Mali_hybases * 3)\n",
    "for hybas_id in Mali_hybases:\n",
    "  try:\n",
    "    gauge_reforecast_ds = reforecast_ds.sel(gauge_id = hybas_id).compute()\n",
    "    gauge_reanalysis_ds = reanalysis_ds.sel(gauge_id = hybas_id).compute()\n",
    "    gauge_return_periods_ds = return_periods_ds.sel(gauge_id = hybas_id).compute()\n",
    "\n",
    "    reforecast_file = f'{local_directory}/{hybas_id}_reforecast_ds.nc'\n",
    "    reanalysis_file = f'{local_directory}/{hybas_id}_reanalysis_ds.nc'\n",
    "    return_periods_file = f'{local_directory}/{hybas_id}_return_periods_ds.nc'\n",
    "\n",
    "    gauge_reforecast_ds.to_netcdf(reforecast_file)\n",
    "    gauge_reanalysis_ds.to_netcdf(reanalysis_file)\n",
    "    gauge_return_periods_ds.to_netcdf(return_periods_file)\n",
    "\n",
    "                                # check whether export OK\n",
    "    for file_path in [reforecast_file, reanalysis_file, return_periods_file]:\n",
    "      if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        if file_size > 0:\n",
    "          count += 1\n",
    "        else:\n",
    "          print(f'Warning: {file_path} is empty')\n",
    "      else:\n",
    "        print(f'Error: file {file_path} not found after saving')\n",
    "    print(f'progress: {count}/{expected_count}, {count/expected_count * 100:.2f}%, countMOD3 = {count % 3}')\n",
    "  except Exception as exc:\n",
    "      print(f'An error occurred while processing {hybas_id}: {exc}')\n",
    "\n",
    "print(f'\\n[actual/expected] downloaded files: {count}/{expected_count}')\n",
    "\n",
    "from google.colab import files\n",
    "zip_file = 'gauge_data.zip'\n",
    "\n",
    "if os.path.exists(local_directory):\n",
    "    print(f'Compressing files in {local_directory} into {zip_file}...')\n",
    "    !zip -r {zip_file} {local_directory}\n",
    "    print(f'Compression complete: {zip_file}')\n",
    "else:\n",
    "    print(f'Error: the directory {local_directory} does not exist')\n",
    "\n",
    "if os.path.exists(zip_file):  # download .zip file\n",
    "    files.download(zip_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data being downloaded and added to the directory, it can be classified into reforecast-, reanalysis-, and return periods datasets. Uncomment the cell below to handle this automatically (with the os/shutil package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "#                         # create subdirectories (if non-existent)\n",
    "# base_dir = '../data/GRRR'\n",
    "# source_dir = os.path.join(base_dir, 'gauge_data')\n",
    "# reanalysis_dir = os.path.join(base_dir, 'reanalysis')\n",
    "# reforecast_dir = os.path.join(base_dir, 'reforecast')\n",
    "# return_periods_dir = os.path.join(base_dir, 'return_periods')\n",
    "\n",
    "# goal_dirs = [reanalysis_dir, reforecast_dir, return_periods_dir]\n",
    "# for subdir in goal_dirs:\n",
    "#     if not os.path.exists(subdir):\n",
    "#         os.makedirs(subdir)\n",
    "#         print(f'Created directory: {subdir}')\n",
    "#                         # move files to respective subdirectories w/ shutil.move()\n",
    "# all_files = os.listdir(source_dir)\n",
    "# for filename in all_files:\n",
    "#     file_path = os.path.join(source_dir, filename)\n",
    "#                         # skip directories (although there should be none)\n",
    "#     if os.path.isdir(file_path):\n",
    "#         continue\n",
    "\n",
    "#     if filename.endswith('_reanalysis_ds.nc'):\n",
    "#         destination_dir = reanalysis_dir\n",
    "#     elif filename.endswith('_reforecast_ds.nc'):\n",
    "#         destination_dir = reforecast_dir\n",
    "#     elif filename.endswith('_return_periods_ds.nc'):\n",
    "#         destination_dir = return_periods_dir\n",
    "#     else:\n",
    "#         print(f'{filename} does not match any category')\n",
    "#         continue\n",
    "\n",
    "#                         # if it already exists, print and overwrite\n",
    "#     dest_file_path = os.path.join(destination_dir, filename)\n",
    "#     if os.path.exists(dest_file_path):\n",
    "#         os.remove(dest_file_path)\n",
    "#         print(f'Warning: replacing {filename} in {destination_dir}')\n",
    "\n",
    "#     shutil.move(file_path, destination_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import the datasets into working memory for manipulation and further processing. We put the three types into dictionaries, where the keys are the IDs and values the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, try one hybas_id to test:\n",
    "hybas_id = 'hybas_1120724680'\n",
    "\n",
    "ds_reanalysis_1120724680 = xr.open_dataset(f'../data/GRRR/reanalysis/{hybas_id}_reanalysis_ds.nc')\n",
    "ds_reforecast_1120724680 = xr.open_dataset(f'../data/GRRR/reforecast/{hybas_id}_reforecast_ds.nc')\n",
    "ds_return_ps_1120724680 = xr.open_dataset(f'../data/GRRR/return_periods/{hybas_id}_return_periods_ds.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(ds_reanalysis_1120724680))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_return_ps_1120724680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, download for all hybases:\n",
    "# dict_ds_data_Mali = {}\n",
    "dict_datasets = {}\n",
    "dict_return_periods = {}\n",
    "dict_reanalysis = {}\n",
    "\n",
    "for hybas in df_Mali_gauges['gaugeId']:\n",
    "    ds_reanalysis = xr.open_dataset(f'../data/GRRR/reanalysis/{hybas}_reanalysis_ds.nc')\n",
    "    ds_reforecast = xr.open_dataset(f'../data/GRRR/reforecast/{hybas}_reforecast_ds.nc')\n",
    "    ds_return_ps = xr.open_dataset(f'../data/GRRR/return_periods/{hybas}_return_periods_ds.nc')\n",
    "\n",
    "    # dict_ds_data_Mali[hybas] = {'reanalysis': ds_reanalysis,\n",
    "    #                             'reforecast': ds_reforecast,\n",
    "    #                             'return_periods': ds_return_ps}\n",
    "    # We download the reforecast analysis to evaluate forecasts,\n",
    "    # the return periods to see when certain thresholds are exceeded,\n",
    "    # and the reanalysis to recalculate the return period values\n",
    "    dict_datasets[f'{hybas}'] = ds_reforecast\n",
    "    dict_return_periods[hybas] = ds_return_ps\n",
    "    dict_reanalysis[hybas] = ds_reanalysis\n",
    "\n",
    "\n",
    "dict_datasets = {key.replace('hybas_', 'ds_reforecast_'): value \\\n",
    "                 for key, value in dict_datasets.items()}\n",
    "dict_reanalysis = {key.replace('hybas_', 'ds_reanalysis_'): value \\\n",
    "                   for key, value in dict_reanalysis.items()}\n",
    "dict_return_periods = {key.replace('hybas_', 'ds_return_ps_'): value \\\n",
    "                       for key, value in dict_return_periods.items()}\n",
    "\n",
    "# The lengths of the dictionaries should be equal to the number of gauges\n",
    "# ans also equal to each other. If not, downloading failed somewhere.\n",
    "print(len(dict_datasets))\n",
    "print(len(dict_return_periods))\n",
    "print(len(dict_reanalysis))\n",
    "# print(dict_datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the complete reforecast/reanalysis data for two locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_time_start_date = '2016-05-01'\n",
    "issue_time_end_date = '2024-10-30'\n",
    "\n",
    "analyse.plot_reforecast(issue_time_start_date, issue_time_end_date,\n",
    "                        ds_reforecast_1120724680, ds_return_ps_1120724680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_time_start_date = '1980-05-01'\n",
    "issue_time_end_date = '2024-10-30'\n",
    "\n",
    "analyse.plot_reanalysis(issue_time_start_date, issue_time_end_date,\n",
    "                        ds_reanalysis_1120724680, ds_return_ps_1120724680)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot all reanalysis data for Mali, 2016 - 2024:\n",
    "# issue_time_start_date = '2016-05-01'\n",
    "# issue_time_end_date = '2024-10-30'\n",
    "\n",
    "# for hybas_id in dict_ds_data_Mali.keys():\n",
    "#     ds_reanalysis = dict_ds_data_Mali[hybas_id]['reanalysis']\n",
    "#     ds_return_ps = dict_ds_data_Mali[hybas_id]['return_periods']\n",
    "\n",
    "#     analyse.plot_reanalysis(issue_time_start_date, issue_time_end_date,\n",
    "#                             ds_reanalysis, ds_return_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot all reforecast data for Mali, 2016 - 2024:\n",
    "# issue_time_start_date = '2016-05-01'\n",
    "# issue_time_end_date = '2024-10-30'\n",
    "\n",
    "# for hybas_id in dict_ds_data_Mali.keys():\n",
    "#     ds_reforecast = dict_ds_data_Mali[hybas_id]['reforecast']\n",
    "#     ds_return_ps = dict_ds_data_Mali[hybas_id]['return_periods']\n",
    "\n",
    "#     analyse.plot_reforecast(issue_time_start_date, issue_time_end_date,\n",
    "#                             ds_reforecast, ds_return_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_reanalysis_1120724680"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because not all gauges, i.e. most of the unverified ones, contain return period threshold (or in public phrasing in the UI (https://sites.research.google/floods/), \"danger level\") information, and because we want to analyse more thresholds than available, we recalculate the thresholds using the reanalysis data. In the official paper (https://www.nature.com/articles/s41586-024-07145-1), the threshold calculation is done using a Log-Pearson Type III fit; it remains ambiguous, however, on what data and how exactly. We recalculate, however, using a Gumbel fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from pyextremes import EVA\n",
    "# the final step before analysis is to calculate the return periods\n",
    "# for the reforecast data datasets using the reanalysis data. We do\n",
    "# this because not all reforecast data contain more than the 5-yr RP\n",
    "# in the return period datasets, and we need more of them for our analysis.\n",
    "# They can be calculated using a Gumbel distribution or Log-Pearson III\n",
    "def calculate_annual_max(ds: xr.Dataset) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Given a reanalysis dataset, it computes the annual maximum\n",
    "    discharge values for the dataset\n",
    "\n",
    "    :param ds: the reanalysis dataset\n",
    "    :return: the dataset with annual maximum values\n",
    "    \"\"\"\n",
    "    # exclude non-complete years from the reanalysis dataset, because\n",
    "    # these might not include the rain season, and, consequently, neither\n",
    "    # the annual water rises and/or floodings, and thus the annual maxima\n",
    "    # ... this subsetting procedure is moved elsewhere\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(ds['time']):\n",
    "        ds['time'] = pd.to_datetime(ds['time'].values)\n",
    "    return ds['streamflow'].resample(time = 'Y').max()\n",
    "\n",
    "\n",
    "def calculate_RP_Gumbel(ds: xr.Dataset, RP: float) -> float:\n",
    "    \"\"\"\n",
    "    Given a reanalysis dataset and a return period (in years),\n",
    "    it calculates the discharge value that corresponds to the\n",
    "    given return period using a Gumbel distribution fit\n",
    "\n",
    "    :param ds: the reanalysis dataset\n",
    "    :param RP: the return period in years\n",
    "    :return: the discharge value corresponding to the return period\n",
    "    \"\"\"                     \n",
    "    annual_max = calculate_annual_max(ds).to_series()\n",
    "    # print(annual_max)\n",
    "                            # fit a Gumbel distribution to maxima\n",
    "    loc, scale = stats.gumbel_r.fit(annual_max)\n",
    "    return stats.gumbel_r.ppf(1 - (1 / RP), loc = loc, scale = scale)\n",
    "\n",
    "\n",
    "def calculate_RP_LogPearsonIII(ds: xr.Dataset, RP: float) -> float:\n",
    "    \"\"\" \n",
    "    Given a reanalysis dataset and a return period (in years),\n",
    "    it calculates the discharge value that corresponds to the\n",
    "    given return period using a Log-Pearson Type-III dist. fit\n",
    "\n",
    "    :param ds: the reanalysis dataset\n",
    "    :param RP: the return period in years\n",
    "    :return: the discharge value corresponding to the return period\n",
    "    \"\"\"                     \n",
    "    annual_max = calculate_annual_max(ds).to_series()\n",
    "                            # log-transform the maxima and fit\n",
    "                            # a Log-Pearson Type-III distribution\n",
    "    shape, loc, scale = stats.pearson3.fit(np.log10(annual_max))\n",
    "                            # calculate z-value for the given return period,\n",
    "                            # and then the return period value by inverting\n",
    "    return 10 ** stats.pearson3.ppf(1 - (1 / RP), shape, loc = loc, scale = scale)\n",
    "\n",
    "\n",
    "def calculate_RP_GEV(ds: xr.Dataset, RP: float) -> float:\n",
    "    \"\"\" \n",
    "    Given a reanalysis dataset and a return period (in years),\n",
    "    it calculates the discharge value that corresponds to the\n",
    "    given return period using a Generalized Extreme Value dist. fit\n",
    "\n",
    "    :param ds: the reanalysis dataset\n",
    "    :param RP: the return period in years\n",
    "    :return: the discharge value corresponding to the return period\n",
    "    \"\"\"                     \n",
    "    annual_max = calculate_annual_max(ds).to_series()\n",
    "                            # fit a Generalized Extreme Value distribution to maxima\n",
    "    loc, scale, shape = stats.genextreme.fit(annual_max)\n",
    "                            #! Deze werkt niet goed, niet gebruiken\n",
    "    return stats.genextreme.ppf(1 - (1 / RP), loc = loc, scale = scale, c = shape)\n",
    "\n",
    "\n",
    "def calculate_RP_EVA(ds: xr.Dataset, RP: Any, method: str = 'GEV') -> float:\n",
    "    \"\"\" \n",
    "    Given a reanalysis dataset and a return period (in years),\n",
    "    it calculates the discharge value for the RP using the\n",
    "    pyextremes.eva.EVA package for a method of choice\n",
    "    \n",
    "    URL: https://georgebv.github.io/pyextremes/api/eva/\n",
    "\n",
    "    :param ds: the reanalysis dataset\n",
    "    :param RP: the return period(s) in years, as float or array\n",
    "    :param method: the method to use for the calculation\n",
    "    :return: the discharge value corresponding to the return period\n",
    "    \"\"\"\n",
    "    model = EVA(ds['streamflow'].to_series())\n",
    "\n",
    "    if method == 'GEV':    # Generalized Extreme Value distribution with:\n",
    "                           # block method for block maxima, default freq is annual\n",
    "        model.get_extremes(method = 'BM')\n",
    "        model.fit_model(distribution = 'genextreme')\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {method}')\n",
    "                            #! Deze werkt ook niet goed, niet gberuiken\n",
    "    return_value = model.get_return_value(return_period = RP)\n",
    "    # print(type(return_value))\n",
    "    # print(return_value)\n",
    "    return return_value[0]\n",
    "\n",
    "\n",
    "print('Gumbel and Log-Pearson III return periods for RP 2-20:')\n",
    "for idx in range(2, 21):\n",
    "    print(f'RP {idx}: {calculate_RP_Gumbel(ds_reanalysis_1120724680, idx):.0f}, '\n",
    "          f'{calculate_RP_LogPearsonIII(ds_reanalysis_1120724680, idx):.0f}, '\n",
    "          f'{calculate_RP_GEV(ds_reanalysis_1120724680, idx):.0f}, '\n",
    "          f'{calculate_RP_EVA(ds_reanalysis_1120724680, idx):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def add_return_periods_to_dataset(\n",
    "        ds_rf: xr.Dataset,\n",
    "        ds_ra: xr.Dataset,\n",
    "        method: str = 'Gumbel',\n",
    "        rps: List[str] = ['1.5', '2', '5', '7', '10', '20']\n",
    "    ) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Adds return period values to the dataset for the given return periods\n",
    "\n",
    "    :param ds_rf: the (reforecast) dataset to add return periods to\n",
    "    :param ds_ra: the (reanalysis) dataset to use for the calculations\n",
    "    :param method: the method to use for the calculation: either \n",
    "                   'Gumbel', 'Log-Pearson III', 'GEV', or 'EVA'\n",
    "    :param rps: the return periods to calculate, default is\n",
    "                [1.5, 2, 5, 7, 10, 20]\n",
    "    \"\"\"\n",
    "    if method not in ['Gumbel', 'Log-Pearson III', 'GEV', 'EVA']:\n",
    "        raise ValueError(f'Unknown method: {method}')\n",
    "    if method == 'Gumbel':\n",
    "        for rp in rps:\n",
    "            ds_rf.attrs[f'RP_{rp}'] = calculate_RP_Gumbel(ds_ra, float(rp))\n",
    "    elif method == 'Log-Pearson III':\n",
    "        for rp in rps:\n",
    "            ds_rf[f'RP_{rp}'] = calculate_RP_LogPearsonIII(ds_ra, float(rp))\n",
    "    elif method == 'GEV':\n",
    "        for rp in rps:\n",
    "            ds_rf[f'RP_{rp}'] = calculate_RP_GEV(ds_ra, float(rp))\n",
    "    elif method == 'EVA':\n",
    "        for rp in rps:\n",
    "            ds_rf[f'RP_{rp}'] = calculate_RP_EVA(ds_ra, float(rp))\n",
    "    return ds_rf\n",
    "\n",
    "\n",
    "def add_percentiles_to_dataset(\n",
    "        ds_rf: xr.Dataset,\n",
    "        ds_ra: xr.Dataset,\n",
    "        percentiles: List[int] = [95, 98, 99]\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Adds percentile values to the dataset for the given percentiles\n",
    "\n",
    "    :param ds_rf: the (reforecast) dataset to add percentiles to\n",
    "    :param ds_ra: the (reanalysis) dataset to use for the calculations\n",
    "    :param percentiles: the percentiles to calculate, default is\n",
    "                        [95, 98, 99]\n",
    "    \"\"\"\n",
    "    for perc in percentiles:\n",
    "        ds_rf.attrs[f'pc_{perc}th'] = np.percentile(ds_ra['streamflow'], perc)\n",
    "    return ds_rf\n",
    "    \n",
    "\n",
    "def get_common_IDs(\n",
    "        dict_rf: Dict[str, xr.Dataset],\n",
    "        dict_ra: Dict[str, xr.Dataset]\n",
    "    ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns the common IDs between the two dictionaries\n",
    "\n",
    "    :param dict_rf: dictionary with reforecast datasets\n",
    "    :param dict_ra: dictionary with reanalysis datasets\n",
    "    :return: the list of common IDs\n",
    "    \"\"\"\n",
    "    # extract only the ID part from the keys, e.g.\n",
    "    # ds_reforecast_hybas_1120724680 -> 1120724680; and\n",
    "    # ds_reanalysis_hybas_112072468 -> 1120724680, to match IDs\n",
    "    rf_IDs = [key.split('_')[-1] for key in dict_rf.keys()]\n",
    "    ra_IDs = [key.split('_')[-1] for key in dict_ra.keys()]\n",
    "    return list(set(rf_IDs).intersection(set(ra_IDs)))\n",
    "\n",
    "\n",
    "def assure_all_attributes_added(\n",
    "        dict_rf: Dict[str, xr.Dataset],\n",
    "        common_keys: List[str],\n",
    "        rps: List[str],\n",
    "        percentiles: List[int]\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Loops over all datasets and checks if all attributes were added\n",
    "\n",
    "    :param dict_rf: dictionary with reforecast datasets\n",
    "    :param common_keys: the common keys between the dictionaries\n",
    "    :param rps: the return periods to calculate\n",
    "    :param percentiles: the percentiles to calculate\n",
    "    \"\"\"\n",
    "    for key in common_keys:\n",
    "        ds_rf = dict_rf[f'ds_reforecast_{key}']\n",
    "        for rp in rps:\n",
    "            if f'RP_{rp}' not in ds_rf.attrs:\n",
    "                raise ValueError(f'RP_{rp} not added to {key}')\n",
    "        for perc in percentiles:\n",
    "            if f'pc_{perc}th' not in ds_rf.attrs:\n",
    "                raise ValueError(f'pc_{perc}th not added to {key}')\n",
    "\n",
    "\n",
    "def add_RPs_and_percentiles(\n",
    "        dict_rf: Dict[str, xr.Dataset],\n",
    "        dict_ra: Dict[str, xr.Dataset],\n",
    "        dict_rp: Dict[str, xr.Dataset],\n",
    "        method: str = 'Gumbel',\n",
    "        rps: List[str] = ['1.5', '2', '5', '7', '10', '20'],\n",
    "        percentiles: List[int] = [95, 98, 99],\n",
    "        verbose = True\n",
    ") -> Dict[str, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Helper function to add return periods and percentiles to all datasets\n",
    "\n",
    "    :param dict_rf: dictionary with reforecast datasets\n",
    "    :param dict_ra: dictionary with reanalysis datasets\n",
    "    :param dict_rp: dictionary with return periods datasets\n",
    "    :param method: the method to use for the calculation\n",
    "    :param rps: the return periods to calculate\n",
    "    :param percentiles: the percentiles to calculate\n",
    "    :param verbose: whether to print messages\n",
    "    :return: the updated dictionary with all datasets\n",
    "    \"\"\"\n",
    "    common_keys = get_common_IDs(dict_rf, dict_ra)\n",
    "    \n",
    "    if len(common_keys) == 0:\n",
    "        raise ValueError('No common keys found between the dictionaries')\n",
    "    if len(common_keys) != len(dict_rf) or len(common_keys) != len(dict_ra):\n",
    "        raise ValueError('Dictionaries have different lengths')\n",
    "\n",
    "                            # add the RPs and %'s\n",
    "    idx = 1\n",
    "    for key in common_keys:\n",
    "        if verbose:\n",
    "            if idx % 50 == 0:\n",
    "                print(f'{idx/len(common_keys)*100:.2f}% done')\n",
    "            idx += 1\n",
    "\n",
    "        ds_rf = dict_rf[f'ds_reforecast_{key}']\n",
    "        ds_ra = dict_ra[f'ds_reanalysis_{key}']\n",
    "        ds_rp = dict_rp[f'ds_return_ps_{key}']\n",
    "                            # add to reforecast dict datasets\n",
    "        dict_rf[f'ds_reforecast_{key}'] = add_return_periods_to_dataset(\n",
    "            ds_rf, ds_ra, method, rps\n",
    "        )\n",
    "        dict_rf[f'ds_reforecast_{key}'] = add_percentiles_to_dataset(\n",
    "            ds_rf, ds_ra, percentiles\n",
    "        )\n",
    "        #                     # add to return periods dict datasets after\n",
    "        #                     # prefixing old keys with 'old_'\n",
    "        # for attr_key in list(ds_rp.attrs.keys()):\n",
    "        #     if attr_key.startswith(\"return_period_\"):\n",
    "        #         ds_rp.attrs[f\"old_{attr_key}\"] = ds_rp.attrs.pop(attr_key)\n",
    "                            # add to return periods dict datasets\n",
    "        dict_rp[f'ds_return_ps_{key}'] = add_return_periods_to_dataset(\n",
    "            ds_rp, ds_ra, method, rps\n",
    "        )\n",
    "        dict_rp[f'ds_return_ps_{key}'] = add_percentiles_to_dataset(\n",
    "            ds_rp, ds_ra, percentiles\n",
    "        )\n",
    "\n",
    "                            # check if all attributes were added\n",
    "    assure_all_attributes_added(dict_rf, common_keys, rps, percentiles)\n",
    "\n",
    "    print('100% done')\n",
    "    return dict_rf, dict_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add return periods and percentiles to all datasets and create\n",
    "# a dictionary with all values recalculated (rc) for reference\n",
    "dict_datasets, dict_return_periods_rc = \\\n",
    "    add_RPs_and_percentiles(dict_datasets, dict_reanalysis, dict_return_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets['ds_reforecast_1120040380']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the return periods added as attributes, we can easily filter for some \"outliers\", or in lesser terms, just some datasets which are irrelevant for our purposes. As we will aggregate using the maximum, and do it for a theoretically practical response to major floods, we can safely discard gauges with a 5-year return period threshold discharge value of below 100 cubic meters per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare some of the calculated return periods with the actual values from Google\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "original_5YR_RP_values = [float(ds['return_period_5']) for ds in dict_return_periods.values() if\\\n",
    "                           'return_period_5' in ds]\n",
    "calculated_5YR_RP_values = [float(ds.attrs['RP_5']) for ds in dict_datasets.values() if\\\n",
    "                             'RP_5' in ds.attrs]\n",
    "df = pd.concat([pd.DataFrame({'rp': original_5YR_RP_values,\n",
    "                              'type': 'original'}),\n",
    "                pd.DataFrame({'rp': calculated_5YR_RP_values,\n",
    "                              'type': 'calculated'})])\n",
    "sns.histplot(data = df, x = 'rp', hue = 'type', alpha = 0.5, edgecolor = None)\n",
    "plt.axvline(x = 100, color = 'red', linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude all datasets where the 5-yr RP is below 100\n",
    "dict_datasets = {key: value for key, value in dict_datasets.items() if \\\n",
    "                    float(value.attrs['RP_5']) >= 100}\n",
    "dict_return_periods_rc = {key: value for key, value in dict_return_periods_rc.items() if \\\n",
    "                            float(value.attrs['RP_5']) >= 100}\n",
    "print(len(dict_datasets))\n",
    "print(len(dict_return_periods_rc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free up memory\n",
    "del dict_reanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checkpoint: adjust comparison type, lead time, or threshold from here**\n",
    "\n",
    "From here, with the datasets and various return period calculated, the script can be ran with different configurations to generate different kinds of plots and metrics, which will all be exported (except for the plots) to the `data/` folder. The options are:\n",
    "\n",
    "* For comparison type (**COMP_TYPE**): IMPACT or OBS (observation), giving a comparison to either events as entailed by the impact event data or observational timeseries data. Impact data is provided per administrative unit, and to accomodate this, the forecast data is aggregated to this level. Observation data is just station-based. Note that the comparison will differ in sample size due to differing data types and data sizes, and the fact that flood events are discarded based on some criteria from the used comparative dataset (see below).\n",
    "* For lead time (**LEAD_TIME**), the options are 1 to 7, since the GFH model does an autoregressive 7-day horizon forecast at each issue time.\n",
    "* For threshold (**THRESHOLD**), the options are 1.5-, 2-, 5-, 10-year return period thresholds, as calculated through the timeseries maxima, or the 95th, 98th, 99th percentile thresholds, as calculated through the mean. For OBSERVATION though, because we only have 2 years of overlap between the data, we discard more extreme threshold, i.e. the 10-year, 95th, and 99th one.\n",
    "\n",
    "One calculation can take up to 30 minutes, depending on whether some common precalculations are done already.\n",
    "```\n",
    "comp_types = ['IMPACT', 'OBS']\n",
    "thresholds = [1.5, 2, 5, 7, 10, 20, 95, 98, 99]\n",
    "lead_times = [1, 2, 3, 4, 5, 6, 7]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMP_TYPE = 'IMPACT'\n",
    "THRESHOLD = 5\n",
    "LEAD_TIME = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating forecasted/ground truth \"events\" and calculate overlap**\n",
    "\n",
    "Now that we have preprocessed (re)forecast datasets per gauge with a lead time of up to 7 days and with attributes such as coordinates and return period thresholds, we can process them further. We take all datasets in an administrative unit (level 2) and:\n",
    "* subset a certain lead time (LEAD_TIME) to analyse;\n",
    "* aggregate them using the maximum per timestep;\n",
    "* aggregate the return periods with the maximum per threshold; and\n",
    "* create new datasets per administrative unit with the aggregated timeseries and updated attributes.\n",
    "\n",
    "With datasets per admin unit ready, we loop through them to create \"flood events\": periods of consecutive flooding in an administrative unit. A flood event is \"triggered\" once the maximum chosen lead time (LEAD_TIME) predicted discharge in an administrative unit surpasses the return period threshold (THRESHOLD) for more than one day. \n",
    "\n",
    "Later, these can be compared to impact events (IMPACT) or observation events (OBSERVATION) to determine overlap, e.g. through boolean masking, and calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "# add a the qualityVerified flag from the ListGauges() API call\n",
    "# to the gauge datasets as attribute, for plotting and matching\n",
    "def add_quality_verified_flag(d_ds: Dict[str, xr.Dataset], df_flags: pd.DataFrame) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Adds an attribute to each dataset in the dictionary of datasets\n",
    "    from the column 'qualityVerified' in the DataFrame of flags,\n",
    "    corresponding to the matching 'gaugeId' column in the df.\n",
    "\n",
    "    Keys in the dict look like: dict_keys(['ds_reforecast_1120040380',\n",
    "    'ds_reforecast_1120550010', 'ds_reforecast_1120550020'])\n",
    "\n",
    "    and the gaugeId column like: hybas_1120040380\n",
    "\n",
    "    So the strings need to be adjusted to match.\n",
    "\n",
    "    :param d_ds: dictionary of datasets\n",
    "    :param df_flags: DataFrame with cols 'gaugeId' and 'qualityVerified'\n",
    "    :return: dictionary of datasets with added attribute 'qualityVerified'\n",
    "    \"\"\"\n",
    "    for key in d_ds.keys():\n",
    "        gauge_id = key.replace('ds_reforecast_', 'hybas_')\n",
    "        quality_verified = df_flags[df_flags['gaugeId'] == gauge_id]['qualityVerified'].values[0]\n",
    "        d_ds[key].attrs['qualityVerified'] = quality_verified\n",
    "\n",
    "    return d_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets = add_quality_verified_flag(dict_datasets, df_Mali_gauges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To automate the classification of the gauges into administrative units,\n",
    "# we first need to add coordinates to of every gauge dataset\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def index_country_gauge_coords(df_gauges: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return the DataFrame with gauge names and coordinates of a specific country\n",
    "\n",
    "    :param df_gauges: DataFrame with gauge information\n",
    "    :param country_name: Name of the country\n",
    "    :return: DataFrame with gauge names and coordinates of a specific country\n",
    "    \"\"\"\n",
    "    return df_gauges.set_index('gaugeId')[['latitude', 'longitude']]\n",
    "\n",
    "\n",
    "def export_country_gauge_coords(\n",
    "        df_gauges: pd.DataFrame, country_name: str = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Export gauge names and coordinates of a specific country to .csv.\n",
    "    Optionally prints them as well (default = False)\n",
    "\n",
    "    :param df_gauges: DataFrame with gauge information\n",
    "    :param country_name: Name of the country\n",
    "    \"\"\"\n",
    "    df_subset = index_country_gauge_coords(df_gauges)\n",
    "    df_subset.to_csv(f\"../data/processed/gauge_coords/{country_name}_gauge_coords.csv\",\n",
    "                     index = True,\n",
    "                     sep = ';',\n",
    "                     decimal = '.',\n",
    "                     encoding = 'utf-8')\n",
    "    \n",
    "\n",
    "def get_country_gauge_coords(country: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the coordinates of the gauges in a country, stored in data/ folder\n",
    "\n",
    "    :param country: name of the country\n",
    "    :return: DataFrame with the gaugeId, latitude and longitude\n",
    "    \"\"\"\n",
    "    if country[0].islower():\n",
    "        country = country.capitalize()\n",
    "    return pd.read_csv(f'../data/processed/gauge_coords/{country}_gauge_coords.csv',\n",
    "                       index_col = None, sep = ';', decimal = '.')\n",
    "\n",
    "\n",
    "def assign_coords_to_datasets(\n",
    "        datasets: Dict[str, xr.Dataset], country: str\n",
    "    ) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Takes a dict of datasets and assigns their coordinates, which it gets\n",
    "    from get_country_gauge_coords(), and assigns it to each dataset. The\n",
    "    dict contains the names of the datasets as keys and the datasets as values\n",
    "\n",
    "    :param datasets: dict of datasets\n",
    "    :param country: name of the country\n",
    "    :return: dict of datasets with coordinates\n",
    "    \"\"\"\n",
    "    df_coords = get_country_gauge_coords(country)\n",
    "\n",
    "    for gauge_id, dataset in datasets.items():\n",
    "        # assumes full name, e.g. 'hybas_1120661040', in df_coords, thus creating\n",
    "        # a comparison of solely the hybas numbers, not the full name or dataset identifier\n",
    "        coords = df_coords[\n",
    "            df_coords['gaugeId'].apply(lambda x: x.split('_')[-1]) == gauge_id.split('_')[-1]\n",
    "        ]\n",
    "\n",
    "        if not coords.empty:\n",
    "            # add the coordinates to the dataset as attributes\n",
    "            dataset.attrs['latitude'] = coords['latitude'].values[0]\n",
    "            dataset.attrs['longitude'] = coords['longitude'].values[0]\n",
    "            # add the hybas_id to the dataset as well (e.g. '1120661040')\n",
    "            dataset.attrs['gauge_id'] = gauge_id.split('_')[-1]\n",
    "        else:\n",
    "            print(f'No coordinates found for gauge {gauge_id}') \n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the coordinates added, we can classify the gauges into administrative units\n",
    "import analyse\n",
    "from typing import Dict\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def create_coords_df_from_ds(dict_ds: Dict[str, xr.Dataset]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame with all gauge ID's and coordinates in a\n",
    "    dictionary with xarray Datasets\n",
    "\n",
    "    :param dict_ds: xarray Dataset\n",
    "    :return: DataFrame with the coordinates\n",
    "    \"\"\"\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            'gauge_id': ds.attrs['gauge_id'],\n",
    "            'longitude': ds.attrs['longitude'],\n",
    "            'latitude': ds.attrs['latitude']\n",
    "        }\n",
    "        for ds in dict_ds.values()\n",
    "    ])\n",
    "\n",
    "\n",
    "def assure_admin_units_assigned(dict_ds: Dict[str, xr.Dataset]) -> None:\n",
    "    \"\"\"\n",
    "    Check if all datasets have been assigned an admin unit\n",
    "\n",
    "    :param dict_ds: dict of datasets\n",
    "    \"\"\"\n",
    "    for ds in dict_ds.values():\n",
    "        if 'admin_unit' not in ds.attrs:\n",
    "            print(f'No admin unit assigned to dataset {ds.attrs[\"gauge_id\"]}')\n",
    "            continue\n",
    "        admin_unit = ds.attrs['admin_unit']\n",
    "        if any(pd.isna(unit) for unit in admin_unit):\n",
    "            print(f'No admin unit assigned to dataset {ds.attrs[\"gauge_id\"]} (NaN found)')\n",
    "\n",
    "\n",
    "def handle_NaN_admin_units(codes: pd.Series) -> list:\n",
    "    \"\"\"\n",
    "    Handle NaN values in the series of administrative unit assignments for gauges\n",
    "\n",
    "    :param codes: Series containing administrative unit codes (ADM2_PCODE) for a gauge\n",
    "    :return: list of valid administrative unit codes (ADM2_PCODE)\n",
    "    \"\"\"\n",
    "    if codes.isna().any():\n",
    "        print(f\"Warning: Found NaN in administrative unit assignment \"\n",
    "              f\"for gauge IDs: {codes[codes.isna()].index.tolist()}\")\n",
    "    return list(codes.dropna())\n",
    "\n",
    "\n",
    "def assign_admin_unit_to_datasets(\n",
    "        dict_ds: Dict[str, xr.Dataset],\n",
    "        country: str = 'Mali',\n",
    "        verbose: bool = False,\n",
    "        # Path to the shape file with admin level 2 units for Mali\n",
    "        path: str = 'mali_ALL/mli_adm_ab_shp/mli_admbnda_adm2_1m_gov_20211220.shp',\n",
    "        buffer_radius: int = 5000\n",
    "    ) -> Dict[str, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Assigns the administrative unit to each dataset in the dictionary by:\n",
    "    (1) assiging coordinates to the datasets, with assign_coords_to_datasets(),\n",
    "        which takes information queried by ListGauges() in the 'extract' package\n",
    "    (2) creating a GeoDataFrame from the dataset coordinates, which includes a\n",
    "        5 km buffer around the gauges, to account for shape file inaccuracies and,\n",
    "        more importantly, the fact that gauges are usually located in rivers, which,\n",
    "        in turn, are usually borders between administrative units, causing gauges to\n",
    "        be located in only onr of the units, while they effectively tell about both.\n",
    "        With a buffer, this is accounted for, and, as a result, gauges can be assigned\n",
    "        to multiple administrative units, if they simply intersect with multiple units\n",
    "    (3) creating a GeoDataFrame from the shape file with the admin units, source:\n",
    "        (https://www.naturalearthdata.com/downloads/10m-cultural-vectors/10m-admin-0-countries/)\n",
    "    (4) classifying the gauges into the administrative units by joining the two above\n",
    "    (5) adding the admin unit names to the datasets by matching the gauge ID's and\n",
    "        the found administrative units\n",
    "    (6) returning the updated dictionary with the datasets, with datasets that now have\n",
    "        the attributes 'longitude', 'latitude', and 'admin_unit'\n",
    "\n",
    "    :param dict_ds: dict of datasets\n",
    "    :param country: name of the country\n",
    "    :param verbose: whether to print some test print-s's\n",
    "    :param path: path to the shape file with the admin units\n",
    "    :param buffer_radius: radius of the buffer around the gauges, standard is 5 km\n",
    "    :return: dict of datasets with administrative units\n",
    "    \"\"\"\n",
    "    #* (1): assign coordinates to the datasets\n",
    "    dict_ds = assign_coords_to_datasets(dict_ds, country)\n",
    "    # print(next(iter(dict_ds.items()))) if verbose else None\n",
    "\n",
    "    #* (2): create a GeoDataFrame from the dataset coordinates;\n",
    "    # geometry is a point for each gauge, with coords (x, y)\n",
    "    df_gauge_coords = create_coords_df_from_ds(dict_ds)\n",
    "    gpd_Mali_gauge_coords = gpd.GeoDataFrame(\n",
    "        df_gauge_coords,\n",
    "        geometry = gpd.points_from_xy(\n",
    "            df_gauge_coords['longitude'], df_gauge_coords['latitude']\n",
    "        ),\n",
    "        crs = 'EPSG:4326'\n",
    "    )\n",
    "    # add a buffer of 5 km around the points to account for inaccuracies,\n",
    "    # where 1 degree is approx. 111,32 km at the equator, so in degrees (which\n",
    "    # we have to use as the coordinate system is WGS84), 5 km is 5000 meter \\\n",
    "    # divided by 111.320 meters (5000 / 111320). This is a rough estimate, but\n",
    "    # should be sufficient, since the number of 5 km is too mostly arbirtrary.\n",
    "    # Also, surpress the warning that the buffer is not exact, as we are aware\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gpd_Mali_gauge_coords['geometry'] = \\\n",
    "            gpd_Mali_gauge_coords.geometry.buffer(buffer_radius / 111320)\n",
    "\n",
    "    #* (3): read the shape file into a GeoDataFrame and convert it to WGS84\n",
    "    # (which is the coordinate system used by the gauge data)\n",
    "    gpd_adm_units_Mali = analyse.get_shape_file(path).to_crs('EPSG:4326')\n",
    "    # check if the coord systems are the same\n",
    "    if gpd_adm_units_Mali.crs != gpd_Mali_gauge_coords.crs:\n",
    "        gpd_adm_units_Mali = gpd_adm_units_Mali.to_crs(gpd_Mali_gauge_coords.crs)\n",
    "    analyse.assert_same_coord_system(gpd_adm_units_Mali, gpd_Mali_gauge_coords)\n",
    "\n",
    "    #* (4) now we can classify the gauges into the administrative units:\n",
    "    # creating a joined dataframe with the gauges as basis, meaning\n",
    "    # that gauges get assigned to the admin unit they are within,\n",
    "    # including their metadata (such as the shape of the admin unit).\n",
    "    # (And, thus, the rest of the admin units are not considered.)\n",
    "    gpd_gauges_classified = gpd.sjoin(\n",
    "        gpd_Mali_gauge_coords, gpd_adm_units_Mali,\n",
    "        how = 'left',           # joins left, i.e. the gauges serve as basis\n",
    "                                # checks if the gauge intersects with the admin unit\n",
    "        predicate = 'intersects',\n",
    "        lsuffix = 'gauge', rsuffix = 'adm'\n",
    "    )\n",
    "    print(gpd_gauges_classified.head(1)) if verbose else None\n",
    "    # make a mapping of the gauge ID's and the admin unit names:\n",
    "    # group by gauge ID; select the admin unit names; check for NaNs;\n",
    "    # convert to list; then dictionary with {gauge_id: [admin_unit]}\n",
    "    mapping = gpd_gauges_classified.groupby('gauge_id')['ADM2_PCODE']\\\n",
    "        .apply(handle_NaN_admin_units).to_dict()\n",
    "\n",
    "    #* (5) lastly, we add the admin unit names to the datasets\n",
    "    print(mapping) if verbose else None\n",
    "    for gauge_id, admin_units in mapping.items():\n",
    "        dict_ds[f'ds_reforecast_{gauge_id}'].attrs['admin_unit'] = admin_units\n",
    "\n",
    "    #* (6) check result and return\n",
    "    assure_admin_units_assigned(dict_ds)\n",
    "    print('\\n\\n', next(iter(dict_ds.items()))) if verbose else None\n",
    "    return dict_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using coordinates gathered through the API, the GRRR dataset, and shape files for Mali's administrative level 2, we assign each gauge dataset an admin unit. A caveat here is that gauges are often located in or near rivers, and that rivers (and waterbodies in general) often form borders between certain regions. In this case, administrative units of level 2. As such, an on-border gauge will arbitrarily fall into one of its nearby regions, while a flood at its location will affect all its nearby regions. To account for this, we change a gauge's location from a point to a circle with a radius of 5 km, and all regions that overlap are considered a match.\n",
    "\n",
    "After this step, the datasets contain a:\n",
    "* **gauge ID**: unique identifier;\n",
    "* **quality flag**: whether the gauge's forecasts are \"verified\" or \"unverified\"\n",
    "* **administrative unit ID**: the administrative units te gauge falls into; and\n",
    "* **coordinates**: a latitude and longitude,\n",
    "\n",
    "which are assured below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure country gauge coords are available and then assign them\n",
    "export_country_gauge_coords(df_Mali_gauges, 'Mali')\n",
    "\n",
    "dict_datasets = assign_admin_unit_to_datasets(dict_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all datasets in dict_datasets have been assigned the \n",
    "# attributes: 'longitude', 'latitude', and 'admin_unit'\n",
    "def assure_admin_units_assigned(dict_datasets: Dict[str, xr.Dataset]):\n",
    "    \"\"\" \n",
    "    Check if all datasets have been assigned an admin unit,\n",
    "    including coordinates, gauge_id, and qualityVerified\n",
    "\n",
    "    :param dict_datasets: dict of datasets\n",
    "    \"\"\"\n",
    "    for ds in dict_datasets.values():\n",
    "        assert 'gauge_id' in ds.attrs, f'No gauge ID assigned to dataset {ds}'\n",
    "        assert 'qualityVerified' in ds.attrs, f'No qualityVerified assigned to dataset {ds.attrs[\"gauge_id\"]}'\n",
    "        assert 'admin_unit' in ds.attrs, f'No admin unit assigned to dataset {ds.attrs[\"gauge_id\"]}'\n",
    "        assert 'latitude' in ds.attrs, f'No latitude assigned to dataset {ds.attrs[\"gauge_id\"]}'\n",
    "        assert 'longitude' in ds.attrs, f'No longitude assigned to dataset {ds.attrs[\"gauge_id\"]}'\n",
    "\n",
    "\n",
    "assure_admin_units_assigned(dict_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_datasets['ds_reforecast_1120766460'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_DNH_station_coords(\n",
    "        path = '../data/processed/gauge_coords/DNH_station_coords.csv'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Import the coordinates of the DNH stations from the .csv file\n",
    "\n",
    "    :param path: path to the .csv file with the DNH station coordinates\n",
    "    :return: DataFrame with the DNH station coordinates\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, index_col = None, sep = ',', decimal = '.')\n",
    "    df = df.rename(columns = {'Lat': 'latitude',\n",
    "                              'Lon': 'longitude',\n",
    "                              'Station names': 'name'})\n",
    "    df['name'] = df['name'].str.strip()\n",
    "    return df.drop(columns = 'Catchment area (km2)')\n",
    "\n",
    "\n",
    "df_DNH_station_coords = import_DNH_station_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting purposes, we find the gauge ID that is placed in the\n",
    "# admin unit of Bamako, in the city named Bamako, Mali's capital.\n",
    "# The pcode of admin unit is ML0901, and the river gauge ID 1120714900.\n",
    "ds_Bamako = dict_datasets['ds_reforecast_1120714900']\n",
    "df_Bamako = ds_Bamako['streamflow'].to_dataframe()\n",
    "df_Bamako = df_Bamako.drop(columns = 'gauge_id')\n",
    "df_Bamako = df_Bamako.unstack(level = 'lead_time')\n",
    "df_Bamako.columns = df_Bamako.columns.droplevel()\n",
    "df_Bamako.index = pd.to_datetime(df_Bamako.index)\n",
    "df_Bamako.to_csv('../data/processed/Bamako/Bamako_streamflow.csv')\n",
    "df_Bamako"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def plot_admin_units_with_gauges(\n",
    "        dict_ds: Dict[str, xr.Dataset],\n",
    "        add_dots: bool = False,\n",
    "        add_quality_verified: bool = False,\n",
    "        add_DNH_stations: bool = False,\n",
    "        df_DNH_stations: pd.DataFrame = None,\n",
    "        path = 'mali_ALL/mli_adm_ab_shp/mli_admbnda_adm2_1m_gov_20211220.shp'\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plot the administrative units of a country with the gauges contained in them\n",
    "\n",
    "    :param dict_ds: dict of gauge datasets\n",
    "    :param add_dots: whether to add dots for the gauges\n",
    "    :param add_quality_verified: whether to distinct between (un)verified gauges\n",
    "    :param add_DNH_stations: whether to add the DNH stations to the plot\n",
    "    :param path: path to the shape file with the admin units\n",
    "    \"\"\"\n",
    "    # create a GeoDataFrame with the coordinates of the gauges\n",
    "    # (code copied from assign_admin_unit_to_datasets())\n",
    "    df_gauge_coords = create_coords_df_from_ds(dict_ds)\n",
    "    gpd_Mali_gauge_coords = gpd.GeoDataFrame(\n",
    "        df_gauge_coords,\n",
    "        geometry = gpd.points_from_xy(\n",
    "            df_gauge_coords['longitude'], df_gauge_coords['latitude']\n",
    "        ),\n",
    "        crs = 'EPSG:4326'\n",
    "    )\n",
    "    # add a buffer of 5 km around the points to account for inaccuracies,\n",
    "    # where 1 degree is approx. 111,32 km at the equator, so in degrees (which\n",
    "    # we have to use as the coordinate system is WGS84), 5 km is 5000 meter \\\n",
    "    # divided by 111.320 meters (5000 / 111320). This is a rough estimate, but\n",
    "    # should be sufficient, since the number of 5 km is too mostly arbirtrary.\n",
    "    # Also, surpress the warning that the buffer is not exact, as we are aware\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gpd_Mali_gauge_coords['geometry'] = \\\n",
    "            gpd_Mali_gauge_coords.geometry.buffer(5000 / 111320)\n",
    "        \n",
    "    # make a GeoDataFrame with the administrative units\n",
    "    gpd_adm_units_Mali = analyse.get_shape_file(path).to_crs('EPSG:4326')\n",
    "    # A quick intermediary plot to see which admin units are contained in the datasets:\n",
    "    # We want to make a plot where each administrative unit with a gauge gets a tinted\n",
    "    # colour. Thus, we now join the GeoDataFrames the other way around\n",
    "    gpd_adm_units_with_gauges = gpd.sjoin(\n",
    "        gpd_adm_units_Mali, gpd_Mali_gauge_coords,\n",
    "        how = 'left',\n",
    "        predicate = 'intersects', # see if they intersect with one another\n",
    "        lsuffix = 'adm', rsuffix = 'gauge'\n",
    "    )\n",
    "\n",
    "    print('Administrative units in Mali:', len(gpd_adm_units_Mali))\n",
    "    # Drop the column where index_gauge (aka index_right) is NaN,\n",
    "    # because that means that the admin unit does not contain a gauge\n",
    "    gpd_adm_units_with_gauges = gpd_adm_units_with_gauges.dropna(subset = ['index_gauge'])\n",
    "    # 15 gauges left, so there's one administrative unit with two gauges\n",
    "    print('Administrative units with a gauge: ', end = '')\n",
    "    print(len(gpd_adm_units_with_gauges['ADM2_PCODE'].unique()))\n",
    "    # Plot using 510 colours\n",
    "    red, blue, dot_c, green, yellow = '#DB0A13', '#092448', '#C6E7FF', '#118B50', '#FFE31A'\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    gpd_adm_units_Mali.plot(ax = ax, color = blue, edgecolor = 'black', linewidth = 0.5)\n",
    "    gpd_adm_units_with_gauges.plot(ax = ax, color = red, edgecolor = 'black', linewidth = 0.5)\n",
    "    # Add legend handles as list (which gives the option to add more later)\n",
    "    legend_handles = [\n",
    "        Patch(color = red, label = 'AU with gauge'),\n",
    "        Patch(color = blue, label = 'AU without gauge'),\n",
    "    ]\n",
    "    # If add dots, add all coordinates of the gauges as dots, including as legend;\n",
    "    # if add_quality_verified, add a different colour for the qualityVerified gauges\n",
    "    if add_dots:\n",
    "        latitudes = [ds.latitude for ds in dict_ds.values()]\n",
    "        longitudes = [ds.longitude for ds in dict_ds.values()]\n",
    "        if add_quality_verified:\n",
    "            quality_verified = [ds.qualityVerified for ds in dict_ds.values()]\n",
    "            x_verified = [lon for lon, qv in zip(longitudes, quality_verified) if qv]\n",
    "            y_verified = [lat for lat, qv in zip(latitudes, quality_verified) if qv]\n",
    "            x_unverified = [lon for lon, qv in zip(longitudes, quality_verified) if not qv]\n",
    "            y_unverified = [lat for lat, qv in zip(latitudes, quality_verified) if not qv]\n",
    "            ax.scatter(x_unverified, y_unverified, color = dot_c, s = 6, alpha = 1)\n",
    "            ax.scatter(x_verified, y_verified, color = green, s = 22, alpha = 1)\n",
    "            legend_handles.extend([\n",
    "                Line2D([0], [0], marker = 'o', color = green, markersize = 5, alpha = 1, linestyle = '',\n",
    "                          label = 'Verified gauge'),\n",
    "                Line2D([0], [0], marker = 'o', color = dot_c, markersize = 5, alpha = 1, linestyle = '',\n",
    "                            label = 'Unverified gauge')\n",
    "                # Patch(color = green, label = 'Verified gauge'),\n",
    "                # Patch(color = dot_c, label = 'Unverified gauge')\n",
    "            ])\n",
    "        else:\n",
    "            ax.scatter(longitudes, latitudes, color = dot_c, s = 6, alpha = 1)\n",
    "            legend_handles.append(\n",
    "                Line2D([0], [0], marker = 'o', color = dot_c, markersize = 5, alpha = 1, linestyle = '',\n",
    "                       label = 'FloodHub gauges')\n",
    "            )\n",
    "\n",
    "        if add_DNH_stations:\n",
    "            gpd_DNH_stations = gpd.GeoDataFrame(df_DNH_stations,\n",
    "                                                geometry = gpd.points_from_xy(\n",
    "                                                    df_DNH_stations['longitude'], df_DNH_stations['latitude']\n",
    "                                                ), crs = 'EPSG:4326')\n",
    "            ax.scatter(\n",
    "                gpd_DNH_stations.geometry.x, gpd_DNH_stations.geometry.y,\n",
    "                color = yellow, s = 22, alpha = 1, marker = 'D'\n",
    "            )\n",
    "            legend_handles.append(\n",
    "                Line2D([0], [0], marker = 'D', color = yellow, markersize = 5, alpha = 1, linestyle = '',\n",
    "                       label = 'DNH stations')\n",
    "            )\n",
    "            \n",
    "            # # create 10 km buffer around the DNH stations    \n",
    "            # gpd_buffer = gpd.GeoDataFrame(geometry = gpd_DNH_stations.geometry.buffer(10 / 111320),\n",
    "            #                               crs = 'EPSG:4326')\n",
    "            # gpd_buffer.plot(ax = ax, facecolor = 'none', edgecolor = yellow,\n",
    "            #                 linewidth = 1, alpha = 1)\n",
    "            # legend_handles.append(\n",
    "            #     Patch(facecolor = 'none', edgecolor = yellow, linewidth = 1, alpha = 0.5, label = f'10 km buffer')\n",
    "            # )\n",
    "\n",
    "    # This comment is now irrelevant but kept for reference (e.g. to use as example in paper)\n",
    "    # # The coordinates of gauge 1120766460 seem to be slightly off, and\n",
    "    # # coincidentally, it is placed outside of Mali and not within an admin unit   \n",
    "    # # ax.plot(-8.485416666669321, 11.22291666666553, 'go', markersize = 2)\n",
    "    ax.legend(handles = legend_handles, loc = 'upper right')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('Administrative units with gauges in Mali')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_admin_units_with_gauges(dict_datasets, True, True, True, df_DNH_station_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we match DNH stations to either a verified unit within a radius of\n",
    "# 10 km, or else, to the closest gauge available. We export a mapping.\n",
    "from scipy.spatial import cKDTree\n",
    "import json\n",
    "\n",
    "\n",
    "def haversine(\n",
    "        lon1: float, lat1: float, lon2: float, lat2: float\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees);\n",
    "    https://stackoverflow.com/questions/4913349/haversine-formula\n",
    "    -in-python-bearing-and-distance-between-two-gps-points\n",
    "\n",
    "    :param lon1: longitude of point 1\n",
    "    :param lat1: latitude of point 1\n",
    "    :param lon2: longitude of point 2\n",
    "    :param lat2: latitude of point 2\n",
    "    :return: distance in km\n",
    "    \"\"\"\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * \\\n",
    "        np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return 6367 * c\n",
    "\n",
    "\n",
    "def match_DNH_stations_to_gauges(\n",
    "        df_DNH: pd.DataFrame,\n",
    "        dict_gauges: Dict[str, xr.Dataset],\n",
    "        radius: int = 10\n",
    "    ) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Match DNH stations to the closest gauges using a KDTree O(NlogN)\n",
    "    if no verified gauge is available within a radius of 10 km\n",
    "\n",
    "    :param df_DNH: DataFrame with the DNH station coordinates\n",
    "    :param dict_gauges: the dictionary with gauge datasets\n",
    "    :param radius: the radius to search for verified gauges\n",
    "    :return: mapping of DNH to closest gauge\n",
    "    \"\"\"\n",
    "    verified_gauges = [gauge_id for gauge_id, ds in dict_gauges.items() \\\n",
    "            if ds.attrs['qualityVerified'] == True]\n",
    "    verified_tree = cKDTree(np.array([\n",
    "        [dict_gauges[g].attrs['latitude'], dict_gauges[g].attrs['longitude']] \\\n",
    "            for g in verified_gauges\n",
    "    ]))\n",
    "    # for every DNH station, see if a gauge is within 10 km radius by calculating th\n",
    "    # radius of a 10 km circle (without adjusting for the earth's curvature) and then\n",
    "    # seeing if there is a verified gauge within that Euclidian distance. If not, just\n",
    "    # pick the closest one from the normal Tree\n",
    "    all_gauges = list(dict_gauges.keys())\n",
    "    normal_tree = cKDTree(np.array([\n",
    "        [dict_gauges[g].attrs['latitude'], dict_gauges[g].attrs['longitude']] \\\n",
    "        for g in all_gauges\n",
    "    ]))\n",
    "    \n",
    "    mapping_to_PCODE = {}       # make a mapping to admin unit just for later; and\n",
    "    mapping = {}                # for every DNH station, get a verified gauge\n",
    "                                # from the verified tree, see if it's within\n",
    "                                # the radius, else get one from the normal tree \n",
    "    for _, row in df_DNH.iterrows():\n",
    "        coords = np.array([row['latitude'], row['longitude']])\n",
    "        distances, idx = verified_tree.query(coords, k = 1)\n",
    "        # print(distances, idx)\n",
    "\n",
    "        g_id = verified_gauges[idx]\n",
    "        lat = dict_gauges[verified_gauges[idx]].attrs['latitude']\n",
    "        lon = dict_gauges[verified_gauges[idx]].attrs['longitude']\n",
    "                                # calculate the distance in km using haversine's formula,\n",
    "                                # where we omit the earth's curvature for simplicity\n",
    "        dist_km = haversine(lon, lat, row['longitude'], row['latitude'])\n",
    "\n",
    "        if dist_km <= radius:\n",
    "            closest_gauge_id = g_id\n",
    "            print('Found verified gauge within radius')\n",
    "        else:\n",
    "            _, idx = normal_tree.query(coords, k = 1)\n",
    "            closest_gauge_id = all_gauges[idx]\n",
    "\n",
    "        mapping[row['name']] = closest_gauge_id\n",
    "        mapping_to_PCODE[row['name']] = dict_gauges[closest_gauge_id].attrs['admin_unit']\n",
    "\n",
    "    with open('../data/mappings/DNH_to_gauge.json', 'w') as f:\n",
    "        json.dump(mapping, f, indent = 4)\n",
    "    with open('../data/mappings/DNH_to_PCODE.json', 'w') as f:\n",
    "        json.dump(mapping_to_PCODE, f, indent = 4)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the comparison with observational data, we do not aggregate per administrative unit, but just compare point-to-point. For every DNH station location, we search for any verified gauge within 10 km (with the gauge now as point again, not a circle with a radius of 5 km used in the administrative unit assignment), and match this one if available. If not, we take the nearest unverified gauge. This way, give a slight prioritization to verified gauges. This results in a subset of the forecast data with similar locations and thus good, theoretical, comparative quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_DNH = match_DNH_stations_to_gauges(df_DNH_station_coords, dict_datasets, 10)\n",
    "n_verified = len([g for g in mapping_DNH.values() if g in [ds for ds in dict_datasets.keys() if\\\n",
    "                                                            dict_datasets[ds].attrs['qualityVerified']]])\n",
    "print(f'Of the {len(mapping_DNH)} DNH stations, {n_verified} are matched to verified gauges\\n')\n",
    "print(mapping_DNH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_datasets['ds_reforecast_1121939410']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_datasets['ds_reforecast_1120766460']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the datasets classified into admin units, we will now aggregate\n",
    "# the data per admin unit and create a dataset with just the maximum value of\n",
    "# the reforecast data at each possible timestep available in the datasets for\n",
    "# each admin units (resulting in one dataset per admin unit).\n",
    "#   This is done specifically for the comparison with impact data, which is provided\n",
    "# per admin unit, in contrast to observation data, which is per station.\n",
    "import os\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "\n",
    "def export_datasets_to_netcdf(\n",
    "        d_ds: Dict[str, xr.Dataset], path: str, lt: int\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Export a dataset to a netCDF file\n",
    "\n",
    "    :param d_ds: dict of datasets\n",
    "    :param path: path to the file\n",
    "    :param lt: lead time of the dataset\n",
    "    \"\"\"\n",
    "    for key, ds in d_ds.items():\n",
    "        try:\n",
    "            if os.path.dirname(path) and not os.path.exists(os.path.dirname(path)):\n",
    "                os.makedirs(os.path.dirname(path), exist_ok = True)\n",
    "            ds.to_netcdf(f'{path}{key}_{lt * 24}lt.nc')\n",
    "        except FileNotFoundError as fnf_error:\n",
    "            print(f\"File not found error: {fnf_error}\")\n",
    "        except PermissionError as perm_error:\n",
    "            print(f\"Permission error: {perm_error}\")\n",
    "        except TypeError as type_error:\n",
    "            print(f\"Type error: {type_error}\")\n",
    "        except Exception as exc:\n",
    "            print(f\"Failed to dataset to '{path}': {exc}\")\n",
    "\n",
    "\n",
    "def import_datasets_from_netcdf(path: str, lt: int) -> Dict[str, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Import a dictionary of datasets from a folder with netCDF files:\n",
    "    the inverse of export_datasets_to_netcdf()\n",
    "\n",
    "    :param path: path to the files\n",
    "    :param lt: lead time of the datasets to import\n",
    "    :return: dict of datasets\n",
    "    \"\"\"\n",
    "    d_ds = {}\n",
    "    if not os.path.exists(path):\n",
    "        raise ValueError('Path does not exist')\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(f'_{lt * 24}lt.nc'):\n",
    "            key = filename.split('_')[0]\n",
    "            file_path = os.path.join(path, filename)\n",
    "            try:\n",
    "                ds = xr.open_dataset(file_path)\n",
    "                d_ds[key] = ds\n",
    "            except Exception as exc:\n",
    "                print(f\"Failed to import dataset from '{file_path}': {exc}\")\n",
    "    return d_ds\n",
    "\n",
    "\n",
    "# def get_upstream_gauge_return_period(\n",
    "#         admin_unit: str,\n",
    "#         dict_ds_au: Dict[str, xr.Dataset],\n",
    "#         dict_return_periods: Dict[str, xr.Dataset],\n",
    "#         rp_value: int = 5,\n",
    "#     ) -> float:\n",
    "#     \"\"\"\n",
    "#     Searches in the admin unit for the gauge that is the most \"upstream\"\n",
    "\n",
    "#     :param admin_unit: administrative unit ID\n",
    "#     :param dict_ds_au: dict of datasets per admin unit\n",
    "#     :param dict_return_periods: dictionary of return period dataset per gauge\n",
    "#     :param rp_value: return period value to aggregate\n",
    "#     :return: return period value of the most upstream gauge\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "def create_admin_unit_set(dict_ds: Dict[str, xr.Dataset]) -> set:\n",
    "    \"\"\"\n",
    "    Create a unique set of the admin units in the datasets\n",
    "\n",
    "    :param dict_ds: dict of datasets\n",
    "    :return: set of admin units\n",
    "    \"\"\"\n",
    "    admin_units = set()\n",
    "    for ds in dict_ds.values():\n",
    "        if 'admin_unit' in ds.attrs:\n",
    "            if ds.attrs['admin_unit'] is None:\n",
    "                print(f'No admin unit found in dataset {ds.attrs[\"gauge_id\"]}')\n",
    "            else:\n",
    "                admin_units.update(ds.attrs['admin_unit'])\n",
    "        else:\n",
    "            raise ValueError('No admin unit found in dataset')\n",
    "    return admin_units\n",
    "\n",
    "\n",
    "def get_dict_ds_per_admin_unit(dict_ds: Dict[str, xr.Dataset]) -> Dict[str, list]:\n",
    "    \"\"\"\n",
    "    Get a dictionary with the datasets per admin unit\n",
    "\n",
    "    :param dict_ds: dict of datasets\n",
    "    :return: dict of datasets per admin unit\n",
    "    \"\"\"\n",
    "    admin_units = create_admin_unit_set(dict_ds)\n",
    "    dict_ds_per_admin_unit = {unit: [] for unit in admin_units}\n",
    "    for ds in dict_ds.values():\n",
    "        for unit in ds.attrs['admin_unit']:\n",
    "            dict_ds_per_admin_unit[unit].append(ds)\n",
    "    return dict_ds_per_admin_unit\n",
    "\n",
    "\n",
    "def pretty_print_list(l: list) -> None:\n",
    "    \"\"\"\n",
    "    Pretty print a list\n",
    "\n",
    "    :param l: list\n",
    "    \"\"\"\n",
    "    print(', '.join(l))\n",
    "\n",
    "\n",
    "def subset_lead_time(ds: xr.Dataset, lt: int) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Subset the dataset to a certain lead time\n",
    "\n",
    "    :param ds: xarray Dataset\n",
    "    :param lt: lead time to subset to\n",
    "    :return: xarray Dataset with subsetted lead time\n",
    "    \"\"\"\n",
    "    if lt < 0 or lt > 7:\n",
    "        raise ValueError('Lead time must be between 0 and 7 days')\n",
    "    return ds.sel(lead_time = pd.Timedelta(days = lt))\n",
    "\n",
    "\n",
    "def assign_actual_dates_to_dataset(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Assign the actual dates to the dataset as new coordinates\n",
    "\n",
    "    :param ds: xarray Dataset\n",
    "    :return: xarray Dataset with actual dates as coordinates\n",
    "    \"\"\"\n",
    "    actual_dates = ds['issue_time'] + ds['lead_time']\n",
    "    return ds.assign_coords(actual_date = ('issue_time', actual_dates.data))\n",
    "\n",
    "\n",
    "def aggregate_per_admin_unit(\n",
    "        dict_datasets: Dict[str, xr.Dataset],\n",
    "        lead_time: int = 7,\n",
    "        method_streamflow: str = 'max',\n",
    "        method_return_periods: str = 'max',\n",
    "        verbose: bool = True\n",
    "    ) -> Dict[str, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Aggregate the data per administrative unit:\n",
    "    - with lead time, subset the forecast horizon can be subsetted\n",
    "    - method_streamflow: method of aggregation for the streamflow data,\n",
    "                         defaulting to 'max', i.e. just taking the max per timestep\n",
    "    - method_return_periods: method of aggregation for the return periods\n",
    "\n",
    "    :param dict_datasets: dict of datasets\n",
    "    :param lead_time: lead time of the forecast to aggregate\n",
    "    :param method_streamflow: method of aggregation for the streamflow data\n",
    "    :param method_return_periods: method of aggregation for the return periods\n",
    "    :param verbose: whether to print some test print-s's\n",
    "    :return: dict of datasets with aggregated data\n",
    "    \"\"\"\n",
    "    # get the datasets per admin unit for aggregation next;\n",
    "    # check which admin units did not get any dataset assigned\n",
    "    grouped_datasets = get_dict_ds_per_admin_unit(dict_datasets)\n",
    "    if verbose:\n",
    "        print(\"[admin unit ID] : list([gauge ID's])\")\n",
    "        for unit, datasets in grouped_datasets.items():\n",
    "            print(unit, end = ' : ')\n",
    "            pretty_print_list([ds.attrs['gauge_id'] for ds in datasets])\n",
    "        print('\\n')\n",
    "\n",
    "    dict_datasets_aggregated = {}\n",
    "    dict_rp_aggregated = {}\n",
    "    # time complexity is O(n), where n is the number of admin units\n",
    "    idx = 1\n",
    "    for admin_unit, datasets in grouped_datasets.items():\n",
    "        if verbose:\n",
    "            print(f'aggregating {idx}/{len(grouped_datasets)}: {admin_unit}')\n",
    "            idx += 1\n",
    "        \n",
    "        # (2) concatenate the datasets into one dataset and add gauge_id dimension;\n",
    "        # (3) filter by lead time, discarding the other lead times; (4) assign the\n",
    "        # actual dates to the dataset, a.k.a. the date at which the forecast actually\n",
    "        # applies to; (5) aggregate the data by 'actual date' and calculate with 'method'\n",
    "        ds_combined = xr.concat(datasets, dim = 'gauge_id')\n",
    "        ds_combined_subset = subset_lead_time(ds_combined, lead_time)\n",
    "        ds_combined_actual_dates = assign_actual_dates_to_dataset(ds_combined_subset)\n",
    "        \n",
    "        if method_streamflow == 'max':\n",
    "            ds_aggregated = \\\n",
    "                ds_combined_actual_dates.groupby('actual_date').max(dim = 'gauge_id')\n",
    "        elif method_streamflow == 'mean':\n",
    "            ds_aggregated = \\\n",
    "                ds_combined_actual_dates.groupby('actual_date').mean(dim = 'gauge_id')\n",
    "        else:\n",
    "            raise ValueError('Method parameter not recognised')\n",
    "        ds_aggregated.attrs = {}\n",
    "\n",
    "        # we'll also aggregate the return periods present in the dataset attributes here\n",
    "        # using a method of choice, and add these to the aggregated datasets as attributes.\n",
    "        # we do this by (1) collecting the relevant attributes from the datasets, (2)\n",
    "        # aggregating them with the method, and (3) adding them as attributes to the\n",
    "        # aggregated datasets, and also saving them separately for reference\n",
    "        dict_rp_aggregated[admin_unit] = xr.Dataset({}, attrs = {'identifier': admin_unit})\n",
    "        attrs_collector = {}\n",
    "        for ds in datasets:\n",
    "            for attr in ds.attrs:\n",
    "                if attr.startswith('RP_') or attr.startswith('pc_'):\n",
    "                    if attr not in attrs_collector:\n",
    "                        attrs_collector[attr] = []\n",
    "                    attrs_collector[attr].append(ds.attrs[attr])\n",
    "        for attr_key, values in attrs_collector.items():\n",
    "            v = [float(v) for v in values]\n",
    "            if len(v) != len(datasets):\n",
    "                raise ValueError('Not all datasets have the same attributes')\n",
    "            if method_return_periods == 'max':\n",
    "                max_value = float(np.max(v))\n",
    "                ds_aggregated.attrs[attr_key] = max_value\n",
    "                dict_rp_aggregated[admin_unit].attrs[attr_key] = max_value\n",
    "            elif method_return_periods == 'mean':\n",
    "                mean_value = float(np.mean(v))\n",
    "                ds_aggregated.attrs[attr_key] = mean_value\n",
    "                dict_rp_aggregated[admin_unit].attrs[attr_key] = mean_value\n",
    "            else:\n",
    "                raise ValueError('RP-aggregation method not recognised')\n",
    "            \n",
    "        # last for this step, we update the attributes of the dataset of the admin unit,\n",
    "        # since now we're up a level from gauges to units, asking for a replacement\n",
    "        # of the attributes: we drop longitude, latitude, gauge_id, and admin_unit,\n",
    "        # and add the admin_unit and the gauge_id's of the gauges in the unit\n",
    "        ds_aggregated.attrs['identifier'] = admin_unit\n",
    "        ds_aggregated.attrs['gauge_ids'] = [ds.attrs['gauge_id'] for ds in datasets]\n",
    "        dict_datasets_aggregated[admin_unit] = ds_aggregated\n",
    "    \n",
    "    return dict_datasets_aggregated, dict_rp_aggregated\n",
    "\n",
    "\n",
    "def aggregate_or_load_per_admin_unit(\n",
    "        dict_datasets: Dict[str, xr.Dataset],\n",
    "        lt: int = 7,\n",
    "        method_streamflow: str = 'max',\n",
    "        method_return_periods: str = 'max',\n",
    "        verbose: bool = True,\n",
    "    ) -> Dict[str, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Helper function to aggregate the data per gauge (with aggregate_per_admin_unit())\n",
    "    to per admin unit if not done already (then load the datasets instead)\n",
    "\n",
    "    :param dict_datasets: dict of datasets\n",
    "    :param lt: lead time of the forecast to aggregate (e.g. 7)\n",
    "    :param method_streamflow: method of aggregation for the streamflow data\n",
    "    :param method_return_periods: method of aggregation for the return periods\n",
    "    :param verbose: whether to print some test print-s's\n",
    "    :return: dict of datasets with aggregated data\n",
    "    \"\"\"\n",
    "    if type(lt) != int or lt < 0 or lt > 7:\n",
    "        raise ValueError('Lead time must be an integer between 0 and 7')\n",
    "    \n",
    "    n_to_load = len(create_admin_unit_set(dict_datasets))\n",
    "    if not os.path.exists('../data/GRRR/aggregated/') or not os.path.exists('../data/GRRR/aggregated_rp/'):\n",
    "        os.makedirs('../data/GRRR/aggregated/', exist_ok = True)\n",
    "        os.makedirs('../data/GRRR/aggregated_rp/', exist_ok = True)\n",
    "    # Check whether the below if statements work with some printing statements\n",
    "    print('Checking if datasets are already loaded...')\n",
    "    # print(n_to_load)\n",
    "    # print(len([f for f in os.listdir('../data/GRRR/aggregated/') if f.endswith(f'_{lt * 24}lt.nc')]))\n",
    "    # print(len([f for f in os.listdir('../data/GRRR/aggregated_rp/') if f.endswith(f'_{lt * 24}lt.nc')]))\n",
    "\n",
    "\n",
    "    n_datasets_loaded = len([f for f in os.listdir('../data/GRRR/aggregated/') if \\\n",
    "                             f.endswith(f'_{lt * 24}lt.nc')])\n",
    "    n_rps_loaded = len([f for f in os.listdir('../data/GRRR/aggregated_rp/') if \\\n",
    "                        f.endswith(f'_{lt * 24}lt.nc')])\n",
    "    if (n_to_load == n_datasets_loaded) and (n_to_load == n_rps_loaded):\n",
    "        print('Loading in datasets...')\n",
    "        dict_datasets_au = import_datasets_from_netcdf('../data/GRRR/aggregated/', lt)\n",
    "        dict_return_periods_au = import_datasets_from_netcdf('../data/GRRR/aggregated_rp/', lt)\n",
    "        print('Loading complete')\n",
    "    else:\n",
    "        print('Datasets not loaded yet; aggregating...')\n",
    "        dict_datasets_au, dict_return_periods_au = aggregate_per_admin_unit(\n",
    "            dict_datasets, lt, method_streamflow, method_return_periods, verbose)\n",
    "        export_datasets_to_netcdf(dict_datasets_au, '../data/GRRR/aggregated/', lt)\n",
    "        export_datasets_to_netcdf(dict_return_periods_au, '../data/GRRR/aggregated_rp/', lt)\n",
    "        print('Aggregation complete')\n",
    "\n",
    "    return dict_datasets_au, dict_return_periods_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the code with just 10 gauges from the dict_datasets and dict_return_periods\n",
    "# dict_datasets_subset = {key: dict_datasets[key] for key in list(dict_datasets.keys())[:20]}\n",
    "# dict_return_periods_subset = {key: dict_return_periods_rc[key] for key in list(dict_return_periods_rc.keys())[:20]}\n",
    "# dict_datasets_aggregated, dict_return_periods_aggregated = \\\n",
    "#     aggregate_per_admin_unit(\n",
    "#         dict_datasets_subset, dict_return_periods_subset, verbose = True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs_datasets(\n",
    "    dict_ds: Dict[str, xr.Dataset],\n",
    "    mapping: Dict[str, str],\n",
    "    lead_time: int\n",
    ") -> Dict[str, xr.Dataset]:\n",
    "    \"\"\"\n",
    "    Preprocess DNH datasets to same format as admin unit datasets\n",
    "\n",
    "    :param dict_ds: dictionary with hybas datasets\n",
    "    :param mapping: dictionary with DNH mappings\n",
    "    :param lead_time: the lead time (0..7)\n",
    "    :return: dictionary with preprocessed datasets\n",
    "    \"\"\"\n",
    "    if not (0 <= lead_time <= 7):\n",
    "        raise ValueError(\"Lead time must be between 0 and 7 days\")\n",
    "    \n",
    "    dict_out = {}\n",
    "    for obs_id, gauge_id in mapping.items():\n",
    "        ds = dict_ds[gauge_id]\n",
    "        ds_sub = ds.sel(lead_time = pd.Timedelta(days = lead_time))\n",
    "                                # assign actual dates to the dataset\n",
    "        actual_dates = ds_sub['issue_time'] + ds_sub['lead_time']\n",
    "        ds_sub = ds_sub.assign_coords(actual_date = ('issue_time', actual_dates.data))\n",
    "        \n",
    "        ds_sub.attrs['identifier'] = obs_id\n",
    "\n",
    "        dict_out[obs_id] = ds_sub\n",
    "        \n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through all datasets and print out all administrative unit names\n",
    "# idx = 0\n",
    "# for hybas, ds in dict_datasets.items():\n",
    "#     print(idx, hybas, ds.attrs['admin_unit'])\n",
    "#     idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the **impact data versus forecast data** analysis, all gauges within an admin unit are grouped, and then aggregated into a new dataset using the maximum at each timestep. As each issue time contains multiple lead times, we subset the lead time as well.\n",
    "\n",
    "For the **observational data versus forecast data** analysis, we just subset the lead time and bring the datasets to uniform data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the data per gauge to per admin unit if not done already\n",
    "dict_datasets_au, dict_return_periods_au = aggregate_or_load_per_admin_unit(\n",
    "    dict_datasets, LEAD_TIME\n",
    ")\n",
    "\n",
    "# make a dictionary with as keys the gauge ID and as value the dataset\n",
    "# using the mapping of the DNH stations to the gauges\n",
    "dict_DNH_datasets = preprocess_obs_datasets(dict_datasets, mapping_DNH, LEAD_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "# make a plot to visualise the aggregated data for a certain admin unit\n",
    "# by plotting the individual gauges in the admin unit and the aggregated data\n",
    "# which should nicely visualise the aggregation process, while also serving as a check\n",
    "def add_RPs_to_plot(\n",
    "        ax: mpl.axes, ds: xr.Dataset, thr: Tuple[int], set_legend: bool = True\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Adds horizontal lines with return periods to a plot\n",
    "\n",
    "    :param ax: axis to add the return periods to\n",
    "    :param ds: dataset with attributes for the return periods\n",
    "    :param thr: list of thresholds to add return periods for\n",
    "    \"\"\"\n",
    "    colors = ['yellow', 'orange', 'red', 'brown', 'black']\n",
    "    ys = [ds.attrs[f'RP_{threshold}'].item() for threshold in thr]\n",
    "\n",
    "    # if there is only one colour, set it to orange\n",
    "    # add legend, make a legend with a line \n",
    "    for threshold, y, color in zip(thr, ys, colors):\n",
    "        ax.axhline(\n",
    "            y = y,\n",
    "            color = color if len(thr) > 1 else 'orange',\n",
    "            label = f'{threshold}-yr return period' if set_legend else None,\n",
    "            linestyle = '--'\n",
    "        )\n",
    "    if set_legend:\n",
    "        ax.legend(loc = 'upper right')\n",
    "\n",
    "        \n",
    "def plot_aggregated_reforecast(\n",
    "        issue_time_start_date: str, issue_time_end_date: str,\n",
    "        l_ds_gauges: List[xr.Dataset],\n",
    "        ds_au: xr.Dataset,\n",
    "        thresholds: Tuple[int] = (2, 5, 20)\n",
    "    ) -> None:\n",
    "    \"\"\" \n",
    "    Plots the gauges in an administrative unit and the aggregated data\n",
    "    for that same administrative unit, showing if the aggregation process\n",
    "    was processed correctly. (In contrast to plot_reforecast(), this function\n",
    "    does not plot distinct lead times, since they're already filtered out\n",
    "    in the aggregation process.)\n",
    "\n",
    "    :param issue_time_start_date: start date for the issue time\n",
    "    :param issue_time_end_date: end date for the issue time\n",
    "    :param l_ds_gauges: list of xarray datasets for individual gauges\n",
    "    :param ds_au: aggregated xarray dataset for the admin unit\n",
    "    :param thresholds: tuple with the thresholds for the return periods\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize = (20, 4))\n",
    "    \n",
    "    # plot individual gauges in the administrative unit\n",
    "    for ds in l_ds_gauges:\n",
    "        # for the gauge datasets, we still need to subset the lead time\n",
    "        ds_7_day = ds.sel(lead_time = pd.Timedelta(days = 7))\n",
    "        issue_time_slice = ds_7_day.sel(issue_time = \\\n",
    "                            slice(issue_time_start_date, issue_time_end_date))\n",
    "        ax.plot(\n",
    "            pd.to_datetime(issue_time_slice['issue_time'].values),\n",
    "            issue_time_slice['streamflow'].values,\n",
    "            alpha = 0.5,        # make the lines little bit transparent\n",
    "            label = f'gauge {ds_7_day.attrs[\"gauge_id\"]}'\n",
    "        )\n",
    "    \n",
    "    # plot the aggregated timeseries (usually the maximum)\n",
    "    ds_aggregated_slice = ds_au.sel(issue_time = \\\n",
    "                            slice(issue_time_start_date, issue_time_end_date))\n",
    "    ax.plot(\n",
    "        pd.to_datetime(ds_aggregated_slice['issue_time'].values),\n",
    "        ds_aggregated_slice['streamflow'].values,\n",
    "        color = '#092448',\n",
    "        linewidth = 2,\n",
    "        label = 'aggregated',\n",
    "        zorder = 3              # make sure the aggregated data is on top\n",
    "    )\n",
    "    \n",
    "    # # add the return periods for all gauges in the admin unit,\n",
    "    # # only add the legend for the return periods once\n",
    "    # set_legend = True\n",
    "    # for gauge_ID in ds_au.attrs['gauge_ids']:\n",
    "    #     print(gauge_ID)\n",
    "    #     # Here, I'd need the dict_datasets attributes for the RPs, but let's forget them for now.\n",
    "    #     add_RPs_to_plot(ax, ds, thresholds, set_legend)\n",
    "    #     set_legend = False\n",
    "        \n",
    "    # set title, labels\n",
    "    ax.set_title(f'aggregated forecast for {ds_au.attrs[\"admin_unit\"]} with return periods per gauge')\n",
    "    ax.set_xlabel('issue time')\n",
    "    ax.set_ylabel(r'streamflow ($\\mathrm{m}^3/\\mathrm{s}$)')\n",
    "    # plt.legend(loc = 'upper right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_datasets_unit_with_most_gauges(\n",
    "        dict_ds_agg: Dict[str, xr.Dataset],\n",
    "        dict_ds: Dict[str, xr.Dataset]\n",
    "    ) -> Tuple[List[xr.Dataset], str]:\n",
    "    \"\"\"\n",
    "    Finds the administrative unit with the most gauges and returns \n",
    "    the gauge datasets belonging to that unit in a list, taken from\n",
    "    the non-aggregated dictionary of datasets\n",
    "\n",
    "    :param dict_ds_agg: dict with the datasets\n",
    "    :param dict_ds: dict with the datasets\n",
    "    :return: list with the datasets of the admin unit with most gauges and unit name\n",
    "    \"\"\"\n",
    "    most_common_admin_unit = \\\n",
    "        max(dict_ds_agg.keys(), key = lambda k: len(dict_ds_agg[k].attrs['gauge_ids']))\n",
    "    \n",
    "    datasets_for_admin_unit = [\n",
    "        ds for ds in dict_ds.values() if most_common_admin_unit in ds.attrs['admin_unit']\n",
    "    ]\n",
    "\n",
    "    return datasets_for_admin_unit, most_common_admin_unit\n",
    "\n",
    "\n",
    "def get_datasets_for_xth_admin_unit(\n",
    "        dict_ds_agg: Dict[str, xr.Dataset],\n",
    "        dict_ds: Dict[str, xr.Dataset],\n",
    "        x: int\n",
    "    ) -> Tuple[List[xr.Dataset], str]:\n",
    "    \"\"\"\n",
    "    Finds the administrative unit with the xth most gauges and returns \n",
    "    the gauge datasets belonging to that unit in a list, taken from\n",
    "    the non-aggregated dictionary of datasets\n",
    "\n",
    "    :param dict_ds_agg: dict with the datasets\n",
    "    :param dict_ds: dict with the datasets\n",
    "    :param x: the xth admin unit to get the datasets for\n",
    "    :return: list with the datasets of the admin unit with most gauges and unit name\n",
    "    \"\"\"\n",
    "    aus_sorted = sorted(dict_ds_agg.keys(),\n",
    "                        key = lambda k: len(dict_ds_agg[k].attrs['gauge_ids']), \n",
    "                        reverse = True)\n",
    "    if x < 1 or x > len(aus_sorted):\n",
    "        raise ValueError(f'x must be between 1 and {len(aus_sorted)}')\n",
    "    \n",
    "    xth_admin_unit = aus_sorted[x - 1]\n",
    "    datasets_for_admin_unit = [\n",
    "        ds for ds in dict_ds.values() if xth_admin_unit in ds.attrs['admin_unit']\n",
    "    ]\n",
    "\n",
    "    # for the datasets in datasets_for_admin_unit, see if the\n",
    "    # gauge ID's are correct by comparing them to the gauge ID\n",
    "    # list in the attributes of the dataset for the admin unit\n",
    "    gauge_ids = dict_ds_agg[xth_admin_unit].attrs['gauge_ids']\n",
    "    for ds in datasets_for_admin_unit:\n",
    "        if ds.attrs['gauge_id'] not in gauge_ids:\n",
    "            raise ValueError(f'gauge ID {ds.attrs[\"gauge_id\"]} not in gauge ID list')\n",
    "\n",
    "    return datasets_for_admin_unit, xth_admin_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_gauges_10th_admin_unit, admin_unit_10th_gauges = \\\n",
    "    get_datasets_for_xth_admin_unit(dict_datasets_au, dict_datasets, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_gauges_admin_unit_most, admin_unit_most_gauges = \\\n",
    "    get_datasets_unit_with_most_gauges(dict_datasets_au, dict_datasets)\n",
    "\n",
    "plot_aggregated_reforecast('2018-05-01', '2018-10-30',\n",
    "                           l_gauges_admin_unit_most,\n",
    "                           dict_datasets_au[admin_unit_most_gauges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_gauges_xth_admin_unit, admin_unit_xth_gauges = \\\n",
    "    get_datasets_for_xth_admin_unit(dict_datasets_au, dict_datasets, 15)\n",
    "\n",
    "plot_aggregated_reforecast('2019-05-01', '2019-10-30',\n",
    "                           l_gauges_xth_admin_unit,\n",
    "                           dict_datasets_au[admin_unit_xth_gauges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To determine when to trigger and identify an \"event\", we need to\n",
    "# # choose how we want to aggregate the return periods of each gauge\n",
    "# # into one value for the administrative unit. Multiple options can \n",
    "# # be argued for, but we will start with the mean value of the return\n",
    "# # periods of the gauges in the administrative unit, with the reasoning\n",
    "# # that minima and maxima can be too extreme/skewed\n",
    "# import pandas as pd\n",
    "# import geopandas as gpd\n",
    "# import numpy as np\n",
    "# import xarray as xr\n",
    "# from typing import Dict, List\n",
    "\n",
    "\n",
    "# # def get_upstream_gauge_return_period(\n",
    "# #         admin_unit: str,\n",
    "# #         dict_ds_au: Dict[str, xr.Dataset],\n",
    "# #         dict_return_periods: Dict[str, xr.Dataset],\n",
    "# #         rp_value: int = 5,\n",
    "# #     ) -> float:\n",
    "# #     \"\"\"\n",
    "# #     Searches in the admin unit for the gauge that is the most \"upstream\"\n",
    "\n",
    "# #     :param admin_unit: administrative unit ID\n",
    "# #     :param dict_ds_au: dict of datasets per admin unit\n",
    "# #     :param dict_return_periods: dictionary of return period dataset per gauge\n",
    "# #     :param rp_value: return period value to aggregate\n",
    "# #     :return: return period value of the most upstream gauge\n",
    "# #     \"\"\"\n",
    "    \n",
    "\n",
    "# def aggregate_return_periods(\n",
    "#         dict_ds_au: Dict[str, xr.Dataset],\n",
    "#         dict_return_periods: Dict[str, xr.Dataset],\n",
    "#         rp_value: int = 5,\n",
    "#         method: str = 'mean'\n",
    "#     ) -> Dict[str, xr.Dataset]:\n",
    "#     \"\"\"\n",
    "#     Aggregate return periods per administrative unit based on the specified method\n",
    "    \n",
    "#     :param dict_datasets_au: dict of datasets per admin unit\n",
    "#     :param dict_return_periods: dictionary of return period dataset per gauge\n",
    "#     :param rp_value: return period value to aggregate\n",
    "#     :param method: aggregation method ('min', 'mean', 'max', 'upstream')\n",
    "#     :return: dict with admin unit ID : dataset with rp dataaset\n",
    "#     \"\"\"\n",
    "#     if rp_value not in [2, 5, 20]:\n",
    "#         raise ValueError(\"threshold must be 2, 5, or 20\")\n",
    "#     return_periods_per_admin = {}\n",
    "#                                     # for each admin unit ds:\n",
    "#     for admin_unit, ds in dict_ds_au.items():\n",
    "#         return_period_values = []   # loop through its gauges and get the return periods\n",
    "#         for gauge_id in ds.attrs['gauge_ids']:\n",
    "#             if f'hybas_{gauge_id}' in dict_return_periods:\n",
    "#                 return_period_ds = dict_return_periods.get(f'hybas_{gauge_id}')\n",
    "#             else:\n",
    "#                 print(f'No return period dataset found for gauge {gauge_id}')\n",
    "#                 continue\n",
    "#                                     # store the return period value of choice\n",
    "#             if f'return_period_{rp_value}' in return_period_ds:\n",
    "#                 return_period_values.append(return_period_ds[f'return_period_{5}'].item())\n",
    "#             else:\n",
    "#                 print(f'No return period value {rp_value} found for gauge {gauge_id}')\n",
    "#                                     # store the return period values per admin unit\n",
    "#         return_periods_per_admin[admin_unit] = return_period_values\n",
    "\n",
    "#     print(return_periods_per_admin)\n",
    "#     # aggregate with method of choice\n",
    "#     aggregated_return_periods = {}\n",
    "#     for admin_unit, values in return_periods_per_admin.items():\n",
    "#         if method == 'min':\n",
    "#             aggregated_value = np.min(values)\n",
    "#         elif method == 'mean':\n",
    "#             aggregated_value = np.mean(values)\n",
    "#         elif method == 'max':\n",
    "#             aggregated_value = np.max(values)\n",
    "#         # elif method == 'upstream':\n",
    "#         #     aggregated_value = get_upstream_gauge_return_period(admin_unit,\n",
    "#         #                                                         dict_ds_au,\n",
    "#         #                                                         dict_return_periods,\n",
    "#         #                                                         rp_value)\n",
    "#         else:\n",
    "#             raise ValueError(\"Method not recognised\")\n",
    "\n",
    "#         # create and save new dataset with the aggregated rp's\n",
    "#         aggregated_return_periods[admin_unit] = xr.Dataset({\n",
    "#                 'return_period_5': aggregated_value\n",
    "#             }, attrs = {\n",
    "#                 'admin_unit': admin_unit\n",
    "#             })\n",
    "\n",
    "#     return aggregated_return_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dict_flood_events_to_csv(\n",
    "        dict_flood_events: Dict[str, pd.DataFrame], th: Any,\n",
    "        lt: int, verbose: bool = False\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Export the dictionary with flood events to a csv file\n",
    "    with a MultiIndex for keys and events\n",
    "\n",
    "    :param dict_flood_events: dictionary with flood events\n",
    "    :param th: return period threshold\n",
    "    :param lt: lead time\n",
    "    :param verbose: whether to print some test print-s's\n",
    "    \"\"\"\n",
    "    if not dict_flood_events:\n",
    "        print('No flood events found')\n",
    "        return\n",
    "    df_flood_events = pd.concat(dict_flood_events,\n",
    "                                names = ['identifier', 'events'])\n",
    "    print(df_flood_events) if verbose else None\n",
    "                            # sort the index to make the csv file more readable\n",
    "    df_flood_events = df_flood_events.sort_index()\n",
    "                            # add n_flood_events to name to\n",
    "                            # see difference between files\n",
    "    n_flood_events = df_flood_events.shape[0]\n",
    "    print(f'exporting {n_flood_events} flood events to csv')\n",
    "    \n",
    "    if th in [95, 98, 99]:\n",
    "        th_str = f'{th}pc'\n",
    "    else:\n",
    "        th_str = f'{th}rp'\n",
    "    df_flood_events.to_csv(\n",
    "        f'../data/processed/flood_events/flood_events_au_{lt * 24}lt_{th_str}_n{n_flood_events}.csv',\n",
    "        sep = ';', decimal = '.')\n",
    "    \n",
    "    \n",
    "def create_flood_mask(\n",
    "        ds: xr.Dataset, threshold: int = 5\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates a boolean mask to identify when the streamflow\n",
    "    exceeds the return period threshold\n",
    "\n",
    "    :param ds: dataset with streamflow data\n",
    "    :param threshold: return period threshold\n",
    "    :return: boolean mask\n",
    "    \"\"\"\n",
    "    if f'RP_{threshold}' not in ds.attrs and f'pc_{threshold}th' not in ds.attrs:\n",
    "        raise ValueError(f\"No return period key ({threshold}) found for {ds.attrs['identifier']}\")\n",
    "    # if ds.attrs[f'RP_{threshold}'] is None and ds.attrs[f'pc_{threshold}th'] is None:\n",
    "    #     raise ValueError(f\"No return period value ({threshold}) found for {ds.attrs['identifier']}\")\n",
    "\n",
    "    # return a boolean mask where True indicates that the streamflow\n",
    "    # exceeds the return period threshold for the administrative unit\n",
    "    if f'RP_{threshold}' in ds.attrs:\n",
    "        return ds['streamflow'] > ds.attrs[f'RP_{threshold}']\n",
    "    else:\n",
    "        return ds['streamflow'] > ds.attrs[f'pc_{threshold}th']\n",
    "\n",
    "\n",
    "def create_event_dict(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create a dictionary with flood events per admin unit\n",
    "\n",
    "    :param df: DataFrame with flood events\n",
    "    :return: dict with df of flood events per admin unit\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for admin_unit, group in df.groupby('identifier'):\n",
    "        d[admin_unit] = group.reset_index(drop = True)\n",
    "    return d\n",
    "\n",
    "\n",
    "def merge_sequential_flood_events(\n",
    "        df: pd.DataFrame, ds: xr.Dataset, action_lifetime: int = 10\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge flood events that are within one action lifetime of one another\n",
    "\n",
    "    :param df: DataFrame with flood events\n",
    "    :param ds: dataset with streamflow data (for calculation of peak streamflow)\n",
    "    :param action_lifetime: days allowed between events\n",
    "    :return: DataFrame with merged flood events\n",
    "    \"\"\"\n",
    "                            # convert the DataFrame to a list of dictionaries\n",
    "    list_events = [df.iloc[0].to_dict()]\n",
    "    for idx in range(1, len(df)):\n",
    "                            # for each flood event, get the current and last event,\n",
    "                            # check if they are within the action lifetime of one another\n",
    "                            # and merge them if they are\n",
    "        current_event = df.iloc[idx]\n",
    "        last_event = list_events[-1]\n",
    "\n",
    "        days_between = (pd.Timestamp(current_event['flood_start']) - \\\n",
    "                        pd.Timestamp(last_event['flood_end'])).days\n",
    "        if days_between <= action_lifetime:\n",
    "                            # within if-s, merge events\n",
    "            last_event['flood_end'] = pd.Timestamp(current_event['flood_end'])\n",
    "            last_event['duration'] = (last_event['flood_end'] - last_event['flood_start']).days + 1\n",
    "                            # recompute peak_streamflow over the new period\n",
    "            last_event['peak_streamflow'] = ds['streamflow'].sel(\n",
    "                issue_time = slice(last_event['flood_start'], last_event['flood_end'])\n",
    "            ).max().item()\n",
    "        else:\n",
    "                            # else, no merge needed\n",
    "            list_events.append(current_event.to_dict())\n",
    "    \n",
    "    return pd.DataFrame(list_events)\n",
    "\n",
    "\n",
    "def get_admin_unit_name(p: str) -> str:\n",
    "    \"\"\"\n",
    "    Gets the admin unit name given a pcode\n",
    "\n",
    "    :param p: PCODE\n",
    "    :return: admin unit name\n",
    "    \"\"\"\n",
    "    with open('../data/mappings/PCODE_to_Cercle.json', 'r') as f:\n",
    "        mapping = json.load(f)\n",
    "    return mapping.get(p)\n",
    "\n",
    "\n",
    "def add_admin_unit_name_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column to a DataFrame with the Cercle name based on the \n",
    "    pcode in the 'admin_unit' column\n",
    "\n",
    "    :param df: DataFrame with 'admin_unit' column\n",
    "    :return: DataFrame with added 'admin_unit_NAME' column\n",
    "    \"\"\"\n",
    "    with open('../data/mappings/PCODE_to_Cercle.json', 'r') as f:\n",
    "        mapping = json.load(f)\n",
    "    df['admin_unit_NAME'] = df['admin_unit'].apply(lambda x: mapping.get(x, None))\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_flood_events(\n",
    "        dict_ds_au: Dict[str, xr.Dataset],\n",
    "        merge = True,\n",
    "        threshold: int = 5,\n",
    "        action_lifetime: int = 10\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates flood events based on the return period threshold.\n",
    "    A flood event is defined as a consecutive period where the\n",
    "    streamflow exceeds the given threshold for the return period\n",
    "\n",
    "    :param dict_ds_au: dict of ds per admin unit\n",
    "    :param threshold: rp-value to trigger flood event (default is 5)\n",
    "    :param action_lifetime: days allowed between events (default is 10)\n",
    "    :return: dict with df of flood events per admin unit\n",
    "    \"\"\"\n",
    "    if threshold not in [1.5, 2, 5, 7, 10, 20, 95, 98, 99]:\n",
    "        raise ValueError(\"threshold must be 1.5, 2, 5, 7, 10, 20\")\n",
    "    # ds_rp_agg = aggregate_return_periods(dict_ds_au, ds_rp_agg, threshold)\n",
    "    \n",
    "    flood_events_per_au = {}    # for each admin unit ds:\n",
    "    for admin_unit, ds in dict_ds_au.items():\n",
    "                                # create a boolean mask to identify when\n",
    "                                # the streamflow exceeds the return period threshold\n",
    "        flood_mask = create_flood_mask(ds, threshold)\n",
    "        \n",
    "        flood_events = []\n",
    "                                # find the start and end of flood events by checking\n",
    "                                # consecutive True values in the mask\n",
    "        flood_start, flood_end = None, None\n",
    "        for idx, is_flood in enumerate(flood_mask):\n",
    "            if is_flood and flood_start is None:\n",
    "                                # start of a new flood event\n",
    "                flood_start = ds['actual_date'].values[idx]\n",
    "            elif not is_flood and flood_start is not None:\n",
    "                                # end of the current flood event\n",
    "                flood_end = ds['actual_date'].values[idx - 1]\n",
    "                                # inclusive difference (+1) between the two dates;\n",
    "                                # if the flood event is only one day, the duration is 1;\n",
    "                                # and, if duration == 1, the event is ignored, as,\n",
    "                                # hydrologically, the river is expected to handle it\n",
    "                duration = pd.Timedelta(flood_end - flood_start).days + 1\n",
    "                if duration == 1:\n",
    "                    flood_start, flood_end = None, None\n",
    "                    continue\n",
    "                                # store the flood event details\n",
    "                flood_events.append({\n",
    "                    'flood_start': flood_start,\n",
    "                    'flood_end': flood_end,\n",
    "                    'duration': duration,\n",
    "                    'peak_streamflow': \\\n",
    "                        ds['streamflow'].sel(issue_time = slice(flood_start,\n",
    "                                                                flood_end)).max().item()\n",
    "                })\n",
    "                                # reset flood_start and flood_end\n",
    "                flood_start, flood_end = None, None\n",
    "                                # by now, if flood_start is not None,\n",
    "                                # it means the flood event is still going\n",
    "        if flood_start is not None:\n",
    "            flood_end = ds['actual_date'].values[-1]\n",
    "            flood_events.append({\n",
    "                'flood_start': flood_start,\n",
    "                'flood_end': flood_end,\n",
    "                'duration': pd.Timedelta(flood_end - flood_start).days + 1,\n",
    "                'peak_streamflow': \\\n",
    "                    ds['streamflow'].sel(issue_time = \\\n",
    "                                         slice(flood_start, flood_end)).max().item()\n",
    "            })\n",
    "\n",
    "                                # store the flood events per admin unit in a df,\n",
    "                                # and merge sequential flood events when needed\n",
    "                                # (as they are likely the same event)\n",
    "        df = pd.DataFrame(flood_events)\n",
    "        if df.empty:            # if no flood events are found, skip the admin unit\n",
    "            print('No flood events found for admin unit', admin_unit)\n",
    "            continue\n",
    "\n",
    "        if merge and len(flood_events) > 0:       \n",
    "            df = merge_sequential_flood_events(df, ds, action_lifetime)\n",
    "        flood_events_per_au[admin_unit] = df\n",
    "\n",
    "    return flood_events_per_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_datasets_au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_DNH_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating flood events starts with combining the forecast timeseries ($Q(t)$) and a threshold ($Q_{th}$) into a boolean mask per timestep, with **True** / **False** for $Q(t) > Q_{th}$ and $Q(t) < Q_{th}$, respectively.\n",
    "\n",
    "If the flood is just one day, i.e. only one consecutive **True** value is encountered, the river is assumed to handle the water itself. If not, a flood event is created and stored in a DataFrame per administrative unit or station (depending on assessing impact data or observational data) accessible with the corresponding identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (COMP_TYPE == 'IMPACT' or COMP_TYPE == 'OBS'):\n",
    "    raise ValueError('COMP_TYPE must be either IMPACT or OBS')\n",
    "# Note that the flood events are already calculated by their \"actual date\",\n",
    "# hence, we export it using LEAD_TIME (* 24 hours) as lead time identifier\n",
    "if COMP_TYPE == 'IMPACT':\n",
    "    dict_flood_events = create_flood_events(dict_datasets_au, True, THRESHOLD)\n",
    "if COMP_TYPE == 'OBS':\n",
    "    dict_flood_events = create_flood_events(dict_DNH_datasets, True, THRESHOLD)\n",
    "export_dict_flood_events_to_csv(dict_flood_events, THRESHOLD, LEAD_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agg_reforecast(start_date: str, end_date: str,\n",
    "                        ds_rf: xr.Dataset, threshold: int = 5,\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Plots aggregated reforecast data for a given time range\n",
    "\n",
    "    :param start_date: start date for the time range\n",
    "    :param end_date: end date for the time range\n",
    "    :param ds_rf: reforecast dataset of admin unit\n",
    "    :param thresholds: list of thresholds to add return periods for\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize = (20, 4))\n",
    "    ds_subset = ds_rf.streamflow.sel(issue_time = slice(start_date, end_date))\n",
    "    \n",
    "    ax.plot(ds_subset.issue_time, ds_subset.values)\n",
    "    ax.axhline(y = ds_rf.attrs[f'RP_{threshold}'], color = 'orange',\n",
    "               label = f'{threshold}-yr return period', linestyle = '--'\n",
    "    )\n",
    "    ax.set_title(f'Reforecast data for {ds_rf.attrs[\"admin_unit\"]} with {threshold}-yr return period')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the flood events are correctly created using some plots:\n",
    "\"\"\"\n",
    "ML0201;0;2018-09-05;2018-09-05;0;134.45091247558594\n",
    "ML0201;1;2019-08-20;2019-08-20;0;128.57044982910156\n",
    "ML0201;2;2019-08-22;2019-08-26;4;195.2600860595703\n",
    "ML0201;3;2020-08-20;2020-08-20;0;163.1853790283203\n",
    "ML0201;4;2020-08-22;2020-09-04;13;299.27252197265625\n",
    "\"\"\"\n",
    "# ML0201, for example, has 5 flood events, with some having a duration of 1 day\n",
    "# Let's investigate with a plot and see if this has been processed correctly\n",
    "issue_time_start_date = '2018-05-01'\n",
    "issue_time_end_date = '2020-10-30'\n",
    "\n",
    "plot_agg_reforecast(issue_time_start_date, issue_time_end_date, dict_datasets_au['ML0201'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual .xlsx to .csv conversion errors, so we'll recreate the .csv manually\n",
    "# \"\"\"\n",
    "# df_impact_data = pd.read_csv('../data/impact_data/impact_data_Mali.csv',\n",
    "#                              sep = ',',\n",
    "#                              header = 0,\n",
    "#                              encoding = 'utf-8')\n",
    "# print(df_impact_data.head())\n",
    "# \"\"\"\n",
    "# import openpyxl\n",
    "# import csv\n",
    "\n",
    "# sheet = openpyxl.load_workbook('../data/impact_data/impact_data_Mali.xlsx').active\n",
    "\n",
    "# with open('../data/impact_data/impact_data_Mali_recreated.csv',\n",
    "#           'w',\n",
    "#           newline = '',\n",
    "#           encoding = 'utf-8') as csvfile:\n",
    "#     csvwriter = csv.writer(csvfile, delimiter = ';')\n",
    "#     for row in sheet.iter_rows():\n",
    "#         csvwriter.writerow([cell.value if cell.value is not None else \"\" for cell in row])\n",
    "\n",
    "# df_impact_data = pd.read_csv('../data/impact_data/impact_data_Mali_recreated.csv',\n",
    "#                                 sep = ',',\n",
    "#                                 header = 0,\n",
    "#                                 encoding = 'utf-8')\n",
    "# print(df_impact_data.columns)\n",
    "# print(df_impact_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'IMPACT':\n",
    "\n",
    "    # the next step is to load in the impact data and to transform it to\n",
    "    # a format that is similar to the flood events, such that we can compare\n",
    "    # and evaluate the model's performance\n",
    "    df_impact_data = pd.read_csv('../data/impact_data/impact_data_Mali.csv',\n",
    "                                    sep = ',',\n",
    "                                    header = 0,\n",
    "                                    encoding = 'utf-8')\n",
    "\n",
    "    print(df_impact_data.columns)\n",
    "    print(df_impact_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def fill_cercle_from_commune(d: Dict[str, List[str]], row: pd.Series) -> str:\n",
    "    \"\"\" \n",
    "    Tries to fill in an empty 'Cercle' column with the corresponding\n",
    "    'Cercle' value from the 'Commune' entry in the dict if there is one\n",
    "\n",
    "    :param dict: dictionary with commune-cercle mapping\n",
    "    :param row: row from df\n",
    "    :return: 'Cercle' value (hopefully)\n",
    "    \"\"\"\n",
    "    if pd.isna(row['Cercle']) or row['Cercle'].strip() == '':\n",
    "        commune = row['Commune']\n",
    "        if commune in d and len(d[commune]) == 1:\n",
    "            return d[commune][0]\n",
    "        else:\n",
    "            return row['Cercle']\n",
    "    else:\n",
    "        return row['Cercle']\n",
    "\n",
    "\n",
    "def export_dict_impact_events_to_csv(\n",
    "        dict_impact_events: Dict[str, pd.DataFrame],\n",
    "        obs: bool = False, verbose: bool = False\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Export the dictionary with impact events to a csv file\n",
    "    with a MultiIndex for keys and events\n",
    "\n",
    "    :param dict_flood_events: dictionary with flood events\n",
    "    :param obs: whether to export observational data\n",
    "    :param verbose: whether to be verbose\n",
    "    \"\"\"\n",
    "    list_df_impact_events = []\n",
    "    for admin_unit, df in dict_impact_events.items():\n",
    "        df = df.copy()\n",
    "        df.reset_index(inplace = True)\n",
    "        df['identifier'] = admin_unit\n",
    "        list_df_impact_events.append(df)\n",
    "\n",
    "    df_flood_events = pd.concat(list_df_impact_events, ignore_index = True)\n",
    "    print(df_flood_events) if verbose else None\n",
    "\n",
    "    if not obs:\n",
    "        print(f'exporting {df_flood_events.shape[0]} impact events to csv')\n",
    "        df_flood_events.to_csv(\n",
    "            f'../data/processed/flood_events/impact_events_au_{df_flood_events.shape[0]}.csv',\n",
    "            sep = ';', decimal = '.', index = False)\n",
    "    else:\n",
    "        print(f'exporting {df_flood_events.shape[0]} observational events to csv')\n",
    "        df_flood_events.to_csv(\n",
    "            f'../data/processed/flood_events/obs_events_au_{df_flood_events.shape[0]}.csv',\n",
    "            sep = ';', decimal = '.', index = False)\n",
    "    \n",
    "\n",
    "def map_cercle_names_to_pcodes(\n",
    "        df: pd.DataFrame,\n",
    "        path: str = 'mali_ALL/mli_adm_ab_shp/mli_admbnda_adm2_1m_gov_20211220.shp',\n",
    "        verbose: bool = False\n",
    "        ) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Maps the Cercle names in the impact data to the pcodes found in the\n",
    "    shape file of the administrative units and used for the flood events\n",
    "\n",
    "    :param df: dataframe with impact data\n",
    "    :param path: path to the shape file\n",
    "    :param verbose: whether to print some test prints\n",
    "    :return: dictionary with Cercle names mapped to pcodes\n",
    "    \"\"\"\n",
    "                        # load shape file and normalise columns of interest\n",
    "    gdf = analyse.get_shape_file(path).to_crs('EPSG:4326')\n",
    "    gdf['ADM2_FR'] = gdf['ADM2_FR'].str.strip().str.upper()\\\n",
    "                        .str.normalize('NFKD').str.encode('ascii',errors = 'ignore')\\\n",
    "                            .str.decode('utf-8') \n",
    "                        # use the print statements to compare admin unit names\n",
    "                        # of the shape file and of the impact data\n",
    "    # print(np.sort(gdf['ADM2_FR'].unique()))\n",
    "                        # print out the identifiers in the impact data which are not\n",
    "                        # found in the shape file, so we can manually check and correct\n",
    "    if verbose:\n",
    "        print('Identifiers in impact data not found in shape file:')\n",
    "        print(np.sort(np.setdiff1d(df['Cercle'].unique(), gdf['ADM2_FR'].unique())))\n",
    "                        # the print statement above and this one produce:\n",
    "    \"\"\"\n",
    "    ['ABEIBARA' 'ANDERAMBOUKANE' 'ANSONGO' 'BAFOULABE' 'BAMAKO' 'BANAMBA'\n",
    "    'BANDIAGARA' 'BANKASS' 'BARAOUELI' 'BLA' 'BOUGOUNI' 'BOUREM' 'DIEMA'\n",
    "    'DIOILA' 'DIRE' 'DJENNE' 'DOUENTZA' 'GAO' 'GOUNDAM' 'GOURMA-RHAROUS'\n",
    "    'INEKAR' 'KADIOLO' 'KANGABA' 'KATI' 'KAYES' 'KENIEBA' 'KIDAL' 'KITA'\n",
    "    'KOLOKANI' 'KOLONDIEBA' 'KORO' 'KOULIKORO' 'KOUTIALA' 'MACINA' 'MENAKA'\n",
    "    'MOPTI' 'NARA' 'NIAFUNKE' 'NIONO' 'NIORO' 'SAN' 'SEGOU' 'SIKASSO'\n",
    "    'TENENKOU' 'TESSALIT' 'TIDERMENE' 'TIN-ESSAKO' 'TOMBOUCTOU' 'TOMINIAN'\n",
    "    'YANFOLILA' 'YELIMANE' 'YOROSSO' 'YOUWAROU']\n",
    "    <class 'numpy.ndarray'>\n",
    "    ['GOUMERA' 'ESSOUK' 'INTACHDAYTE' 'AGUELHOK' 'TAMKOUTAT' 'ZEGOUA' 'SONA'\n",
    "    'KOURY' 'BIRAMABOUGOU' 'SANDIA' 'TOUBAKORO' 'KENEKOUN']\n",
    "    \"\"\"\n",
    "                        # create a mapping from Cercle names to pcodes\n",
    "    mapping = gdf.set_index('ADM2_FR')['ADM2_PCODE'].to_dict()\n",
    "                        # some manual corrections made by searching Google Maps and GeoView\n",
    "    manual_corrections = {\n",
    "        'GOUMERA' : mapping.get('KAYES'),\n",
    "        'ESSOUK' : mapping.get('KIDAL'),\n",
    "        'INTACHDAYTE' : mapping.get('KIDAL'),\n",
    "        'AGUELHOK' : mapping.get('TESSALIT'),\n",
    "        'TAMKOUTAT' : mapping.get('GAO'),\n",
    "        'ZEGOUA' : mapping.get('KADIOLO'),\n",
    "        'SONA' : mapping.get('YOROSSO'),\n",
    "        'KOURY' : mapping.get('YOROSSO'),\n",
    "        'BIRAMABOUGOU' : mapping.get('SIKASSO'),\n",
    "        'SANDIA' : mapping.get('KAYES'),\n",
    "        'TOUBAKORO' : mapping.get('KOULIKORO'),\n",
    "        'KENEKOUN' : mapping.get('KANGABA')\n",
    "    }\n",
    "                        # update the mapping with the manual corrections\n",
    "                        # and apply the mapping to the 'Cercle' column    \n",
    "    mapping.update(manual_corrections)\n",
    "                        # save PCODE as the standard an NAME as subsidiary\n",
    "    df['admin_unit'] = df['Cercle'].apply(lambda x: mapping.get(x, None))\n",
    "    df['admin_unit_NAME'] = df['Cercle']\n",
    "    print(df.head()) if verbose else None\n",
    "                        # handle unmapped Cercle values\n",
    "    unmapped_cercles = df[df['admin_unit'].isnull()]['Cercle'].unique()\n",
    "    if unmapped_cercles.size > 0:\n",
    "        raise ValueError(f'Unmapped Cercle values: {unmapped_cercles}')\n",
    "    \n",
    "                        # at last, export the mapping as a .json file with\n",
    "                        # as keys the pcodes and as values the Cercle names,\n",
    "                        # and also the inverse mapping (it should exist)\n",
    "    with open('../data/mappings/Cercle_to_PCODE.json', 'w') as f:\n",
    "        json.dump(mapping, f, indent = 4)\n",
    "    inverse_mapping = {v: k for k, v in mapping.items()}\n",
    "    with open('../data/mappings/PCODE_to_Cercle.json', 'w') as f:\n",
    "        json.dump(inverse_mapping, f, indent = 4)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_duplicate_events(d_events: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Filter out double impact events by checking whether events have the \n",
    "    same start date, and then merge them to an event with the start date\n",
    "    of the first, and end date of the last event with the same start date.\n",
    "    Added later: also adjacent or overlapping events are merged.\n",
    "\n",
    "    :param dict_events: dict with au codas keys and events dfs as values\n",
    "    :return: same dict, but merged\n",
    "    \"\"\"\n",
    "    d_events_merged = {}\n",
    "\n",
    "    for admin_unit, df_events in d_events.items():\n",
    "                            # bring event_ID back as column\n",
    "        df_events = df_events.reset_index()\n",
    "        # grouped = df_events.groupby('flood_start')\n",
    "\n",
    "        merged_rows = []\n",
    "        current_event = df_events.iloc[0].to_dict()\n",
    "        # for flood_start, group in grouped:\n",
    "        for idx in range(1, len(df_events)):\n",
    "            next_event = df_events.iloc[idx].to_dict()\n",
    "\n",
    "            if next_event['flood_start'] <= current_event['flood_end']:\n",
    "                current_event['flood_end'] = max(current_event['flood_end'], next_event['flood_end'])\n",
    "                current_event['duration'] = (current_event['flood_end'] - \\\n",
    "                                             current_event['flood_start']).days + 1\n",
    "            else:\n",
    "                merged_rows.append(current_event)\n",
    "                current_event = next_event.copy()\n",
    "        merged_rows.append(current_event)\n",
    "\n",
    "        \n",
    "            #                 # if one event, don't merge\n",
    "            # if len(group == 1):\n",
    "            #     merged_rows.append(group.iloc[0])\n",
    "            # else:\n",
    "            #                 # find the latest flood end\n",
    "            #                 # and update columns accordingly\n",
    "            #     idx_max_end = group['flood_end'].idxmax()\n",
    "            #     max_end_row = group.loc[idx_max_end].to_dict()\n",
    "            #     max_end_row['flood_end'] = group['flood_end'].max()\n",
    "            #     max_end_row['duration'] = (max_end_row['flood_end'] - \\\n",
    "            #                                max_end_row['flood_start']).days + 1\n",
    "            #     merged_rows.append(max_end_row)\n",
    "                     \n",
    "        merged_events = pd.DataFrame(merged_rows)\n",
    "        merged_events.set_index('event_ID', inplace = True)\n",
    "        merged_events.sort_values('flood_start', inplace = True)\n",
    "        d_events_merged[admin_unit] = merged_events            \n",
    "\n",
    "    return d_events_merged\n",
    "\n",
    "\n",
    "def process_impact_data_to_events(\n",
    "        df: pd.DataFrame, verbose: bool = False\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process the \"raw\" impact data to events similar\n",
    "    to the events created from the flood data, with\n",
    "    specifically, per administative unit:\n",
    "    - start date of the event;\n",
    "    - end date of the event;\n",
    "    - duration of the event; and\n",
    "    \n",
    "    Here, especially classification into administrative units\n",
    "    is problematic, as some data is missing. How this is handled\n",
    "    can vary per your specific use case, and for missing rows\n",
    "    information is printed when verbose is True\n",
    "\n",
    "    :param df: dataframe with impact data\n",
    "    :param verbose: whether to be verbose\n",
    "    :return: dictionary with events per administrative unit\n",
    "    \"\"\"                  \n",
    "                        # keep only (possitbly) relevant columns, of which notable:\n",
    "                        # - 'Start date' and 'End date': self-explanatory\n",
    "                        # - 'Cercle': administrative unit\n",
    "                        # - the rest gives more location information, which might be\n",
    "                        #   useful if administrative unit information is missing\n",
    "    df = df[['Start Date', 'End Date', 'Région', 'Cercle', 'Commune', 'Quartier/Villages']].copy()\n",
    "                        # rename 'Start date' and 'End date' to 'flood_start' and 'flood_end'\n",
    "                        # and convert them to datetime objects with flag 'raise' for errors:\n",
    "                        # if any dates are missing they can be reconstructed using the\n",
    "                        # 'Années', 'Mois', and 'Jours' columns, but was not needed here\n",
    "    df.rename(columns = {'Start Date': 'flood_start', 'End Date': 'flood_end'}, inplace = True)\n",
    "    df['flood_start'] = pd.to_datetime(df['flood_start'], format = '%d/%m/%Y', errors = 'raise')\n",
    "    df['flood_end'] = pd.to_datetime(df['flood_end'], format = '%d/%m/%Y', errors = 'raise')\n",
    "                        # after printing out rows with missing dates with the following line:\n",
    "    \"\"\"\n",
    "    print(df[df['flood_start'].isnull() | df['flood_end'].isnull()]) if verbose else None\n",
    "    \"\"\" \n",
    "                        # row 133 was empty, so we drop it\n",
    "    df.dropna(subset = ['flood_start', 'flood_end'], inplace = True)\n",
    "                        # double check if no dates are missing\n",
    "    if df['flood_start'].isnull().values.any() or df['flood_end'].isnull().values.any():\n",
    "        raise ValueError('Missing dates in impact data')\n",
    "                        # next, we normalise 'Cercle' column to no whitespace and uppercase,\n",
    "                        # and also delete any accents on top of letters or dashes etc to make\n",
    "                        # sure the 'Cercle' column is as clean and uniform as possible\n",
    "    df['Cercle'] = df['Cercle'].str.strip().str.upper()\n",
    "    df['Cercle'] = df['Cercle'].str.normalize('NFKD').str.encode('ascii', errors = 'ignore').str.decode('utf-8')\n",
    "                        # solve some other specific naming ambiguities:\n",
    "    df['Cercle'] = df['Cercle'].str.replace(r'\\bTOMBOUCTO\\b', 'TOMBOUCTOU', regex = True)\n",
    "    df['Cercle'] = df['Cercle'].str.replace(r'\\bGOURMA-RH\\b', 'GOURMA-RHAROUS', regex = True)\n",
    "    df['Cercle'] = df['Cercle'].str.replace('NIAFUNKE + MORE LOCATIONS', 'NIAFUNKE')\n",
    "    df['Cercle'] = df['Cercle'].str.replace(r'\\bBANDIAGAR\\b', 'BANDIAGARA', regex = True)\n",
    "    df['Cercle'] = df['Cercle'].str.replace('BAROUELI', 'BARAOUELI')\n",
    "    df['Cercle'] = df['Cercle'].str.replace('DIOLA', 'DIOILA')\n",
    "    df['Cercle'] = df['Cercle'].str.replace('NIANFUNKE', 'NIAFUNKE')\n",
    "    df['Cercle'] = df['Cercle'].str.replace('ASANGO', 'ANSONGO')\n",
    "    df['Cercle'] = df['Cercle'].str.replace('KOLONDIEB', 'KOLONDIEBA')\n",
    "\n",
    "                        # to handle missing 'Cercle' information, we make a mapping/dict\n",
    "                        # with as key the Commune (which is a subset of Cercle) and as\n",
    "                        # value the corresponding Cercle, which can be used after to fill\n",
    "                        # in missing Cercle information. We also check if there are not\n",
    "                        # illogical double entries, which would need manual handling. We\n",
    "                        # also change Cercle names with 'District' to NaN, so they can maybe\n",
    "                        # be picked up by the Commune-Cercle mapping\n",
    "    df['Cercle'] = df['Cercle'].replace('DISTRICT', np.nan)\n",
    "    df['Commune'] = df['Commune'].str.strip().str.upper()\n",
    "    dict_communce_cercle = df.groupby('Commune')['Cercle'].unique().to_dict()\n",
    "                        # delete NaN's from the arrays itself in the dictionary;\n",
    "                        # loop over all dictionary items and remove NaN's\n",
    "    for commune, cercles in dict_communce_cercle.items():\n",
    "        dict_communce_cercle[commune] = [cercle for cercle in cercles if isinstance(cercle, str)]\n",
    "                        # check whether a commune is associated w/ a missing or multiple cercles\n",
    "    for commune, cercles in dict_communce_cercle.items():\n",
    "        if len(cercles) == 1 and (not isinstance(cercles[0], str) or cercles[0].strip() == ''):\n",
    "            if verbose:\n",
    "                print(f\"Commune '{commune}' is associated with a missing Cercle\")\n",
    "    for commune, cercles in dict_communce_cercle.items():\n",
    "        if len(cercles) > 1:\n",
    "            if verbose:\n",
    "                print(f\"Commune '{commune}' is associated with multiple Cercles: {cercles}\")\n",
    "                        # which prints:\n",
    "    \"\"\"\n",
    "    Commune 'BLA' is associated with multiple Cercles: ['SAN', 'BLA']\n",
    "    Commune 'LOGO' is associated with multiple Cercles: ['YELIMANE', 'KAYES']\n",
    "    Commune 'TIENFALA' is associated with multiple Cercles: ['BOUGOUNI', 'KOULIKORO']\n",
    "    \"\"\"\n",
    "                        # so we handle these manually (after checking Google Maps etc.)\n",
    "    dict_communce_cercle['BLA'] = ['SEGOU']\n",
    "    dict_communce_cercle['LOGO'] = ['KAYES']\n",
    "    dict_communce_cercle['TIENFALA'] = ['KOULIKORO']\n",
    "\n",
    "                        # next, we fill in missing 'Cercle' information with the dictionary\n",
    "                        # we created above, and we print out the rows with missing 'Cercle'\n",
    "    df['Cercle'] = df.apply(lambda row: fill_cercle_from_commune(dict_communce_cercle, row),\n",
    "                            axis = 1)\n",
    "    df_missing_cercle = df[df['Cercle'].isnull() | \\\n",
    "        df['Cercle'].apply(lambda x: not isinstance(x, str) or x.strip() == '')]\n",
    "    if verbose:         # Note: the above modifications \"saved\" 13 flood events\n",
    "        print(f'\\nImpact events without Cercle info: {df_missing_cercle.shape[0]}')\n",
    "        print(f'Impact events with Cercle info: {df.shape[0] - df_missing_cercle.shape[0]}\\n')\n",
    "                        # Note: if after above, 'Cercle' is still missing, we *could* try\n",
    "                        # and see if the 'Region' column is (coincidently?) the same as a\n",
    "                        # 'Cercle' column, but we ignore this possibility for now.\n",
    "                        # Next, we separate the data which has no 'Cercle' information\n",
    "                        # and drop the three unnecessary remaining columns \n",
    "    df = df[~df.index.isin(df_missing_cercle.index)]\n",
    "                        #! Comment this to not drop the indicated columns and also\n",
    "                        #! add them to the line: df_events = df_group[[...]] column list\n",
    "                        #! Don't forget the line which gets rid of the accent in Region\n",
    "    df = df.drop(columns = ['Région', 'Commune', 'Quartier/Villages'])\n",
    "    # df = df.rename(columns = {'Région': 'Region'})\n",
    "                        # calculate the duration of each flood event and add as column,\n",
    "                        # where the difference is inclusive, so we add 1 to the result\n",
    "    df['duration'] = (df['flood_end'] - df['flood_start']).dt.days + 1\n",
    "\n",
    "                        # next, we map the names of the 'Cercle' column to the admin units\n",
    "                        # in the flood data, and we check if all 'Cercle' values are mapped\n",
    "    df = map_cercle_names_to_pcodes(df) \n",
    "    df_missing_mapping = df[df['admin_unit'].isnull()]\n",
    "    if df_missing_mapping.shape[0] > 0:\n",
    "        print(f'unmapped Cercle values: {df_missing_mapping[\"Cercle\"].unique()}')\n",
    "    # df.to_csv('../data/impact_data/impact_data_Mali_tidied.csv')\n",
    "                        # lastly, we group the data by 'Cercle' and create a dictionary\n",
    "                        # with the 'Cercle' as key and the corresponding dataframe as value\n",
    "    dict_events = {}\n",
    "                        # order the data and store in dictionary\n",
    "    for cercle, df_group in df.groupby('admin_unit'):\n",
    "                        # ensure chronological order of the events\n",
    "        df_group = df_group.sort_values(by = ['flood_start', 'flood_end']).reset_index(drop = True)\n",
    "                        # give events an identifier and set as index\n",
    "        df_group['event'] = df_group.index\n",
    "        df_group['event_ID'] = df_group.apply(lambda x: f\"{cercle}_{x['event']}\",\n",
    "                                              axis = 1)\n",
    "        df_group.set_index('event_ID', inplace = True)\n",
    "        df_events = df_group[['admin_unit_NAME', 'admin_unit',\n",
    "                              'flood_start', 'flood_end', 'duration',]]\n",
    "                                # 'Region', 'Commune', 'Quartier/Villages']]\n",
    "        dict_events[cercle] = df_events\n",
    "        \n",
    "                        # merge duplicate events\n",
    "    dict_events_merged = merge_duplicate_events(dict_events)\n",
    "                        # sort again\n",
    "    for _, value in dict_events_merged.items():\n",
    "        value.sort_values(by = ['flood_start', 'flood_end'], inplace = True)\n",
    "\n",
    "                        # export to csv and return\n",
    "    export_dict_impact_events_to_csv(dict_events_merged, False, verbose)\n",
    "    return dict_events_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counterpart to the flood events are created here: \"impact events\" and \"observational events\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'IMPACT':\n",
    "    dict_impact_events = process_impact_data_to_events(df_impact_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_obs_events(thr: int = 5) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Gets the observation data events for the given threshold and transforms\n",
    "    them to the same format as the flood events and impact events\n",
    "\n",
    "    :param thr: return period threshold\n",
    "    :return: dictionary with observation events\n",
    "    \"\"\"\n",
    "    if thr not in [1.5, 2, 5, 10, 95, 98, 99]:\n",
    "        raise ValueError(\"threshold must be 1.5, 2, 5, 7, 10, 20\")\n",
    "    if thr in [1.5, 2, 5, 10]:\n",
    "        thr = float(thr)\n",
    "        path_obs = f'../../comparison/observation/data/observational_flood_events_RP_{thr}yr.csv'\n",
    "    if thr in [95, 98, 99]:\n",
    "        #! TODO add the correct path when these RPs are available\n",
    "        pass\n",
    "\n",
    "    df_obs = pd.read_csv(path_obs, sep = ',', index_col = 0, header = 0, encoding = 'utf-8')\n",
    "    df_obs = df_obs[['StationName', 'ADM2_PCODE', 'Start Date', 'End Date']]\n",
    "    df_obs = df_obs[df_obs['Start Date'] >= '2016-01-01']\n",
    "    df_obs.rename(columns = {'Start Date': 'flood_start',\n",
    "                             'End Date': 'flood_end',\n",
    "                             'StationName': 'identifier',\n",
    "                             'ADM2_PCODE': 'admin_unit'},\n",
    "                  inplace = True)\n",
    "    df_obs['flood_start'] = pd.to_datetime(df_obs['flood_start']).dt.date\n",
    "    df_obs['flood_end'] = pd.to_datetime(df_obs['flood_end']).dt.date\n",
    "    df_obs['flood_start'] = pd.to_datetime(df_obs['flood_start'])\n",
    "    df_obs['flood_end'] = pd.to_datetime(df_obs['flood_end'])\n",
    "    df_obs['duration'] = (df_obs['flood_end'] - df_obs['flood_start']).dt.days + 1\n",
    "    df_obs.sort_values(['identifier', 'flood_start'], inplace = True)\n",
    "    df_obs.reset_index(drop = True, inplace = True)\n",
    "    df_obs['event'] = df_obs.groupby('identifier').cumcount()\n",
    "    df_obs['event_ID'] = df_obs.apply(lambda x: f\"{x['identifier']}_{x['event']}\", axis = 1)\n",
    "    with open('../data/mappings/PCODE_to_Cercle.json', 'r') as f:\n",
    "        inverse_mapping = json.load(f)\n",
    "    df_obs['admin_unit_NAME'] = df_obs['admin_unit'].map(inverse_mapping)\n",
    "    df_obs = df_obs[['event_ID', 'identifier', 'flood_start', 'flood_end',\n",
    "                     'duration', 'admin_unit', 'admin_unit_NAME']]\n",
    "    df_obs.sort_values('event_ID', inplace = True)\n",
    "    df_obs.reset_index(drop = True, inplace = True)\n",
    "    dict_obs_events = create_event_dict(df_obs)\n",
    "    dict_obs_events = merge_duplicate_events(dict_obs_events)\n",
    "    for _, value in dict_obs_events.items():\n",
    "        value.sort_values(by = ['flood_start', 'flood_end'], inplace = True)\n",
    "    export_dict_impact_events_to_csv(dict_obs_events, True, False)\n",
    "    return dict_obs_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'OBS':\n",
    "    dict_obs_events_final = get_obs_events(THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next and final step of impact data preprocessing is subsetting by the available dates\n",
    "# in the forecast data and by the available administrative units in the forecast data\n",
    "def determine_min_max_date_of_dataset(d_ds: Dict[str, xr.Dataset]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Determine the earliest and latest date of the datasets\n",
    "\n",
    "    :param d_ds: dictionary with datasets\n",
    "    :return: tuple with earliest and latest date\n",
    "    \"\"\"\n",
    "    earliest_dates = []\n",
    "    latest_dates = []\n",
    "    for ds in d_ds.values():\n",
    "        earliest_dates.append(ds['issue_time'].values[0])\n",
    "        latest_dates.append(ds['issue_time'].values[-1])\n",
    "\n",
    "    return min(earliest_dates), max(latest_dates)\n",
    "\n",
    "\n",
    "def subset_events_on_unit_and_date(\n",
    "        d_impact_events: Dict[str, pd.DataFrame],\n",
    "        d_forecast_units: Dict[str, xr.Dataset],\n",
    "        earliest_date: str, latest_date: str\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Subset impact events based on available forecast administrative units \n",
    "    and specified date range. Events are filtered to include only those \n",
    "    within the specified date range and belonging to administrative units \n",
    "    present in the forecast data. Administrative units with no valid impact \n",
    "    data within the date range or not in the forecast data are logged\n",
    "    \n",
    "    :param d_impact_events: dictionary with impact events\n",
    "    :param d_units: dictionary with available administrative units, i.e.\n",
    "                    the units for which forecast data is available\n",
    "    :param earliest_date: earliest date\n",
    "    :param latest_date: latest date\n",
    "    :return: dictionary with subsetted events\n",
    "    \"\"\"\n",
    "    d_subsetted = {}\n",
    "    l_no_impact_data = []\n",
    "    earliest_date = pd.to_datetime(earliest_date)\n",
    "    latest_date = pd.to_datetime(latest_date)\n",
    "                            # for every admin unit in the impact data:\n",
    "    for admin_unit, df in d_impact_events.items():\n",
    "                            # check if it is in the forecast data\n",
    "                            # and, if so, if it is in the time range\n",
    "        if admin_unit in d_forecast_units:\n",
    "            df_c = df.copy()\n",
    "            df_c['flood_start'] = pd.to_datetime(df_c['flood_start'])\n",
    "            df_c['flood_end'] = pd.to_datetime(df_c['flood_end'])\n",
    "\n",
    "            df_subset = df_c[(df_c['flood_start'] >= earliest_date) & \\\n",
    "                             (df_c['flood_end'] <= latest_date)]\n",
    "                            # if the subset is not empty, add to dict\n",
    "            if not df_subset.empty:\n",
    "                d_subsetted[admin_unit] = df_subset\n",
    "            else:\n",
    "                print(f'\\tno impact data found for {admin_unit}')\n",
    "                l_no_impact_data.append(admin_unit)\n",
    "        else:\n",
    "            print(f'{admin_unit} impact data discarded due to lack of forecast data')\n",
    "    \n",
    "    return d_subsetted, l_no_impact_data\n",
    "\n",
    "\n",
    "def subset_events_on_date(\n",
    "        d_events: Dict[str, pd.DataFrame],\n",
    "        earliest_date: str, latest_date: str\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Subset events based on specified date range\n",
    "\n",
    "    :param d_events: dictionary with events\n",
    "    :param earliest_date: earliest date\n",
    "    :param latest_date: latest date\n",
    "    :return: dictionary with subsetted events\n",
    "    \"\"\"\n",
    "    d_subsetted = {}\n",
    "    earliest_date = pd.to_datetime(earliest_date)\n",
    "    latest_date = pd.to_datetime(latest_date)\n",
    "    for identifier, df in d_events.items():\n",
    "        df_c = df.copy()\n",
    "        df_c['flood_start'] = pd.to_datetime(df_c['flood_start'])\n",
    "        df_c['flood_end'] = pd.to_datetime(df_c['flood_end'])\n",
    "\n",
    "        df_subset = df_c[(df_c['flood_start'] >= earliest_date) & \\\n",
    "                         (df_c['flood_end'] <= latest_date)]\n",
    "        if not df_subset.empty:\n",
    "            d_subsetted[identifier] = df_subset\n",
    "        else:\n",
    "            print(f'\\tno (obs) data found for {identifier}')\n",
    "    \n",
    "    return d_subsetted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the impact data and/or forecast data don't necessarily have the same time range and/or spatial coverage per year, we subset them here on a year-to-year basis for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'IMPACT':\n",
    "                            # determine available units and earliest and latest dates\n",
    "    available_admin_units = set(dict_datasets_au.keys())\n",
    "    earliest_date, latest_date = determine_min_max_date_of_dataset(dict_datasets_au)\n",
    "                            # subset the impact events on the available units and dates\n",
    "    dict_impact_events_final, _ = subset_events_on_unit_and_date(\n",
    "        dict_impact_events, available_admin_units, earliest_date, latest_date)\n",
    "    export_dict_impact_events_to_csv(dict_impact_events_final, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_flood_events_by_impact_per_year(\n",
    "    d_floods: Dict[str, pd.DataFrame], d_impacts: Dict[str, pd.DataFrame]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Filters flood events per administrative unit and per year, excluding flood\n",
    "    events for years where there is no impact data for that administrative unit\n",
    "\n",
    "    :param d_floods: dict of flood events per admin unit\n",
    "    :param d_impacts: dict of impact events per admin unit\n",
    "    :return: filtered dict of flood events per admin unit\n",
    "    \"\"\"\n",
    "    d_floods_filtered = {}\n",
    "\n",
    "    for admin_unit, df_floods in d_floods.items():\n",
    "                            # if there is impact data at all for the flood\n",
    "                            # events, then start filtering by year; if not,\n",
    "                            # the flood events are discarded entirely\n",
    "        if admin_unit in d_impacts:\n",
    "                            # make copy and ensure datatypes\n",
    "            df_floods_c = df_floods.copy()\n",
    "            df_impacts_c = d_impacts[admin_unit].copy()\n",
    "            df_floods_c['flood_start'] = pd.to_datetime(df_floods_c['flood_start'])\n",
    "            df_floods_c['flood_end'] = pd.to_datetime(df_floods_c['flood_end'])\n",
    "            df_impacts_c['flood_start'] = pd.to_datetime(df_impacts_c['flood_start'])\n",
    "            df_impacts_c['flood_end'] = pd.to_datetime(df_impacts_c['flood_end'])\n",
    "                            # get set of impact years, and add a 'year' column\n",
    "                            # to flood events for filtering\n",
    "            impact_years = set(df_impacts_c['flood_start'].dt.year.unique())\n",
    "            df_floods_c['year'] = df_floods_c['flood_start'].dt.year\n",
    "                            # get set of original flood event years\n",
    "                            # and filter, getting a df with just flood events for\n",
    "                            # years where there is impact data available\n",
    "            original_years = set(df_floods_c['year'].unique())\n",
    "            df_floods_c = df_floods_c[df_floods_c['year'].isin(impact_years)]\n",
    "                            # get set of excluded years and drop the 'year' column again\n",
    "            excluded_years = original_years - impact_years\n",
    "            df_floods_c.drop(columns = ['year'], inplace = True)\n",
    "\n",
    "                            #* explanation of printing statements:\n",
    "                            #* no \\t:  best case scenario, all flood events retained\n",
    "                            #* one \\t: 2nd best, excluding some years of flood events\n",
    "                            #* two \\t: worst casr, no flood events retained after looking\n",
    "                            #*         for impact data (years), so all flood events discarded\n",
    "\n",
    "                            # there are flood events remaining, add to result\n",
    "            if not df_floods_c.empty:\n",
    "                d_floods_filtered[admin_unit] = df_floods_c\n",
    "                            # if some years are excluded, print them\n",
    "                if not excluded_years:\n",
    "                    print(f'{admin_unit}: all flood events retained')\n",
    "                else:  \n",
    "                    excluded_years = ', '.join([str(year) for year in excluded_years])\n",
    "                    print(f'\\t{admin_unit}: excluding flood events of years {excluded_years}')\n",
    "                            # else, no flood events left after filtering for years\n",
    "                            # with impact data (so there potentially was no impact\n",
    "                            # data to match with the flood events); print this out\n",
    "            else:\n",
    "                print(f'\\t\\t{admin_unit}: all flood events discarded due to lack of impact data')\n",
    "        else:\n",
    "            print(f'\\t\\t{admin_unit}: all flood events discarded due to lack of impact data')\n",
    "\n",
    "    return d_floods_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_flood_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'IMPACT':\n",
    "    # we use the impact events per administrative unit per year to \n",
    "    # get the admin units with no impact data out of the flood events entirely,\n",
    "    # and flood events for which there is no impact data are also removed\n",
    "    dict_flood_events_final = filter_flood_events_by_impact_per_year(\n",
    "        dict_flood_events, dict_impact_events_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'OBS':\n",
    "    dict_flood_events_final = filter_flood_events_by_impact_per_year(\n",
    "        dict_flood_events, dict_obs_events_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_flood_events_final.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dict_flood_events_to_csv(dict_flood_events_final, THRESHOLD, LEAD_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a map of all the admin units with at least one flood event\n",
    "# and impact event in the same year, i.e. all admin units that are\n",
    "# relevant for the analysis\n",
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def plot_leftover_admin_units(\n",
    "        d: Dict[str, Any],\n",
    "        path: str = 'mali_ALL/mli_adm_ab_shp/mli_admbnda_adm2_1m_gov_20211220.shp'\n",
    "    ) -> None:\n",
    "    \"\"\" \n",
    "    Plots the admin units that are leftover after filtering out\n",
    "\n",
    "    :param d: dictionary with admin units as keys, the values can be Any\n",
    "    :param path: path to the shape file\n",
    "    \"\"\"\n",
    "    red, blue = '#DB0A13', '#092448'\n",
    "    gpd_adm_units_Mali = analyse.get_shape_file(path).to_crs('EPSG:4326')\n",
    "    gpd_adm_units_Mali['col'] = gpd_adm_units_Mali['ADM2_PCODE'].apply(\n",
    "        lambda x: red if x in d else blue\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize = (10, 10))\n",
    "    gpd_adm_units_Mali.plot(\n",
    "        ax = ax, color = gpd_adm_units_Mali['col'], edgecolor = 'black', linewidth = 0.5\n",
    "    )\n",
    "    red_patch = Patch(color = red, label = 'Admin units with flood and impact data')\n",
    "    blue_patch = Patch(color = blue, label = 'Admin units without flood and impact data')\n",
    "    plt.legend(handles = [red_patch, blue_patch], loc = 'upper right')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('Administrative units after filtering')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'IMPACT':\n",
    "    plot_leftover_admin_units(dict_impact_events_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_flood_events_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dict_events_to_df(d_events: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine the dictionary of events to a single dataframe\n",
    "\n",
    "    :param d_events: dictionary with events\n",
    "    :return: dataframe with all events\n",
    "    \"\"\"\n",
    "    column_list = ['identifier', 'flood_start', 'flood_end']\n",
    "    if not d_events:\n",
    "        return pd.DataFrame(columns = column_list)\n",
    "    events_list = []\n",
    "    for admin_unit, df in d_events.items():\n",
    "        df = df.copy()\n",
    "        if 'identifier' not in df.columns:\n",
    "            df['identifier'] = admin_unit\n",
    "        events_list.append(df)\n",
    "    df = pd.concat(events_list, ignore_index = True)\n",
    "    df['flood_start'] = pd.to_datetime(df['flood_start'])\n",
    "    df['flood_end'] = pd.to_datetime(df['flood_end'])\n",
    "    return df[column_list]\n",
    "\n",
    "\n",
    "def calculate_metrics(d_metrics: Dict[str, Dict[str, int]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the performance metrics from the metrics dictionary;\n",
    "    metrics:\n",
    "    - probability of detection (POD): TP / (TP + FN)\n",
    "    - false alarm ratio (FAR): FP / (TP + FP)\n",
    "    - precision: TP / (TP + FP)\n",
    "    - recall: TP / (TP + FN)\n",
    "    - f1-score: 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    :param d_metrics: dictionary with metrics per admin unit\n",
    "    :return: dataframe with metrics\n",
    "    \"\"\"\n",
    "    metrics_list = []       # init storages\n",
    "    total_TP, total_FP, total_FN = 0, 0, 0\n",
    "\n",
    "    for admin_unit, metrics in d_metrics.items():\n",
    "        TP = metrics['TP']\n",
    "        FP = metrics['FP']\n",
    "        FN = metrics['FN']\n",
    "        total_TP += TP\n",
    "        total_FP += FP\n",
    "        total_FN += FN\n",
    "                            # calculate metrics\n",
    "        POD = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        FAR = FP / (TP + FP) if TP + FP > 0 else 0\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "                            # store metrics\n",
    "        metrics_list.append({\n",
    "            'identifier': admin_unit,\n",
    "            'TP': TP,\n",
    "            'FP': FP,\n",
    "            'FN': FN,\n",
    "            'POD': POD,\t\n",
    "            'FAR': FAR,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        })\n",
    "                            # calculate total metrics\n",
    "    total_POD = total_TP / (total_TP + total_FN) if total_TP + total_FN > 0 else 0\n",
    "    total_FAR = total_FP / (total_TP + total_FP) if total_TP + total_FP > 0 else 0\n",
    "    total_precision = total_TP / (total_TP + total_FP) if total_TP + total_FP > 0 else 0\n",
    "    total_recall = total_TP / (total_TP + total_FN) if total_TP + total_FN > 0 else 0\n",
    "    total_f1 = 2 * (total_precision * total_recall) / (total_precision + total_recall) if \\\n",
    "        total_precision + total_recall > 0 else 0\n",
    "                            # store total metrics at first index (0th)\n",
    "    metrics_list.insert(0, {\n",
    "        'identifier': 'total',\n",
    "        'TP': total_TP,\n",
    "        'FP': total_FP,\n",
    "        'FN': total_FN,\n",
    "        'POD': total_POD,\n",
    "        'FAR': total_FAR,\n",
    "        'precision': total_precision,\n",
    "        'recall': total_recall,\n",
    "        'f1': total_f1\t\n",
    "    })\n",
    "\n",
    "                            # round floats to 3 decimals \n",
    "    for metric in metrics_list:\n",
    "        for key, value in metric.items():\n",
    "            if isinstance(value, float):\n",
    "                metric[key] = np.round(value, 3)\n",
    "\n",
    "    return pd.DataFrame(metrics_list).sort_values('identifier')\n",
    "    # return pd.DataFrame(metrics_list)\n",
    "\n",
    "\n",
    "def match_events_and_get_metrics(\n",
    "        d_flood_events: Dict[str, pd.DataFrame],\n",
    "        d_impact_events: Dict[str, pd.DataFrame],\n",
    "        action_lifetime: int = 10\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Matches flood events to impact events and calculates performance metrics;\n",
    "    it calculates the following metrics per administrative unit:\n",
    "    - True Positives (TP): the number of correctly predicted flood events;\n",
    "    - False Positives (FP): the number of incorrectly predicted flood events; and\n",
    "    - False Negatives (FN): the number of missed flood events,\n",
    "    and returns them as a dictionary with admin unit as key, metrics as value.\n",
    "\n",
    "    :param d_flood_events: dictionary with flood events\n",
    "    :param d_impact_events: dictionary with impact events\n",
    "    :param action_lifetime: margin of error for comparison of dates (or could\n",
    "                            also be called, in a practical sense, the action lifetime)\n",
    "    :return: df with total and per unit metrics (through helper function)\n",
    "    \"\"\"\n",
    "                            # convert the event dictionaries back to dataframes\n",
    "    df_flood_events = combine_dict_events_to_df(d_flood_events)\n",
    "    df_impact_events = combine_dict_events_to_df(d_impact_events)\n",
    "    metrics = {}\n",
    "                            # get the set of unique administrative units from\n",
    "                            # the flood and impact events, although they should\n",
    "                            # be the same, we take the union anyway\n",
    "    admin_units = set(df_flood_events['identifier']).union(set(df_impact_events['identifier']))\n",
    "    for admin_unit in admin_units:\n",
    "                            # zero-init metrics\n",
    "        TP, FP, FN = 0, 0, 0\n",
    "                            # get the flood and impact events for the admin unit\n",
    "        events_pred = \\\n",
    "            df_flood_events[df_flood_events['identifier'] == admin_unit].reset_index(drop = True)\n",
    "        events_true = \\\n",
    "            df_impact_events[df_impact_events['identifier'] == admin_unit].reset_index(drop = True)\n",
    "                            # check if there are any events for the admin unit, if not\n",
    "                            # we can skip the calculation and add metrics immediately;\n",
    "                            # if no events at all, all metrics are zero\n",
    "        if events_pred.empty and events_true.empty:\n",
    "            metrics[admin_unit] = {'TP': TP, 'FP': FP, 'FN': FN}\n",
    "            continue        # if no predicted events, all true events are false negatives\n",
    "        if events_pred.empty:\n",
    "            metrics[admin_unit] = {'TP': TP, 'FP': FP, 'FN': events_true.shape[0]}\n",
    "            continue        # if no true events, all predicted events are false positives\n",
    "        if events_true.empty:\n",
    "            metrics[admin_unit] = {'TP': TP, 'FP': events_pred.shape[0], 'FN': FN}\n",
    "                            # convert the events to intervals for comparison, with\n",
    "                            # closed as 'both' and add margin of error for the preds:\n",
    "                            # https://pandas.pydata.org/docs/reference/api/pandas.Interval.html\n",
    "        intervals_pred = events_pred.apply(\n",
    "            lambda row: pd.Interval(row['flood_start'] - pd.Timedelta(days = action_lifetime),\n",
    "                                    row['flood_end'] + pd.Timedelta(days = action_lifetime),\n",
    "                                    closed = 'both'), axis = 1)\n",
    "        intervals_true = events_true.apply(\n",
    "            lambda row: pd.Interval(row['flood_start'],\n",
    "                                    row['flood_end'],\n",
    "                                    closed = 'both'),axis = 1)\n",
    "        \n",
    "                            # sets to keep track of matched events and avoid double counting;\n",
    "                            # loop over the predicted events and check if they\n",
    "                            # match with the true events, and update the metrics\n",
    "                            # (matched_pred is unused, but might be useful later)\n",
    "        matched_pred = set()\n",
    "        matched_true = set()\n",
    "        for idx_pred, interval_pred in intervals_pred.items():\n",
    "                            # set flag to False\n",
    "            is_match = False\n",
    "                            # look for overlap of prediction with an impact event\n",
    "            for idx_imp, interval_imp in intervals_true.items():\n",
    "                            # skip if event already matched (although this\n",
    "                            # wouldn't be triggered in practice)\n",
    "                if idx_imp in matched_true:\n",
    "                    continue\n",
    "                            # if overlap, update metrics and set flag to True\n",
    "                if interval_pred.overlaps(interval_imp):\n",
    "                    TP += 1\n",
    "                    matched_pred.add(idx_pred)\n",
    "                    matched_true.add(idx_imp)\n",
    "                    is_match = True\n",
    "                            # stop looking for impact events once a match is found;\n",
    "                            # the question is whether to do this yes/no. Will discuss on Monday\n",
    "                    # break\n",
    "                            # no match found, so false positive is added\n",
    "            if not is_match:\n",
    "                FP += 1\n",
    "                            # any impact events not matched are false negatives\n",
    "        for idx_imp in intervals_true.index:\n",
    "            if idx_imp not in matched_true:\n",
    "                FN += 1\n",
    "                            # store the metrics for the admin unit\n",
    "        metrics[admin_unit] = {'TP': TP, 'FP': FP, 'FN': FN}\n",
    "\n",
    "    return calculate_metrics(metrics)\n",
    "\n",
    "\n",
    "def export_results(\n",
    "        df: pd.DataFrame, ct: str, lt: int, th: int\n",
    "    ) -> None:\n",
    "    \"\"\" \n",
    "    Simple function that exports the results df in correct name format\n",
    "    \n",
    "    :param df: DataFrame with results\n",
    "    :param ct: comparison type (e.g. 'IMPACT')\n",
    "    :param lt: lead time in days (e.g. 7)\n",
    "    :param th: threshold (e.g. 5)\n",
    "    \"\"\"\n",
    "    if th in [95, 98, 99]:\n",
    "        th_str = f'{th}pc'\n",
    "    else:\n",
    "        th_str = f'{th}rp'\n",
    "    df.to_csv(\n",
    "        f'../data/results/GFH_vs_{ct}_{lt * 24}lt_{th_str}.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the performance of the model by comparing the\n",
    "flood events to the impact events, and we will do this by calculating\n",
    "the following metrics:\n",
    "* **True Positives (TP)**: the number of correctly predicted flood events;\n",
    "* **False Positives (FP)**: the number of incorrectly predicted flood events; and\n",
    "* **False Negatives (FN)**: the number of missed flood events,\n",
    "\n",
    "and with these a bunch of other metrics such as:\n",
    "* **Precision**: TP / (TP + FP);\n",
    "* **Recall**: TP / (TP + FN);\n",
    "* **F1-score**: 2 * Precision * Recall / (Precision + Recall);\n",
    "* **Probability of Detection (POD)**: TP / (TP + FN); and\n",
    "* **False Alarm Rate (FAR)**: FP / (TP + FP).\n",
    "\n",
    "Because we're comparing one \"type\" of event only, we have a sort of \"one-class classification\" or \"anamoly detection\", and therefore no True Negatives (TN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMP_TYPE == 'IMPACT':\n",
    "    dict_comp = dict_impact_events_final\n",
    "elif COMP_TYPE == 'OBS':\n",
    "    dict_comp = dict_obs_events_final\n",
    "\n",
    "# margin of error for the comparison of dates; argumentation for this\n",
    "# can be that the impact data is inexact and that for practical preparation-\n",
    "# wise purposes, a little margin of error does not make a big difference\n",
    "action_lifetime = 10 # also called the margin of error\n",
    "df_metrics = match_events_and_get_metrics(dict_flood_events_final,\n",
    "                                          dict_comp,\n",
    "                                          action_lifetime)\n",
    "# note that when action_lifetime is set to an arbitrarily large number such as\n",
    "# 10000, you'd expect only true positives, but that's not the case, since there\n",
    "# might be impact events for which *the* threshold is not passed within those\n",
    "# 10000 days, i.e. when the threshold is so high it is not passed within the timeframe\n",
    "inevitable_FNs = sum([len(dict_comp[admin_unit]) for admin_unit in \\\n",
    "                      dict_comp.keys() if admin_unit not in dict_flood_events_final])\n",
    "print(f'Admin units with impact data without a flood event for threshold {THRESHOLD} is',\n",
    "      len(dict_comp.keys()) - len(dict_flood_events_final.keys()),\n",
    "      f'\\nleading to {inevitable_FNs} inevitable False Negatives')\n",
    "print()\n",
    "\n",
    "export_results(df_metrics, COMP_TYPE, LEAD_TIME, THRESHOLD)\n",
    "print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def prepare_plot_df(d_events: Dict[str, pd.DataFrame], e_type: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Combines events from dictionary of events (i.e. flood or impact events)\n",
    "    to a DataFrame, with, as addition (and opposed to prior functions), an\n",
    "    extra column with 'event_type' to distinguish between flood and impact,\n",
    "    which is handy for plotting\n",
    "\n",
    "    :param d_events: dictionary with events\n",
    "    :param e_type: type of events (flood or impact)\n",
    "    :return: dataframe with all events\n",
    "    \"\"\"\n",
    "    events = []\n",
    "    for au, df in d_events.items():\n",
    "        df = df.copy()\n",
    "        df['admin_unit'] = au\n",
    "        df['event_type'] = e_type\n",
    "        events.append(df)\n",
    "                        # combine, ensure datetime columns, return\n",
    "    df = pd.concat(events, ignore_index = True)\n",
    "    df['flood_start'] = pd.to_datetime(df['flood_start'])\n",
    "    df['flood_end'] = pd.to_datetime(df['flood_end'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_plot_df_for_admin_unit(\n",
    "        d_floods: Dict[str, pd.DataFrame], d_impacts: Dict[str, pd.DataFrame], admin_unit: str\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Takes the dictionary of events and the dictionary of flood events,\n",
    "    combines them to a DataFrame with prepare_plot_df(), and subsets\n",
    "    for admin unit, giving a DataFrame with all events for that admin unit\n",
    "\n",
    "    :param d_floods: dictionary with events\n",
    "    :param d_impacts: dictionary with flood events\n",
    "    :param admin_unit: administrative unit\n",
    "    :return: dataframe with all events for admin unit\n",
    "    \"\"\"\n",
    "    if not admin_unit in d_floods.keys() or not admin_unit in d_impacts.keys():\n",
    "        print(f'Cannot continue with plot, {admin_unit} not in events')\n",
    "        return None\n",
    "    \n",
    "    df_floods = prepare_plot_df(d_floods, 'flood')\n",
    "    df_impacts = prepare_plot_df(d_impacts, 'impact')\n",
    "    df_all_events = pd.concat([df_floods, df_impacts],\n",
    "                               ignore_index = True).sort_values(['admin_unit', 'flood_start'])\n",
    "    df_all_events = df_all_events[df_all_events['admin_unit'] == admin_unit]\n",
    "\n",
    "    return df_all_events\n",
    "\n",
    "\n",
    "def hex_to_rgb(c: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Converts a hex color string to RGB tuple\n",
    "\n",
    "    :param c: hex color string (e.g., '#DB0000')\n",
    "    :return: tuple of (R, G, B) in 0 - 255 range\n",
    "    \"\"\"\n",
    "    return tuple(int(c.lstrip('#')[idx : idx + 2], 16) for idx in (0, 2, 4))\n",
    "\n",
    "\n",
    "def get_opacity_adjusted_hex(c: str, o: float = 1) -> str:\n",
    "    \"\"\" \n",
    "    Adjusts a colour for a given opactiy level:\n",
    "    https://graphicdesign.stackexchange.com/questions/126031/\n",
    "    make-color-transparent-and-keep-the-same-visual-color\n",
    "\n",
    "    :param c: colour in hex\n",
    "    :param o: opacity level (0 - 1)\n",
    "    :return: adjusted colour in hex\n",
    "    \"\"\"\n",
    "    foreground_c = hex_to_rgb(c)\n",
    "    background_c = (255, 255, 255) # white\n",
    "\n",
    "    return '#{:02x}{:02x}{:02x}'.format(*tuple(\n",
    "        max(0, min(255, int((fg - (1 - o) * bg) / o)))\n",
    "        for fg, bg in zip(foreground_c, background_c)\n",
    "    ))\n",
    "\n",
    "\n",
    "def plot_flood_and_impact_events(\n",
    "    d_floods: Dict[str, pd.DataFrame], d_impacts: Dict[str, pd.DataFrame],\n",
    "    ds_reforecast: xr.Dataset,\n",
    "    admin_unit: str, start_time: str, end_time: str, threshold: float, action_lifetime: int = 10\n",
    ") -> None:\n",
    "    \"\"\" \n",
    "    Timeseries met discharge van FloodHub (en GloFAS) met transparante\n",
    "    boxes(/lijnen bij 1-daagse events) voor events, plus ook return\n",
    "    periods als y. Geeft impressie van wat de modellen doen, wanneer ze\n",
    "    triggeren, alsook hoe ze (niet) matchen met impact\n",
    "    \n",
    "    :param d_floods: dictionary with flood events\n",
    "    :param d_impacts: dictionary with impact events\n",
    "    :param ds_reforecast: reforecast dataset\n",
    "    :param admin_unit: administrative unit\n",
    "    :param start_time: start time of the plot\n",
    "    :param end_time: end time of the plot\n",
    "    :param threshold: return period threshold\n",
    "    :param action_lifetime: margin of error/action lifetime for comparison of dates\n",
    "    \"\"\"\n",
    "    df = get_plot_df_for_admin_unit(d_floods, d_impacts, admin_unit)\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (20, 6))\n",
    "    alpha = 0.5                 # transparency\n",
    "    red, blue = '#DB0A13', '#092448'\n",
    "    red = get_opacity_adjusted_hex(red, alpha)\n",
    "    blue = get_opacity_adjusted_hex(blue, alpha)\n",
    "    \n",
    "    start_time = pd.to_datetime(start_time)\n",
    "    end_time = pd.to_datetime(end_time)\n",
    "    \n",
    "    ds_subset = ds_reforecast.streamflow.sel(issue_time = slice(start_time, end_time))\n",
    "    ax.plot(ds_subset.actual_date, ds_subset.values, color = 'black', linewidth = 2)\n",
    "    thr_string = f'RP_{threshold}' if threshold not in [95, 98, 99] else f'pc_{threshold}th'\n",
    "    ax.axhline(y = ds_reforecast.attrs[thr_string], color = 'orange',\n",
    "               label = f'{threshold}-yr return period', linestyle = '--'\n",
    "    )\n",
    "    # analyse.add_return_periods(ax, ds_return_periods, [5], True)\n",
    "\n",
    "    # if event is of type impact, add margin of error to start and end\n",
    "    for idx, row in df.iterrows():\n",
    "                                # subtract one day because of indexing error of the boxes\n",
    "        start = row['flood_start'] - pd.Timedelta(days = 1)\n",
    "        end = row['flood_end'] - pd.Timedelta(days = 1)\n",
    "                                # add action lifetime to the flood events\n",
    "        if row['event_type'] == 'flood':\n",
    "            start = start - pd.Timedelta(days = action_lifetime)\n",
    "            end = end + pd.Timedelta(days = action_lifetime)\n",
    "                                # inclusive duration\n",
    "        duration = (end - start).days + 1\n",
    "                                # add rectangle to plot for each event\n",
    "        rect = patches.Rectangle(\n",
    "                                # (x, y) position, starting from y = 0\n",
    "            (mdates.date2num(start), 0),\n",
    "            duration,           # width in days\n",
    "            ax.get_ylim()[1],   # height from y = 0 to max y (spans entire height)\n",
    "            facecolor = red if row['event_type'] == 'flood' else blue,\n",
    "            alpha = alpha\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlim(start_time, end_time)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    fig.autofmt_xdate() \n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel(r'discharge ($\\mathrm{m}^3/\\mathrm{s}$)')\n",
    "\n",
    "    legend_handles = [\n",
    "        patches.Patch(color = blue, alpha = alpha, label = 'impact events'),\n",
    "        patches.Patch(color = red, alpha = alpha, label = 'forecasted events'),\n",
    "        plt.Line2D([0], [0], color = 'black', linewidth = 2, label = 'forecasted discharge')\n",
    "    ]\n",
    "    ax.legend(handles = legend_handles, loc = 'upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0305'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2018-07-01', '2019-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0305'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2016-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of admin units:\n",
    "admin_units = list(dict_flood_events_final.keys())\n",
    "print(admin_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0206'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2016-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0406'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2016-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0406'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2022-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0401'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2016-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0401'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2022-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0301'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2016-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0301'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2022-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_plot = 'ML0103'\n",
    "plot_flood_and_impact_events(dict_flood_events_final, dict_comp,\n",
    "                             dict_datasets_au[au_plot],\n",
    "                             au_plot, '2016-07-01', '2022-12-31', THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add GeoJSON information to the impact data result dataframes for smooth plotting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for file in os.listdir(\"../data/results\"):\n",
    "    if file.startswith(\"GFH_vs_IMPACT\") and file.endswith(\".csv\"):\n",
    "        df = pd.read_csv(f\"../data/results/{file}\")\n",
    "        df.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "        with open('../data/mappings/PCODE_to_Cercle.json', 'r') as f:\n",
    "            inverse_mapping = json.load(f)\n",
    "        df['admin_unit_NAME'] = df['identifier'].map(inverse_mapping)\n",
    "        df.to_csv(f\"../data/results/{file}\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def add_shape_data_to_df(df: pd.DataFrame, path: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Adds shape data to a DataFrame with admin units\n",
    "    \"\"\"\n",
    "    gpd_adm_units = gpd.read_file(path)\n",
    "    gpd_adm_units = gpd_adm_units[['ADM2_PCODE', 'ADM2_FR', 'geometry']]\n",
    "    df.rename(columns = {'identifier': 'admin_unit'}, inplace = True)\n",
    "    gpd_adm_units.rename(columns = {'ADM2_PCODE': 'admin_unit'}, inplace = True)\n",
    "    merged_df = pd.merge(df, gpd_adm_units, on = 'admin_unit', how = 'left')\n",
    "    merged_gdf = gpd.GeoDataFrame(merged_df, geometry = 'geometry', crs = gpd_adm_units.crs)\n",
    "    return merged_gdf\n",
    "\n",
    "\n",
    "def export_all_results_to_geojson(results_folder: str, geojson_folder: str) -> None:\n",
    "    \"\"\"\n",
    "    Loop through results, merge with shapefile data, export to geojson\n",
    "    \"\"\"\n",
    "    path_shp = '../data/shape_files/mali_ALL/mli_adm_ab_shp/mli_admbnda_adm2_1m_gov_20211220.shp'\n",
    "\n",
    "    if not os.path.exists(geojson_folder):\n",
    "        os.makedirs(geojson_folder)\n",
    "\n",
    "    for file in os.listdir(results_folder):\n",
    "        if file.startswith('GFH_vs_IMPACT'):\n",
    "            csv_path = os.path.join(results_folder, file)\n",
    "            df = pd.read_csv(csv_path, index_col = 0)\n",
    "            gdf = add_shape_data_to_df(df, path_shp)\n",
    "            out_geojson_path = os.path.join(geojson_folder, file.replace('.csv', '.geojson'))\n",
    "            gdf.to_file(out_geojson_path, driver = 'GeoJSON')\n",
    "\n",
    "\n",
    "export_all_results_to_geojson('../data/results', '../data/results_geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Automatically generate all metrics**\n",
    "\n",
    "The cell below generates all metrics for the given configs, and is commented out by default (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comp_types = ['IMPACT']\n",
    "# thresholds = [1.5, 2, 5, 10, 95, 98, 99]\n",
    "# lead_times = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# # comp_types = ['OBS']\n",
    "# # thresholds = [1.5, 2, 5, 10]\n",
    "# # lead_times = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# for comp_type in comp_types:\n",
    "#     for th in thresholds:\n",
    "#         for lt in lead_times:\n",
    "#             COMP_TYPE = comp_type\n",
    "#             THRESHOLD = th\n",
    "#             LEAD_TIME = lt\n",
    "#             print(f'\\n\\n\\t{comp_type}: calculating metrics for threshold',\n",
    "#                   THRESHOLD, 'and lead time', LEAD_TIME, '\\n\\n')\n",
    "\n",
    "#             # PREPARE DATA\n",
    "#             dict_datasets = assign_admin_unit_to_datasets(dict_datasets)\n",
    "#             assure_admin_units_assigned(dict_datasets)\n",
    "#             dict_datasets_au, dict_return_periods_au = aggregate_or_load_per_admin_unit(\n",
    "#                 dict_datasets, LEAD_TIME\n",
    "#             )\n",
    "        \n",
    "#             # FLOOD & IMPACT EVENTS\n",
    "#             if not (COMP_TYPE == 'IMPACT' or COMP_TYPE == 'OBS'):\n",
    "#                 raise ValueError('COMP_TYPE must be either IMPACT or OBS')\n",
    "#             # Note that the flood events are already calculated by their \"actual date\",\n",
    "#             # hence, we export it using LEAD_TIME (* 24 hours) as lead time identifier\n",
    "#             if COMP_TYPE == 'IMPACT':\n",
    "#                 dict_flood_events = create_flood_events(dict_datasets_au, True, THRESHOLD)\n",
    "#             if COMP_TYPE == 'OBS':\n",
    "#                 dict_flood_events = create_flood_events(dict_DNH_datasets, True, THRESHOLD)\n",
    "#             export_dict_flood_events_to_csv(dict_flood_events, THRESHOLD, LEAD_TIME)\n",
    "\n",
    "#             if COMP_TYPE == 'IMPACT':\n",
    "#                 df_impact_data = pd.read_csv('../data/impact_data/impact_data_Mali.csv',\n",
    "#                                                 sep = ',',\n",
    "#                                                 header = 0,\n",
    "#                                                 encoding = 'utf-8')\n",
    "#                 dict_impact_events = process_impact_data_to_events(df_impact_data, False)\n",
    "#             if COMP_TYPE == 'OBS':\n",
    "#                 dict_obs_events_final = get_obs_events(THRESHOLD)\n",
    "#             if COMP_TYPE == 'IMPACT':\n",
    "#                 available_admin_units = set(dict_datasets_au.keys())\n",
    "#                 earliest_date, latest_date = determine_min_max_date_of_dataset(dict_datasets_au)\n",
    "#                 dict_impact_events_final, _ = subset_events_on_unit_and_date(\n",
    "#                     dict_impact_events, available_admin_units, earliest_date, latest_date)\n",
    "#                 export_dict_impact_events_to_csv(dict_impact_events_final, False, False)\n",
    "#             if COMP_TYPE == 'IMPACT':\n",
    "#                 dict_flood_events_final = filter_flood_events_by_impact_per_year(\n",
    "#                     dict_flood_events, dict_impact_events_final)\n",
    "#             if COMP_TYPE == 'OBS':\n",
    "#                 dict_flood_events_final = filter_flood_events_by_impact_per_year(\n",
    "#                     dict_flood_events, dict_obs_events_final)\n",
    "#             export_dict_flood_events_to_csv(dict_flood_events_final, THRESHOLD, LEAD_TIME)\n",
    "\n",
    "#             # PERFORMANCE METRICS\n",
    "#             if COMP_TYPE == 'IMPACT':\n",
    "#                 dict_comp = dict_impact_events_final\n",
    "#             elif COMP_TYPE == 'OBS':\n",
    "#                 dict_comp = dict_obs_events_final\n",
    "#             df_metrics = match_events_and_get_metrics(dict_flood_events_final,\n",
    "#                                                       dict_comp, 10)\n",
    "#             export_results(df_metrics, COMP_TYPE, LEAD_TIME, THRESHOLD)\n",
    "            \n",
    "#             print('\\n\\n\\t\\tCalculated metrics for threshold', THRESHOLD, 'and lead time', LEAD_TIME, '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
