import pandas as pd
import geopandas as gpd
from pathlib import Path
import numpy as np
import unidecode
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from PTM.PTM_prep.ptm_events import ptm_events
from comparison.observation.HydroImpact import events_per_adm, loop_over_stations
from GloFAS.GloFAS_analysis.flood_definer import FloodDefiner
from GloFAS.GloFAS_prep.vectorCheck import checkVectorFormat
import GloFAS.GloFAS_prep.configuration as cfg
class PredictedToImpactPerformanceAnalyzer:
    def __init__(self, DataDir, RPyr, impactData, triggerProb, adminLevel, adminPath, startYear, endYear, years, PredictedEvents_gdf, comparisonType, actionLifetime, model, leadtime):
        """
        Initialize the FloodPerformanceAnalyzer class with the required data.
        
        Parameters:
            floodCommune_gdf (GeoDataFrame): GeoDataFrame containing flood predictions.
            impact_gdf (GeoDataFrame): GeoDataFrame containing actual flood impacts. 
                                        should make this a csv possiblity
            triggerProb (float): Probability threshold to classify flood triggers.
        """

        self.triggerProb = triggerProb
        self.adminLevel = adminLevel
        
        self.DataDir = DataDir 
        self.RPyr = RPyr 
        self.startYear = startYear
        self.endYear = endYear
        self.years = years
        self.actionLifetime = actionLifetime
        self.comparisonType = comparisonType
        self.impactData = impactData
        self.model = model
        self.leadtime = leadtime
        self.comparisonDir = Path(f'{self.DataDir}/{self.model}/{comparisonType}')
        self.comparisonDir.mkdir (parents=True, exist_ok=True)
        gdf_shape = gpd.read_file(adminPath)
        self.gdf_shape = gdf_shape[gdf_shape[f'ADM{self.adminLevel}_FR'].apply(lambda x: isinstance(x, str))]
        #rename shapefile and make capital
        self.gdf_shape.rename(columns={f'ADM{cfg.adminLevel}_FR':f'ADM{cfg.adminLevel}'}, inplace=True)
        self.gdf_shape[f'ADM{cfg.adminLevel}'] = self.gdf_shape[f'ADM{cfg.adminLevel}'].apply(lambda x: unidecode.unidecode(x).upper())
        if self.model == 'PTM':
            self.PredictedEvents = self.openPTM_events (PredictedEvents_gdf)
        else:
            self.PredictedEvents = self.openModel_events (PredictedEvents_gdf)
            
        if comparisonType =='Observation': 
            self.impact_gdf = self.openObservation_gdf()
        elif comparisonType =='Impact': 
            self.impact_gdf = self.openObservedImpact_gdf()
        else: 
            raise ValueError(f'no such comparisonType: {comparisonType}')
         # days before and after validtime the prediction is also valid
        
    def openObservation_gdf(self):
        # Load the data
        if isinstance (self.impactData, str): 
            if self.impactData.endswith('.csv'):
                df = pd.read_csv(self.impactData)
            else: 
                df = gpd.read_file(self.impactData)
        else: # assuming other option is dataframe 
            df = self.impactData

        df[f'{comparisonType}']=1
        # Remove time and convert 'Start Date' and 'End Date' to datetime
        df['Start Date'] = df['Start Date'].astype(str)
        df['End Date'] = df['End Date'].astype(str)

        # Remove the time by splitting at space , just keep the date as it is average mean nayway
        df['Start Date'] = df['Start Date'].str.split(' ', expand=True)[0]
        df['End Date'] = df['End Date'].str.split(' ', expand=True)[0]

        # Convert to datetime without specifying format to auto-detect
        df['Start Date'] = pd.to_datetime(df['Start Date'], errors='coerce')
        df['End Date'] = pd.to_datetime(df['End Date'], errors='coerce')

        # Check the first few rows
        # Filter rows between 2004 and 2022 ()
        df_filtered = df[(df['End Date'].dt.year >= self.startYear) & (df['End Date'].dt.year < self.endYear)]
        # Remove non-string entries from ADM columns
        df_filtered = df_filtered[df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: isinstance(x, str))]
    
        
        df_filtered[f'ADM{self.adminLevel}'] = df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: unidecode.unidecode(x).upper())
        
        # Merge the CSV data with the shapefile data
        impact_gdf = pd.merge(self.gdf_shape, df_filtered, how='left', left_on=f'ADM{cfg.adminLevel}', right_on=f'ADM{cfg.adminLevel}')
        
        return impact_gdf 


    def openObservedImpact_gdf(self):
        # Load the data
        if self.impactData.endswith('.csv'):
            df = pd.read_csv(self.impactData, delimiter=";") # delimiter set back to ','if csv is not generated by Tijn 
        else: 
            df = gpd.read_file(self.impactData)

        # Convert 'End Date' and 'Start Date' to datetime
         # still works, so after this wrong filtering begins 
        
        # make extra column setting impact to one
        df[f'{comparisonType}']=1
        
        df['End Date'] = pd.to_datetime(df['End Date'])#, format='%d/%m/%Y', errors='coerce')
        df['Start Date'] = pd.to_datetime(df['Start Date'])#, format='%d/%m/%Y', errors= 'coerce')
        
        # Filter rows between 2004 and 2022 ()
        df_filtered = df[(df['End Date'].dt.year >= self.startYear) & (df['End Date'].dt.year < self.endYear)]
        # print (df_filtered.head)
        # Remove non-string entries from ADM columns
        df_filtered = df_filtered[df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: isinstance(x, str))]

        # Apply normalization to both DataFrames (converting to uppercase and removing special characters)
        
        df_filtered[f'ADM{self.adminLevel}'] = df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: unidecode.unidecode(x).upper())
        
        # Merge the CSV data with the shapefile data
        impact_gdf = pd.merge(self.gdf_shape, df_filtered, how='left', left_on=f'ADM{cfg.adminLevel}', right_on=f'ADM{cfg.adminLevel}')
        
        return impact_gdf 

    def openModel_events (self, predictedEvents_gdf): 
                # Load the data
        if isinstance (predictedEvents_gdf, str):
            if predictedEvents_gdf.endswith('.csv'):
                df = pd.read_csv(predictedEvents_gdf) # delimiter set back to ','if csv is not generated by Tijn 
            else: 
                df = gpd.read_file(predictedEvents_gdf)
        if isinstance (predictedEvents_gdf, pd.DataFrame):
            df = predictedEvents_gdf
        df[f'Event'] = 1
        df['End Date'] = pd.to_datetime(df['End Date']) + timedelta(days=self.actionLifetime) #, format='%d/%m/%Y', errors='coerce')
        df['Start Date'] = pd.to_datetime(df['Start Date'])#, format='%d/%m/%Y', errors= 'coerce')
        
        # Filter rows between 2004 and 2022 ()
        df_filtered = df[(df['End Date'].dt.year >= self.startYear) & (df['End Date'].dt.year < self.endYear)]

        # Remove non-string entries from ADM columns
        df_filtered = df_filtered[df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: isinstance(x, str))]

        if df_filtered.empty: 
            df_filtered = pd.DataFrame(columns=[f'ADM{self.adminLevel}','StationName', 'Start Date', 'End Date', 'Event'])
            print (f'no flood events predicted for return period: {self.RPyr}')
        else:
            df_filtered[f'ADM{self.adminLevel}'] = df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: unidecode.unidecode(x).upper())
        
                    # Merge the CSV data with the shapefile data
        modelevents= pd.merge(self.gdf_shape, df_filtered, how='left', left_on=f'ADM{self.adminLevel}', right_on=f'ADM{self.adminLevel}')
        return modelevents

    def openPTM_events (self, predictedEvents_gdf): 
                # Load the data
        if isinstance (predictedEvents_gdf, str):
            if predictedEvents_gdf.endswith('.csv'):
                df = pd.read_csv(predictedEvents_gdf) # delimiter set back to ','if csv is not generated by Tijn 
            else: 
                df = gpd.read_file(predictedEvents_gdf)
        if isinstance (predictedEvents_gdf, pd.DataFrame):
            df = predictedEvents_gdf
        # only select the events within the right start and end date, as well as for the right leadtime
    
        df[f'Event']=1
        df['End Date'] = pd.to_datetime(df['End Date']) + timedelta(days=self.actionLifetime) #, format='%d/%m/%Y', errors='coerce')
        df['Start Date'] = pd.to_datetime(df['Start Date'])#, format='%d/%m/%Y', errors= 'coerce')
        
        # Filter rows between 2004 and 2022, as well as for the leadtime of interest
        df_filtered = df[(df['End Date'].dt.year >= self.startYear) & (df['End Date'].dt.year < self.endYear) & (df['LeadTime'] == self.leadtime)]
        # Remove non-string entries from ADM columns
        df_filtered = df_filtered[df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: isinstance(x, str))]

        
        if df_filtered.empty: 
            print (f'no flood events predicted for return period: {self.RPyr}, leadtime: {self.leadtime}')
            return df_filtered # which will then be empty and just lead to no calculation of performance
        else:
            df_filtered[f'ADM{self.adminLevel}'] = df_filtered[f'ADM{self.adminLevel}'].apply(lambda x: unidecode.unidecode(x).upper())
                    # Merge the CSV data with the shapefile data
            modelevents= pd.merge(self.gdf_shape, df_filtered, how='left', left_on=f'ADM{self.adminLevel}', right_on=f'ADM{self.adminLevel}')
            return modelevents

    def _check_ifmatched (self, commune, startdate, enddate):
        match = self.impact_gdf[
                        (self.impact_gdf[f'ADM{self.adminLevel}'] == commune) & 
                        (self.impact_gdf['Start Date'] <= enddate ) &
                        (self.impact_gdf['Start Date'] >= startdate )
                        ]
        return 1 if not match.empty else 0
    def clean_and_add_modelprediction (self, PredictedEvents_gdf): 
        """
        ONLY CLEAN ONCE MATCHES between impact-predicted HAVE BEEN MADE
        Clean the mdoel's dataset by:
        - Removing entries where 'Event' is 0
        - Removing entries where there is no correspondence to the impact data (as to prevent double entries where matches have been made),
        - Removing entries for communes with no recorded impact in that year.
        After cleaning, add the remaining rows from GloFASCommune_gdf to impact_gdf,
        mapping 'Start Date' to 'Start Date', 'End Date' to 'End Date', and
        f'ADM{self.adminLevel}' to f'ADM{self.adminLevel}'.
        """

        PredictedEvents_gdf['Remove'] = PredictedEvents_gdf.apply(
            lambda row: self._check_ifmatched(
                row[f'ADM{self.adminLevel}'], 
                row['Start Date'], 
                row['End Date']),
                axis=1
            )

         # Keep entries from the original GloFASCommune_gdf where there was not yet a match
        PredictedEvents_gdf = PredictedEvents_gdf[PredictedEvents_gdf['Remove'] != 1] # remove was a 1, so we must keep everything that is not 1
        
        #NO YEAR IMPACT? --> no checking : this is the piece of code:
        for year in self.years:
            # Convert year to date range for that year
            start_of_year = pd.to_datetime(f'{year}-01-01')
            end_of_year = pd.to_datetime(f'{year}-12-31')

            # Find communes that recorded an impact in the given year
            recorded_impacts = self.impact_gdf[
                (self.impact_gdf['Start Date'] >= start_of_year) &
                (self.impact_gdf['End Date'] <= end_of_year)
                ][f'ADM{self.adminLevel}'].unique()

            # Remove rows in GloFAS where no impact was recorded for that commune in that year
            PredictedEvents_gdf = PredictedEvents_gdf[
                ~(
                    (PredictedEvents_gdf['Start Date'] >= start_of_year) &
                    (PredictedEvents_gdf['End Date'] <= end_of_year) &
                    (~PredictedEvents_gdf[f'ADM{self.adminLevel}'].isin(recorded_impacts))
                )]

        # Prepare the remaining rows to be added to impact_gdf
        remaining_rows = PredictedEvents_gdf.copy()

        # Rename columns to match impact_gdf structure
        remaining_rows = remaining_rows.rename(
            columns={
                'Start Date': 'Start Date',
                'End Date': 'End Date',
                f'ADM{self.adminLevel}': f'ADM{self.adminLevel}',
                'StationName': 'StationName',
                'LeadTime':'LeadTime'})
                
        remaining_rows [f'{comparisonType}'] = 0 # we have established before that these are not matching to any impact data, so the impact at these remaining rows are 0 
        # Append the renamed rows to impact_gdf
        self.impact_gdf = pd.concat([self.impact_gdf, remaining_rows], ignore_index=True)

    def _check_impact(self, PredictedEvents_gdf, commune, startdate, enddate):
        '''Check if impact that has happened in the commune between given dates is RECORDED by glofas.'''
        match = PredictedEvents_gdf[
                                (PredictedEvents_gdf[f'ADM{self.adminLevel}'] == commune) & 
                                (PredictedEvents_gdf['Start Date'] <= enddate ) &
                                (PredictedEvents_gdf['End Date'] >= startdate) &
                                (PredictedEvents_gdf['Event']==1)
                                ]
        return 1 if not match.empty else 0

    
    def matchImpact_and_Trigger(self):
        """
        Add 'Impact' or observation and 'Event' columns to the floodCommune_gdf,
        and calculate matches only when relevant data exists.
        """
        if self.PredictedEvents.empty or self.impact_gdf.empty:
            print("Skipped matching impacts and triggers as one of the datasets is empty.")
            return

        # Add Impact column using the check impact date (which only works on the impact_gdf)
        self.impact_gdf['Event'] = self.impact_gdf.apply(
            lambda row: self._check_impact(self.PredictedEvents, row[f'ADM{self.adminLevel}'], row['Start Date'], row['End Date']),
            axis=1
        )
        # Clean and add GloFAS to self.impact_gdf
        self.clean_and_add_modelprediction(self.PredictedEvents)
        # Save results
        self.impact_gdf.to_file(f"{self.DataDir}/{self.comparisonType}/model_vs{self.comparisonType}RP{self.RPyr:.1f}_yr.gpkg")


    def calc_performance_scores(self, obs, pred):
        '''Calculate performance scores based on observed and predicted values,
        including accuracy and precision'''
        
        # Initialize counters
        hits = 0           # True Positives
        misses = 0         # False Negatives
        false_alarms = 0   # False Positives

        # Calculate metrics
        for truth, prediction in zip(obs, pred):
            if truth == 1 and prediction == 1:  # True Positive
                hits += 1
            elif truth == 1 and prediction == 0:  # False Negative
                misses += 1
            elif truth == 0 and prediction == 1:  # False Positive
                false_alarms += 1 # there is no use in calculating correct negatives as there are many
        print (f'hits: {hits}, misses: {misses}, false alarms: {false_alarms}, total {comparisonType} = {sum(obs==1)}, should be equal to {misses}+{hits}={misses+hits}')
        # Calculate metrics
        sum_predictions = false_alarms + hits
        if sum_predictions == 0: 
            output = {
                'pod':  np.nan,  # Probability of Detection
                'far': np.nan,  # False Alarm Ratio
                #'pofd': false_alarms / (false_alarms + correct_negatives) if false_alarms + correct_negatives != 0 else np.nan,  # Probability of False Detection
                'csi': np.nan,  # Critical Success Index
                # 'accuracy': (hits + correct_negatives) / (hits + correct_negatives + false_alarms + misses) if hits + correct_negatives + false_alarms + misses != 0 else np.nan,  # Accuracy
                'precision': np.nan,
                'TP': hits,  
                'FN': misses,
                'FP': false_alarms
            }
        else: 
            output = {
                'pod': hits / (hits + misses) if hits + misses != 0 else np.nan,  # Probability of Detection
                'far': false_alarms / (hits + false_alarms) if hits + false_alarms != 0 else np.nan,  # False Alarm Ratio
                #'pofd': false_alarms / (false_alarms + correct_negatives) if false_alarms + correct_negatives != 0 else np.nan,  # Probability of False Detection
                'csi': hits / (hits + false_alarms + misses) if hits + false_alarms + misses != 0 else np.nan,  # Critical Success Index
                # 'accuracy': (hits + correct_negatives) / (hits + correct_negatives + false_alarms + misses) if hits + correct_negatives + false_alarms + misses != 0 else np.nan,  # Accuracy
                'precision': hits / (hits + false_alarms) if hits + false_alarms != 0 else np.nan,
                'TP': hits,  
                'FN': misses,
                'FP': false_alarms
            }

        return pd.Series(output)

    def calculateCommunePerformance(self):
        """
        Calculate the performance scores for each commune and merge them back into the GeoDataFrame,
        only if data is available.
        """
        if self.PredictedEvents.empty or self.impact_gdf.empty:
            print("Skipped performance calculation as no impact data is available.")
            return

        # Group by 'Commune' and calculate performance scores for each group
        scores_by_commune = self.impact_gdf.groupby(f'ADM{self.adminLevel}').apply(
            lambda x: self.calc_performance_scores(x[f'{self.comparisonType}'], x['Event'])
        )
        scores_byCommune_gdf = self.gdf_shape.merge(scores_by_commune, on=f'ADM{cfg.adminLevel}')
        scores_byCommune_gdf.to_file(f"{self.DataDir}/{self.model}/{self.comparisonType}/scores_byCommuneRP{self.RPyr:.1f}_yr_leadtime{self.leadtime}.gpkg")
        scores_byCommune_gdf.drop(columns='geometry').to_csv(f"{self.DataDir}/{self.model}/{self.comparisonType}/scores_byCommuneRP{self.RPyr:.1f}_yr_leadtime{self.leadtime}.csv")
        return scores_byCommune_gdf

if __name__=='__main__':
    impact_csv = f'{cfg.DataDir}/Impact_data/impact_events_per_admin_529.csv'
    comparisonType ='Impact'
    for RPyr in cfg.RPsyr: 
        # PTM_events = f'{cfg.DataDir}/PTM/floodevents_admUnit_RP{RPyr}yr.csv'
        ptm_events_df = ptm_events (cfg.DNHstations, cfg.DataDir, RPyr, cfg.StationCombos)
        PTM_events_per_adm = events_per_adm(cfg.DataDir, cfg.admPath, cfg.adminLevel, cfg.DNHstations, cfg.stationsDir, ptm_events_df, 'PTM', RPyr)
        for leadtime in cfg.leadtimes:
            # print (readcsv(f"{DataDir}/Données partagées - DNH Mali - 2019/Donne╠ües partage╠ües - DNH Mali - 2019/De╠übit du Niger a╠Ç Ansongo.csv"))
            analyzer = PredictedToImpactPerformanceAnalyzer(cfg.DataDir, RPyr, impact_csv, cfg.triggerProb, cfg.adminLevel, cfg.admPath, cfg.startYear, cfg.endYear, cfg.years, PTM_events_per_adm, comparisonType, cfg.actionLifetime, 'PTM', leadtime)
            analyzer.matchImpact_and_Trigger()
            analyzer.calculateCommunePerformance()
    
    comparisonType ='Observation'
    for RPyr in cfg.RPsyr: 
        observation_stations = loop_over_stations(cfg.DNHstations, cfg.DataDir, RPyr)
        observation_per_adm = events_per_adm(cfg.DataDir, cfg.admPath, cfg.adminLevel, cfg.DNHstations, cfg.stationsDir, observation_stations, 'Observation', RPyr) # f'{cfg.DataDir}/{comparisonType}/observationalStation_flood_events_RP_{RPyr:.1f}yr.csv'
                    # PTM_events = f'{cfg.DataDir}/PTM/floodevents_admUnit_RP{RPyr}yr.csv'
        ptm_events_df = ptm_events (cfg.DNHstations, cfg.DataDir, RPyr, cfg.StationCombos)
        PTM_events_per_adm = events_per_adm (cfg.DataDir, cfg.admPath, cfg.adminLevel, cfg.DNHstations, cfg.stationsDir, ptm_events_df, 'PTM', RPyr)
        for leadtime in cfg.leadtimes:
            # print (readcsv(f"{DataDir}/Données partagées - DNH Mali - 2019/Donne╠ües partage╠ües - DNH Mali - 2019/De╠übit du Niger a╠Ç Ansongo.csv"))
            analyzer = PredictedToImpactPerformanceAnalyzer(cfg.DataDir, RPyr, observation_per_adm, cfg.triggerProb, cfg.adminLevel, cfg.admPath, cfg.startYear, cfg.endYear, cfg.years, PTM_events_per_adm, comparisonType, cfg.actionLifetime, 'PTM', leadtime)
            analyzer.matchImpact_and_Trigger()
            analyzer.calculateCommunePerformance()


            